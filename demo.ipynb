{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `qp` Demo\n",
    "\n",
    "In this notebook we use the `qp` module to approximate some standard 1-D PDFs using sets of quantiles, and assess the accuracy of the quantile parametrization(TM).\n",
    "\n",
    "### Requirements\n",
    "\n",
    "To run `qp`, you will need to first install the module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import scipy.interpolate as spi\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `qp.PDF` Class\n",
    "\n",
    "This is the basic element of `qp` - an object representing a probability density function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This now exists in a separate module.  Classes will not appear in the notebook!\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# class PDF(object):\n",
    "\n",
    "#     def __init__(self, truth=None):\n",
    "#         self.truth = truth\n",
    "\n",
    "#     def evaluate(self, loc):#PDF\n",
    "\n",
    "#         return\n",
    "\n",
    "#     def integrate(self, limits):#CDF\n",
    "\n",
    "#         return\n",
    "\n",
    "#     def quantize(self, num_points):#inverse CDF (PPF)\n",
    "\n",
    "#         quanta = 1./num_points\n",
    "#         points = np.linspace(0.+quanta, 1.-quanta, num_points)\n",
    "#         print(points)\n",
    "#         self.quantiles = self.truth.ppf(points)\n",
    "#         print(self.quantiles)\n",
    "\n",
    "#         return self.quantiles\n",
    "\n",
    "#     def interpolate(self):\n",
    "\n",
    "#         return\n",
    "\n",
    "#     def plot(self, limits, num_points):\n",
    "\n",
    "#         x = np.linspace(limits[0], limits[1], 100)\n",
    "\n",
    "#         y = [0., 1.]\n",
    "#         plt.plot(x, self.truth.pdf(x), color='k', linestyle='-', lw=1., alpha=1., label='true pdf')\n",
    "#         plt.vlines(self.quantize(num_points), y[0], y[1], color='k', linestyle='--', lw=1., alpha=1., label='quantiles')\n",
    "#         plt.legend()\n",
    "#         plt.xlabel('x')\n",
    "#         plt.ylabel('probability density')\n",
    "#         plt.savefig('plot.png')\n",
    "\n",
    "#         return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating a Gaussian\n",
    "\n",
    "Let's summon a PDF object, and initialize it with a standard functiom - a Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist = sps.norm(loc=0, scale=1)\n",
    "p = pdf.PDF(truth=dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the PDF object in order to compare the truth and the approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bounds = (-5.0, 5.0)\n",
    "npoints = 10\n",
    "p.plot(limits=bounds, num_points=npoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant code I've already written for other purposes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# util-sim module defines handy tools used in data generation\n",
    "# \"\"\"\n",
    "\n",
    "# import sys\n",
    "# import numpy as np\n",
    "# import scipy as sp\n",
    "# #import random\n",
    "# import bisect\n",
    "\n",
    "# np.random.seed(seed=0)\n",
    "\n",
    "# def lrange(x):\n",
    "#     \"\"\"\n",
    "#     lrange makes a range based on the length of a list or array l\n",
    "#     \"\"\"\n",
    "#     return xrange(len(x))\n",
    "\n",
    "# def safelog(xarr):\n",
    "#     \"\"\"\n",
    "#     safelog takes log of array with zeroes\n",
    "#     \"\"\"\n",
    "#     shape = np.shape(xarr)\n",
    "#     flat = xarr.flatten()\n",
    "#     logged = np.log(np.array([max(x,sys.float_info.epsilon) for x in flat]))\n",
    "#     return logged.reshape(shape)\n",
    "\n",
    "# def extend(arr,front,back):\n",
    "#     \"\"\"\n",
    "#     extend appends zeroes to ends of array\n",
    "#     \"\"\"\n",
    "#     return np.concatenate((np.array([sys.float_info.epsilon]*len(front)),arr,np.array([sys.float_info.epsilon]*len(back))),axis=0)\n",
    "\n",
    "# # tools for sampling an arbitrary discrete distribution, used in data generation\n",
    "# def cdf(weights):\n",
    "#     \"\"\"\n",
    "#     cdf takes weights and makes them a normalized CDF\n",
    "#     \"\"\"\n",
    "#     tot = sum(weights)\n",
    "#     result = []\n",
    "#     cumsum = 0.\n",
    "#     for w in weights:\n",
    "#       cumsum += w\n",
    "#       result.append(cumsum/tot)\n",
    "#     return result\n",
    "\n",
    "# def choice(pop, weights):\n",
    "#     \"\"\"\n",
    "#     choice takes a population and assigns each element a value from 0 to len(weights) based on CDF of weights\n",
    "#     \"\"\"\n",
    "#     assert len(pop) == len(weights)\n",
    "#     cdf_vals = cdf(weights)\n",
    "#     x = np.random.random()\n",
    "#     index = bisect.bisect(cdf_vals,x)\n",
    "#     return pop[index]\n",
    "\n",
    "# def normed(x,scale):\n",
    "#     \"\"\"\n",
    "#     normed takes a numpy array and returns a normalized version of it that integrates to 1\n",
    "#     \"\"\"\n",
    "#     x = np.array(x)\n",
    "#     scale = np.array(scale)\n",
    "#     norm = x/np.dot(x,scale)\n",
    "#     return norm\n",
    "\n",
    "# class tnorm(object):\n",
    "#     def __init__(self,mu,sig,ends):\n",
    "#         self.mu = mu\n",
    "#         self.sig = sig\n",
    "#         (self.min,self.max) = ends\n",
    "#         self.lo = self.loc(self.min)\n",
    "#         self.hi = self.loc(self.max)\n",
    "\n",
    "#     def loc(self,z):\n",
    "#         return (z-self.mu)/self.sig\n",
    "\n",
    "#     def phi(self,z):\n",
    "#         x = z/np.sqrt(2)\n",
    "#         term = sp.special.erf(x)\n",
    "#         return (1.+term)/2.\n",
    "\n",
    "#     def norm(self):\n",
    "#         return max(sys.float_info.epsilon,self.phi(self.hi)-self.phi(self.lo))\n",
    "\n",
    "#     def pdf(self,z):\n",
    "#         x = self.loc(z)\n",
    "#         pdf = sp.stats.norm.pdf(x)\n",
    "#         return pdf/(self.sig*self.norm())\n",
    "\n",
    "#     def cdf(self,z):\n",
    "#         x = self.loc(z)\n",
    "#         cdf = self.phi(x)-self.phi(self.lo)\n",
    "#         result = cdf/self.norm()\n",
    "#         #print('a cdf: {}/{}'.format(result,z))\n",
    "#         return result\n",
    "\n",
    "#     def rvs(self,J):\n",
    "#         func = sp.stats.truncnorm(self.lo,self.hi,loc=self.mu,scale=self.sig)\n",
    "#         return func.rvs(size=J)\n",
    "\n",
    "# class gmix(object):\n",
    "#     \"\"\"\n",
    "#     gmix object takes a numpy array of Gaussian parameters and enables computation of PDF\n",
    "#     \"\"\"\n",
    "#     def __init__(self,inarr,bounds):\n",
    "\n",
    "#         self.minZ,self.maxZ = bounds\n",
    "#         self.comps = inarr\n",
    "#         self.ncomps = len(self.comps)\n",
    "\n",
    "#         self.weights = np.transpose(self.comps)[2]\n",
    "#         self.weights = self.weights/sum(self.weights)\n",
    "# #         mincomps = [(self.minZ-comp[0])/comp[1] for comp in self.comps]\n",
    "# #         maxcomps = [(self.maxZ-comp[0])/comp[1] for comp in self.comps]\n",
    "#         self.comps = [tnorm(comp[0],comp[1],(self.minZ,self.maxZ)) for comp in self.comps]#[sp.stats.truncnorm(mincomps[c],maxcomps[c],loc=self.comps[c][0],scale=self.comps[c][1]) for c in lrange(self.comps)]\n",
    "# #         self.comps = [tnorm(comp[0],comp[1],(self.minZ,self.maxZ)) for comp in self.comps]\n",
    "# #         self.weights = np.array([self.calccdf(c,self.minZ,self.maxZ) for c in lrange(self.comps)])\n",
    "\n",
    "#     def pdfs(self,zs):\n",
    "#         print('zs.shape={}'.format(np.shape(zs)))\n",
    "#         out = np.array([self.weights[c]*self.comps[c].pdf(zs) for c in xrange(self.ncomps)])\n",
    "#         print('pdfs.out.shape={}'.format(np.shape(out)))\n",
    "#         return out\n",
    "\n",
    "#     def pdf(self,zs):\n",
    "#         out = np.sum(self.pdfs(zs),axis=0)\n",
    "#         print('pdf.out.shape={}'.format(np.shape(out)))\n",
    "#         return out\n",
    "\n",
    "#     def cdfs(self,zs):\n",
    "#         out = np.array([self.weights[c]*self.comps[c].cdf(zs) for c in xrange(self.ncomps)])\n",
    "#         return out\n",
    "\n",
    "#     def cdf(self,zs):\n",
    "#         out = np.sum(self.cdfs(zs),axis=0)\n",
    "#         return out\n",
    "\n",
    "#     def binned(self,zs):\n",
    "#         thing = self.cdf(zs)\n",
    "#         return thing[1:]-thing[:-1]\n",
    "\n",
    "#     def sample(self,N):\n",
    "#         choices = [0]*self.ncomps\n",
    "#         for j in xrange(N):\n",
    "#             choices[choice(xrange(self.ncomps), self.weights)] += 1\n",
    "#         samps = np.array([])\n",
    "#         for c in xrange(self.ncomps):\n",
    "#             j = choices[c]\n",
    "#             Zs = self.comps[c].rvs(j)\n",
    "#             samps = np.concatenate((samps,Zs))\n",
    "#         return np.array(samps)\n",
    "\n",
    "# class cont(object):\n",
    "#     \"\"\"\n",
    "#     cont object takes a numpy array of normalized discrete distribution and its range and enables computation of PDF\n",
    "#     \"\"\"\n",
    "#     def __init__(self,inarr,bounds):\n",
    "\n",
    "#         self.ndim = len(inarr)\n",
    "#         self.Zs = bounds\n",
    "#         self.difs = self.Zs[1:]-self.Zs[:-1]\n",
    "#         self.weights = inarr/np.dot(inarr,self.difs)\n",
    "# #         mincomps = [(self.minZ-comp[0])/comp[1] for comp in self.comps]\n",
    "# #         maxcomps = [(self.maxZ-comp[0])/comp[1] for comp in self.comps]\n",
    "#         self.dims = [uniform(loc=self.Zs[k],scale=self.difs[k]) for k in xrange(self.ndim)]#[sp.stats.truncnorm(mincomps[c],maxcomps[c],loc=self.comps[c][0],scale=self.comps[c][1]) for c in lrange(self.comps)]\n",
    "# #         self.comps = [tnorm(comp[0],comp[1],(self.minZ,self.maxZ)) for comp in self.comps]\n",
    "# #         self.weights = np.array([self.calccdf(c,self.minZ,self.maxZ) for c in lrange(self.comps)])\n",
    "\n",
    "#     def pdf(self,zs):\n",
    "#         out = np.array([self.weights[k]*np.array([self.dims[k].pdf(z) for z in zs]) for k in xrange(self.ndim)])\n",
    "#         return out\n",
    "\n",
    "#     def cdf(self,zs):\n",
    "#         out = np.array([self.weights[k]*np.array([self.dims[k].cdf(z) for z in zs]) for k in xrange(self.ndim)])\n",
    "#         return out\n",
    "\n",
    "#     def sample(self,N):\n",
    "#         choices = [0]*self.ndim\n",
    "#         for j in xrange(N):\n",
    "#             choices[choice(xrange(self.ndim), self.weights)] += 1\n",
    "#         samps = np.array([])\n",
    "#         for k in xrange(self.ndim):\n",
    "#             j = choices[k]\n",
    "#             Zs = self.dims[k].rvs(j)\n",
    "#             samps = np.concatenate((samps,Zs))\n",
    "#         return np.array(samps)\n",
    "\n",
    "# def makelf(truZ,zfactor,elements,outlier=None):#,dgen=None):\n",
    "\n",
    "#     if outlier is None:\n",
    "#         outlier = []\n",
    "\n",
    "#     mixmod = [[truZ+elem.shift,elem.stddev*zfactor,elem.weight] for elem in elements]\n",
    "#     mixmod.extend([[elem.obsZ,elem.stddev,elem.weight] for elem in outlier])\n",
    "\n",
    "#     lf = mixmod\n",
    "\n",
    "#     return(lf)#,dgen)\n",
    "\n",
    "# def makepdf(grid,truZ,gal,intp=None,dgen=None,outlier=None):\n",
    "\n",
    "#     elements = gal.elements\n",
    "\n",
    "#     zfactor = gal.makezfactor(truZ)\n",
    "\n",
    "#     difs = grid[1:]-grid[:-1]\n",
    "#     dif = difs[np.argmin(grid-truZ)]\n",
    "#     allsummed = np.zeros(len(grid)-1)\n",
    "\n",
    "#     lf = makelf(truZ,zfactor,elements,outlier=outlier)#,dgen)\n",
    "\n",
    "#     pdf = gmix(lf,(min(grid),max(grid)))\n",
    "\n",
    "#     if dgen != None:\n",
    "#         dgdist = gmix(dgen,(min(grid),max(grid)))\n",
    "#         const = dgdist.pdf(truZ)*dif\n",
    "#     else:\n",
    "#         const = 0.\n",
    "\n",
    "#     cdf = pdf.cdf(grid)\n",
    "#     spread = cdf[1:]-cdf[:-1]\n",
    "#     allsummed += spread\n",
    "#     allsummed += const\n",
    "#     if intp != None:\n",
    "#         pf = intp*allsummed\n",
    "#     else:\n",
    "#         pf = allsummed\n",
    "#     pf = np.array(pf)\n",
    "#     #pf = pf/max(np.dot(pf,difs),sys.float_info.epsilon)\n",
    "\n",
    "#     return(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# util-mcmc module defines handy tools for MCMC\n",
    "# \"\"\"\n",
    "\n",
    "# import sys\n",
    "# import numpy as np\n",
    "# import statistics\n",
    "# import cPickle as cpkl\n",
    "# import scipy as sp\n",
    "\n",
    "# def lrange(l):\n",
    "#     \"\"\"\n",
    "#     lrange(l) makes a range based on the length of a list or array l\n",
    "#     \"\"\"\n",
    "#     return xrange(len(l))\n",
    "\n",
    "# def safelog(xarr):\n",
    "#     \"\"\"\n",
    "#     safelog takes log of array with zeroes\n",
    "#     \"\"\"\n",
    "#     shape = np.shape(xarr)\n",
    "#     flat = xarr.flatten()\n",
    "#     logged = np.log(np.array([max(x,sys.float_info.epsilon) for x in flat]))\n",
    "#     return logged.reshape(shape)\n",
    "\n",
    "# def normed(x,scale):\n",
    "#     \"\"\"\n",
    "#     normed takes a numpy array and returns a normalized version of it that integrates to 1\n",
    "#     \"\"\"\n",
    "#     x = np.array(x)\n",
    "#     scale = np.array(scale)\n",
    "#     norm = x/np.dot(x,scale)\n",
    "#     return norm\n",
    "\n",
    "# class mvn(object):\n",
    "#     \"\"\"\n",
    "#     mvn object is multivariate normal distribution, to be used in data generation and prior to emcee\n",
    "#     \"\"\"\n",
    "#     def __init__(self, mean, cov):\n",
    "#         \"\"\"input multidimensional mean and covariance matrix as numpy arrays\"\"\"\n",
    "#         self.dims = len(mean)\n",
    "#         self.mean = mean\n",
    "#         self.cov = cov\n",
    "#         self.icov = np.linalg.pinv(self.cov, rcond=sys.float_info.epsilon)\n",
    "#         (self.logdetsign, self.logdet) = np.linalg.slogdet(self.cov)\n",
    "\n",
    "#     def logpdf(self, x):\n",
    "#         \"\"\"log probabilities\"\"\"\n",
    "#         delta = x - self.mean\n",
    "#         c = np.dot(delta, np.dot(self.icov, delta))\n",
    "#         prob = -0.5 * c\n",
    "#         return prob\n",
    "\n",
    "#     def sample_ps(self, W):\n",
    "#         \"\"\"W samples directly from distribution\"\"\"\n",
    "#         outsamp = np.random.multivariate_normal(self.mean, self.cov, W)\n",
    "#         return (outsamp, self.mean)\n",
    "\n",
    "#     def sample_gm(self,W):\n",
    "#         \"\"\"W samples around mean of distribution\"\"\"\n",
    "#         outsamp = [self.mean+np.random.randn(self.dims) for w in range(0,W)]\n",
    "#         return (outsamp, self.mean)\n",
    "\n",
    "#     def sample_gs(self, W):\n",
    "#         \"\"\"W samples from a single sample from distribution\"\"\"\n",
    "#         rando = np.random.multivariate_normal(self.mean, self.cov)\n",
    "#         #outsamp = [rando + np.sqrt(rando)*np.random.randn(self.dims) for w in range(0,W)]\n",
    "#         outsamp = [np.random.multivariate_normal(rando,self.cov) for w in range(0,W)]\n",
    "#         return (outsamp, rando)\n",
    "\n",
    "# class post(object):\n",
    "#     \"\"\"\n",
    "#     post object is posterior distribution we wish to sample\n",
    "#     \"\"\"\n",
    "#     def __init__(self,idist,xvals,yprobs,interim):\n",
    "#         \"\"\"data are logged posteriors (ngals*nbins), idist is mvn object\"\"\"\n",
    "#         self.prior = idist\n",
    "#         #self.priormean = idist.mean\n",
    "#         self.interim = interim\n",
    "#         self.xgrid = np.array(xvals)\n",
    "#         self.difs = self.xgrid[1:]-self.xgrid[:-1]#np.array([self.xgrid[k+1]-self.xgrid[k] for k in self.dims])\n",
    "#         self.lndifs = np.log(self.difs)#np.array([m.log(max(self.difs[k],sys.float_info.epsilon)) for k in self.dims])\n",
    "#         self.postprobs = yprobs\n",
    "#         self.constterm = self.lndifs-self.interim#self.priormean\n",
    "#         self.lnprob_ext = post_lnprob\n",
    "\n",
    "#     def priorprob(self,theta):\n",
    "#         \"\"\"this is proportional to log prior probability\"\"\"\n",
    "#         return self.prior.logpdf(theta)\n",
    "\n",
    "#     def lnlike(self,theta):\n",
    "#         \"\"\"specific to N(z) problem\"\"\"\n",
    "#         #return self.lnprob(theta)-self.priorprob(theta)\n",
    "#         constterms = theta+self.constterm\n",
    "#         sumterm = -1.*np.dot(np.exp(theta),self.difs)\n",
    "#         for j in lrange(self.postprobs):\n",
    "#             logterm = np.log(np.sum(np.exp(self.postprobs[j]+constterms)))\n",
    "#             sumterm += logterm\n",
    "#         return sumterm\n",
    "\n",
    "#     def mlnlike(self,theta):\n",
    "#         return -1.*self.lnlike(theta)\n",
    "\n",
    "#     # speed this up some more with matrix magic?\n",
    "#     def lnprob(self,theta):\n",
    "#         \"\"\"calculate log posterior probability\"\"\"\n",
    "# #         constterms = theta+self.constterm\n",
    "# #         sumterm = self.priorprob(theta)-np.dot(np.exp(theta),self.difs)#this should sufficiently penalize poor samples but somehow fails on large datasets\n",
    "# #         for j in lrange(self.postprobs):\n",
    "# #             #logterm = sp.misc.logsumexp(self.postprobs[j]+constterms)#shockingly slower!\n",
    "# #             #logterm = np.logaddexp(self.postprobs[j]+constterms)#only works for two terms\n",
    "# #             logterm = np.log(np.sum(np.exp(self.postprobs[j]+constterms)))\n",
    "# #             sumterm += logterm\n",
    "# #         return sumterm\n",
    "#         return self.lnlike(theta)+self.priorprob(theta)\n",
    "\n",
    "# def post_lnprob(theta, other_self):\n",
    "#     ret = other_self.lnprob(theta)\n",
    "#     return ret\n",
    "\n",
    "# class path(object):\n",
    "#     \"\"\"\n",
    "#     path object takes templates of path style and variables for it and makes os.path objects from them\n",
    "#     \"\"\"\n",
    "#     def __init__(self, path_template, filled = None):\n",
    "#         self.path_template = path_template\n",
    "#         if filled is None:\n",
    "#             self.filled = {}\n",
    "#         else:\n",
    "#             self.filled = filled\n",
    "\n",
    "#     def construct(self, **args):\n",
    "#         \"\"\"actually constructs the final path, as a string.  Optionally takes in any missing parameters\"\"\"\n",
    "#         nfilled = self.filled.copy()\n",
    "#         nfilled.update(args)\n",
    "#         return self.path_template.format(**nfilled)\n",
    "\n",
    "#     def fill(self, **args):\n",
    "#         \"\"\"fills any number of missing parameters, returns new object\"\"\"\n",
    "#         dct = self.filled.copy()\n",
    "#         dct.update(args)\n",
    "#         return path(self.path_template, dct)\n",
    "\n",
    "# class tnorm(object):\n",
    "#     \"\"\"truncated normal distribution object\"\"\"\n",
    "#     def __init__(self,mu,sig,ends):\n",
    "#         self.mu = mu\n",
    "#         self.sig = sig\n",
    "#         (self.min,self.max) = ends\n",
    "#         self.lo = self.loc(self.min)\n",
    "#         self.hi = self.loc(self.max)\n",
    "\n",
    "#     def loc(self,z):\n",
    "#         return (z-self.mu)/self.sig\n",
    "\n",
    "#     def phi(self,z):\n",
    "#         x = z/np.sqrt(2)\n",
    "#         term = sp.special.erf(x)\n",
    "#         return (1.+term)/2.\n",
    "\n",
    "#     def norm(self):\n",
    "#         return max(sys.float_info.epsilon,self.phi(self.hi)-self.phi(self.lo))\n",
    "\n",
    "#     def pdf(self,z):\n",
    "#         x = self.loc(z)\n",
    "#         pdf = sp.stats.norm.pdf(x)\n",
    "#         return pdf/(self.sig*self.norm())\n",
    "\n",
    "#     def cdf(self,z):\n",
    "#         x = self.loc(z)\n",
    "#         cdf = self.phi(x)-self.phi(self.lo)\n",
    "#         return cdf/self.norm()\n",
    "\n",
    "#     def rvs(self,J):\n",
    "#         func = sp.stats.truncnorm(self.lo,self.hi,loc=self.mu,scale=self.sig)\n",
    "#         return func.rvs(size=J)\n",
    "\n",
    "# class gmix(object):\n",
    "#     \"\"\"\n",
    "#     gmix object takes a numpy array of Gaussian parameters and enables computation of PDF\n",
    "#     \"\"\"\n",
    "#     def __init__(self,inarr,bounds):\n",
    "\n",
    "#         self.minZ,self.maxZ = bounds\n",
    "#         self.comps = inarr\n",
    "#         self.ncomps = len(self.comps)\n",
    "\n",
    "#         self.weights = np.transpose(self.comps)[2]\n",
    "# #         mincomps = [(self.minZ-comp[0])/comp[1] for comp in self.comps]\n",
    "# #         maxcomps = [(self.maxZ-comp[0])/comp[1] for comp in self.comps]\n",
    "#         self.comps = [tnorm(comp[0],comp[1],(self.minZ,self.maxZ)) for comp in self.comps]#[sp.stats.truncnorm(mincomps[c],maxcomps[c],loc=self.comps[c][0],scale=self.comps[c][1]) for c in lrange(self.comps)]\n",
    "# #         self.comps = [tnorm(comp[0],comp[1],(self.minZ,self.maxZ)) for comp in self.comps]\n",
    "# #         self.weights = np.array([self.calccdf(c,self.minZ,self.maxZ) for c in lrange(self.comps)])\n",
    "\n",
    "#     def pdfs(self,zs):\n",
    "#         out = np.array([self.weights[c]*np.array([self.comps[c].pdf(z) for z in zs]) for c in xrange(self.ncomps)])\n",
    "#         return out\n",
    "\n",
    "#     def pdf(self,zs):\n",
    "#         return np.sum(self.pdfs(zs),axis=0)\n",
    "\n",
    "#     def cdfs(self,zs):\n",
    "#         out = np.array([self.weights[c]*np.array([self.comps[c].cdf(z) for z in zs]) for c in xrange(self.ncomps)])\n",
    "#         return out\n",
    "\n",
    "#     def cdf(self,zs):\n",
    "#         return np.sum(self.cdfs(zs),axis=0)\n",
    "\n",
    "#     def binned(self,zs):\n",
    "#         thing = self.cdf(zs)\n",
    "#         return thing[1:]-thing[:-1]\n",
    "\n",
    "#     def sample(self,N):\n",
    "#         choices = [0]*self.ncomps\n",
    "#         for j in xrange(N):\n",
    "#             choices[choice(xrange(self.ncomps), self.weights)] += 1\n",
    "#         samps = np.array([])\n",
    "#         for c in xrange(self.ncomps):\n",
    "#             j = choices[c]\n",
    "#             Zs = self.comps[c].rvs(j)\n",
    "#             samps = np.concatenate((samps,Zs))\n",
    "#         return np.array(samps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# stat-mcmc module calculates intermediate statistics for monitoring state\n",
    "# \"\"\"\n",
    "\n",
    "# import statistics\n",
    "# import numpy as np\n",
    "# import cPickle as cpkl\n",
    "# import os\n",
    "# import scipy as sp\n",
    "# import csv\n",
    "\n",
    "# import utilmcmc as um\n",
    "\n",
    "# class calcstats(object):\n",
    "#     \"\"\"\n",
    "#     object class to set up and calculate summary statistics, unite stats for each output\n",
    "#     \"\"\"\n",
    "#     def __init__(self, meta):\n",
    "#         self.meta = meta\n",
    "#     def update(self, ydata):\n",
    "#         if self.meta.plotonly == False:\n",
    "#             stats = self.compute(ydata)\n",
    "#             self.meta.key.add_stats(self.meta.topdir, self.name, stats)\n",
    "\n",
    "# # # statistics involving both log posterior probabilities and parameter values\n",
    "# # class stat_both(calcstats):\n",
    "# #     \"\"\"\n",
    "# #     calculates statistics that require both posterior probabilities and parameter values: log likelihood ratio and MAP parameter values\n",
    "# #     \"\"\"\n",
    "# #     def __init__(self,meta):\n",
    "# #         calcstats.__init__(self,meta)\n",
    "\n",
    "# #         self.name = 'both'\n",
    "\n",
    "# #         self.ll_stk = self.meta.postdist.lnlike(self.meta.logstkNz)\n",
    "# # #         self.ll_map = self.meta.postdist.lnlike(self.meta.logmapNz)\n",
    "# # #         self.ll_exp = self.meta.postdist.lnlike(self.meta.logexpNz)\n",
    "# #         self.ll_int = self.meta.postdist.lnlike(self.meta.logintNz)\n",
    "# #         self.ll_mml = self.meta.postdist.lnlike(self.meta.logmmlNz)\n",
    "# #         self.ll_smp = []\n",
    "# # #         self.mapvals,self.maps = [],[]\n",
    "\n",
    "# # #         self.llr_stk,self.llr_map,\n",
    "# #         self.llr_stk,self.llr_int,self.llr_mml = [],[],[]\n",
    "\n",
    "# #         outdict = {'ll_stk': self.ll_stk,\n",
    "# # #                   'll_map': self.ll_map,\n",
    "# # #                   'll_exp': self.ll_exp,\n",
    "# #                   'll_int': self.ll_int,\n",
    "# #                   'll_mml': self.ll_mml,\n",
    "# #                   'll_smp': self.ll_smp,\n",
    "# #                   'llr_stk': np.array(self.llr_stk),\n",
    "# # #                   'llr_map': np.array(self.llr_map),\n",
    "# # #                   'llr_exp': np.array(self.llr_exp),\n",
    "# #                   'llr_int': np.array(self.llr_int),\n",
    "# #                   'llr_mml': np.array(self.llr_mml)\n",
    "# #                    }\n",
    "# #         if self.meta.plotonly == False:\n",
    "# #             with open(os.path.join(self.meta.topdir,'stat_both.p'),'wb') as statboth:\n",
    "# #                 cpkl.dump(outdict,statboth)\n",
    "\n",
    "# #     def compute(self,ydata):\n",
    "\n",
    "# #         self.probs = ydata['probs']\n",
    "# #         self.chains = ydata['chains']\n",
    "\n",
    "# # #         where = np.unravel_index(np.argmax(self.probs),(self.meta.nwalkers,self.meta.ntimes))\n",
    "# # #         self.mapvals.append(self.chains[where])\n",
    "# # #         self.maps.append(self.probs[where])\n",
    "\n",
    "# #         self.llr_stk = self.calclr(self.llr_stk,self.ll_stk)\n",
    "# # #         self.llr_map = self.calclr(self.llr_map,self.ll_map)\n",
    "# # #         self.llr_exp = self.calclr(self.llr_exp,self.ll_exp)\n",
    "# #         self.llr_mml = self.calclr(self.llr_mml,self.ll_mml)\n",
    "\n",
    "# #         if self.meta.logtruNz is not None:\n",
    "# #             self.calclr(self.ll_smp,0.)\n",
    "\n",
    "# #         with open(os.path.join(self.meta.topdir,'stat_both.p'),'rb') as indict:\n",
    "# #             outdict = cpkl.load(indict)\n",
    "\n",
    "# #         outdict['llr_stk'] = np.array(self.llr_stk)\n",
    "# # #         outdict['llr_map'] = np.array(self.llr_map)\n",
    "# # #         outdict['llr_exp'] = np.array(self.llr_exp)\n",
    "# #         outdict['llr_mml'] = np.array(self.llr_mml)\n",
    "# #         outdict['ll_smp'] = np.array(self.ll_smp).flatten()/2.\n",
    "# # #         outdict['mapvals'] = np.array(self.mapvals)\n",
    "# # #         outdict['maps'] = np.array(self.maps)\n",
    "\n",
    "# #         with open(os.path.join(self.meta.topdir,'stat_both.p'),'wb') as statboth:\n",
    "# #             cpkl.dump(outdict,statboth)\n",
    "# #         return\n",
    "\n",
    "# #     # likelihood ratio test\n",
    "# #     def calclr(self,var,ll):\n",
    "\n",
    "# #         for w in xrange(self.meta.nwalkers):\n",
    "# #             for x in xrange(self.meta.ntimes):\n",
    "# #                 ll_smp = self.probs[w][x]-self.meta.postdist.priorprob(self.chains[w][x])\n",
    "# #                 var.append(2.*(ll_smp-ll))\n",
    "# # #                 self.llr_stk.append(2.*ll_smp-2.*self.ll_stk)\n",
    "# # #                 self.llr_map.append(2.*ll_smp-2.*self.ll_map)\n",
    "# # #                 self.llr_exp.append(2.*ll_smp-2.*self.ll_exp)\n",
    "# #         return(var)\n",
    "\n",
    "# class stat_chains(calcstats):\n",
    "#     \"\"\"\n",
    "#     calculates statistics that need parameter values: variance, chi^2, KLD; statistics involving parameter values\n",
    "#     \"\"\"\n",
    "#     def __init__(self, meta):\n",
    "#         calcstats.__init__(self, meta)\n",
    "\n",
    "#         self.name = 'chains'\n",
    "\n",
    "#         self.var_ls = []\n",
    "#         self.var_s = []\n",
    "# #         self.vslogstk = None\n",
    "# #         self.vslogmap = None\n",
    "# # #         self.vslogexp = None\n",
    "# #         self.vsstk = None\n",
    "# #         self.vsmap = None\n",
    "# # #         self.vsexp = None\n",
    "\n",
    "#         self.chi_ls = []\n",
    "#         self.chi_s = []\n",
    "# #         self.cslogstk = None\n",
    "# #         self.cslogmap = None\n",
    "# # #         self.cslogexp = None\n",
    "# #         self.csstk = None\n",
    "# #         self.csmap = None\n",
    "# # #         self.csexp = None\n",
    "\n",
    "#         self.kl_stkvtru,self.kl_truvstk = None,None\n",
    "#         self.kl_mapvtru,self.kl_truvmap = None,None\n",
    "#         self.kl_expvtru,self.kl_truvexp = None,None\n",
    "#         self.kl_intvtru,self.kl_truvint = float('inf'),float('inf')\n",
    "#         self.kl_mmlvtru,self.kl_truvmml = float('inf'),float('inf')\n",
    "#         self.kl_smpvtru,self.kl_truvsmp = float('inf'),float('inf')\n",
    "\n",
    "#         if self.meta.logtruNz is not None:\n",
    "# #             vslogstk = self.meta.logstkNz-self.meta.logtruNz\n",
    "# #             self.vslogstk = np.dot(vslogstk,vslogstk)\n",
    "# #             vslogmap = self.meta.logmapNz-self.meta.logtruNz\n",
    "# #             self.vslogmap = np.dot(vslogmap,vslogmap)\n",
    "# # #             vslogexp = self.meta.logexpNz-self.meta.logtruNz\n",
    "# # #             self.vslogexp = np.dot(vslogexp,vslogexp)\n",
    "\n",
    "# #             self.cslogstk = np.average((self.meta.logstkNz-self.meta.logtruNz)**2)\n",
    "# #             self.cslogmap = np.average((self.meta.logmapNz-self.meta.logtruNz)**2)\n",
    "# # #             self.cslogexp = np.average((self.meta.logexpNz-self.meta.logtruNz)**2)\n",
    "\n",
    "#             self.kl_stkvtru,self.kl_truvstk = calckl(self.meta.bindifs,self.meta.logstkNz,self.meta.logtruNz)\n",
    "#             self.kl_mapvtru,self.kl_truvmap = calckl(self.meta.bindifs,self.meta.logmapNz,self.meta.logtruNz)\n",
    "#             self.kl_expvtru,self.kl_truvexp = calckl(self.meta.bindifs,self.meta.logexpNz,self.meta.logtruNz)\n",
    "#             self.kl_intvtru,self.kl_truvint = calckl(self.meta.bindifs,self.meta.logintNz,self.meta.logtruNz)\n",
    "#             self.kl_mmlvtru,self.kl_truvmml = calckl(self.meta.bindifs,self.meta.logmmlNz,self.meta.logtruNz)\n",
    "#             self.kl_smpvtru,self.kl_truvsmp = [],[]\n",
    "\n",
    "# #         if self.meta.truNz is not None:\n",
    "# #             vsstk = meta.stkNz-meta.truNz\n",
    "# #             self.vsstk = np.dot(vsstk,vsstk)\n",
    "# #             vsmap = meta.mapNz-meta.truNz\n",
    "# #             self.vsmap = np.dot(vsmap,vsmap)\n",
    "# # #             vsexp = meta.expNz-meta.truNz\n",
    "# # #             self.vsexp = np.dot(vsexp,vsexp)\n",
    "\n",
    "# #             self.csstk = np.average((self.meta.stkNz-self.meta.truNz)**2)\n",
    "# #             self.csmap = np.average((self.meta.mapNz-self.meta.truNz)**2)\n",
    "# #             self.csexp = np.average((self.meta.expNz-self.meta.truNz)**2)\n",
    "\n",
    "#         outdict = {#'vslogstk': self.vslogstk,\n",
    "# #                    'vsstk': self.vsstk,\n",
    "# #                    'vslogmap': self.vslogmap,\n",
    "# #                    'vsmap': self.vsmap,\n",
    "# # #                    'vslogexp': self.vslogexp,\n",
    "# # #                    'vsexp': self.vsexp,\n",
    "# #                    'cslogstk': self.cslogstk,\n",
    "# #                    'csstk': self.csstk,\n",
    "# #                    'cslogmap': self.cslogmap,\n",
    "# #                    'csmap': self.csmap,\n",
    "# # #                    'cslogexp': self.cslogexp,\n",
    "# # #                    'csexp': self.csexp,\n",
    "#                    'kl_stkvtru': self.kl_stkvtru,\n",
    "#                    'kl_mapvtru': self.kl_mapvtru,\n",
    "#                    'kl_expvtru': self.kl_expvtru,\n",
    "#                    'kl_smpvtru': self.kl_smpvtru,\n",
    "#                    'kl_intvtru': self.kl_intvtru,\n",
    "#                    'kl_mmlvtru': self.kl_mmlvtru,\n",
    "#                    'kl_truvstk': self.kl_truvstk,\n",
    "#                    'kl_truvmap': self.kl_truvmap,\n",
    "#                    'kl_truvexp': self.kl_truvexp,\n",
    "#                    'kl_truvsmp': self.kl_truvsmp,\n",
    "#                    'kl_truvint': self.kl_truvint,\n",
    "#                    'kl_truvmml': self.kl_truvmml\n",
    "#               }\n",
    "#         if self.meta.plotonly == False:\n",
    "#             with open(os.path.join(self.meta.topdir,'stat_chains.p'),'wb') as statchains:\n",
    "#                 cpkl.dump(outdict,statchains)\n",
    "\n",
    "#     def compute(self, ydata):#ntimes*nwalkers*nbins\n",
    "\n",
    "#         self.ydata = ydata\n",
    "#         self.eydata = np.exp(self.ydata)\n",
    "\n",
    "# #         print('about to write samples to file: '+str(self.meta.key.burnin))\n",
    "# #         if self.meta.key.burnin == False:\n",
    "\n",
    "#         with open(self.meta.samples,'ab') as csvfile:\n",
    "#             out = csv.writer(csvfile,delimiter=' ')\n",
    "#             for w in xrange(self.meta.nwalkers):\n",
    "#                 out.writerows(self.ydata[w])#[[x for x in row] for row in self.ydata])\n",
    "# #             print(str(self.meta.key.burnin)+'wrote samples to file')\n",
    "# #         else:\n",
    "# #             print('not writing samples to file because still burning in: '+str(self.meta.key.burnin))\n",
    "\n",
    "#         y = np.swapaxes(self.ydata.T,0,1).T#nwalkers*nbins*ntimes\n",
    "#         ey = np.swapaxes(self.eydata.T,0,1).T#np.exp(y)\n",
    "\n",
    "#         if self.meta.logtruNz is None:\n",
    "#             my = np.array([[[sum(by)/len(by)]*self.meta.ntimes for by in wy] for wy in y])#nwalkers*nbins*ntimes\n",
    "#             mey = np.array([[[sum(bey)/len(bey)]*self.meta.ntimes for bey in wey] for wey in ey])#nwalkers*nbins*ntimes\n",
    "#         else:\n",
    "#             my = np.array([[[k]*self.meta.ntimes for k in self.meta.logtruNz]]*self.meta.nwalkers)#nwalkers*nbins*ntimes\n",
    "#             mey = np.array([[[k]*self.meta.ntimes for k in self.meta.truNz]]*self.meta.nwalkers)#nwalkers*nbins*ntimes\n",
    "\n",
    "#         self.sy = np.swapaxes((y-my),1,2)#nwalkers*ntimes*nbins to #nwalkers*nbins*ntimes\n",
    "#         self.sey = np.swapaxes((ey-mey),1,2)\n",
    "\n",
    "#         self.var_ls = self.calcvar(self.var_ls,self.sy)\n",
    "#         self.var_s = self.calcvar(self.var_s,self.sey)\n",
    "#         self.chi_ls = self.calcchi(self.chi_ls,self.sy,self.ydata)\n",
    "#         self.chi_s = self.calcchi(self.chi_s,self.sey,self.eydata)\n",
    "\n",
    "#         with open(os.path.join(self.meta.topdir,'stat_chains.p'),'rb') as indict:\n",
    "#             outdict = cpkl.load(indict)\n",
    "\n",
    "# #         outdict['kl_smpvtru'] = np.array(self.kl_smpvtru)\n",
    "# #         outdict['kl_truvsmp'] = np.array(self.kl_truvsmp)\n",
    "#         #outdict['tot_var_ls'] = self.tot_var_ls\n",
    "#         #outdict['tot_var_s'] = self.tot_var_s\n",
    "#         outdict['var_ls'] = self.var_ls\n",
    "#         outdict['var_s'] = self.var_s\n",
    "#         #outdict['tot_chi_ls'] = self.tot_chi_ls\n",
    "#         #outdict['tot_chi_s'] = self.tot_chi_s\n",
    "#         outdict['chi_ls'] = self.chi_ls\n",
    "#         outdict['chi_s'] = self.chi_s\n",
    "\n",
    "#         with open(os.path.join(self.meta.topdir,'stat_chains.p'),'wb') as indict:\n",
    "#             cpkl.dump(outdict,indict)\n",
    "#             #print('after addition'+str(outdict))\n",
    "# #         return { 'vslogstk': self.vslogstk,\n",
    "# #                'vsstk': self.vsstk,\n",
    "# #                'vslogmap': self.vslogmap,\n",
    "# #                'vsmap': self.vsmap,\n",
    "# #                'vslogexp': self.vslogexp,\n",
    "# #                'vsexp': self.vsexp,\n",
    "# #                'tot_var_ls': self.tot_var_ls,\n",
    "# #                'tot_var_s': self.tot_var_s,\n",
    "# #                'var_ls': self.var_ls,\n",
    "# #                'var_s': self.var_s\n",
    "# #               }\n",
    "\n",
    "#     def calcvar(self,var,s):\n",
    "#         \"\"\"variance of samples\"\"\"\n",
    "#         ans = np.average([[np.dot(s[w][i],s[w][i]) for i in xrange(len(s[w]))] for w in xrange(len(s))])\n",
    "#         var.append(ans)\n",
    "# #         var_ls = np.average([[np.dot(self.sy[w][i],self.sy[w][i]) for i in xrange(self.meta.ntimes)] for w in xrange(self.meta.nwalkers)])#/float(self.meta.nwalkers*self.meta.ntimes*self.meta.nbins)\n",
    "# #         var_s = np.average([[np.dot(self.sey[w][i],self.sey[w][i]) for i in xrange(self.meta.ntimes)] for w in xrange(self.meta.nwalkers)])#/float(self.meta.nwalkers*self.meta.ntimes*self.meta.nbins)\n",
    "# #         self.var_ls.append(var_ls)\n",
    "# #         self.var_s.append(var_s)\n",
    "#         #self.tot_var_ls = self.tot_var_ls+var_ls\n",
    "#         #self.tot_var_s = self.tot_var_s+var_s\n",
    "#         #print(self.meta.name+' var_ls='+str(self.var_ls))\n",
    "#         #print(self.meta.name+' var_s='+str(self.var_s))\n",
    "#         return(var)\n",
    "\n",
    "#     def calcchi(self,var,s,data):\n",
    "#         \"\"\"chi^2 (or Wald test) of samples\"\"\"\n",
    "#         v = np.sum([np.average([statistics.variance(walk) for walk in data.T[b]]) for b in xrange(len(data.T))])#abs(np.linalg.det(np.cov(flatdata)))\n",
    "#         ans = np.average(s**2)/v\n",
    "#         var.append(ans)\n",
    "\n",
    "# #         flatdata = np.array([self.ydata.T[b].flatten() for b in xrange(self.meta.nbins)])\n",
    "# #         eflatdata = np.exp(flatdata)\n",
    "\n",
    "# #         vy = np.sum([np.average([statistics.variance(walk) for walk in self.ydata.T[b]]) for b in xrange(self.meta.nbins)])#abs(np.linalg.det(np.cov(flatdata)))\n",
    "# #         vey = np.sum([np.average([statistics.variance(walk) for walk in self.eydata.T[b]]) for b in xrange(self.meta.nbins)])#abs(np.linalg.det(np.cov(eflatdata)))\n",
    "\n",
    "# #         chi_ls = np.average(self.sy**2)/vy#np.average(sp.stats.chisquare(flatdata.T)[0])#float(self.meta.nwalkers*self.meta.ntimes*self.meta.nbins*vy)\n",
    "# #         chi_s = np.average(self.sey**2)/vey#np.average(sp.stats.chisquare(eflatdata.T)[0])#float(self.meta.nwalkers*self.meta.ntimes*self.meta.nbins*vey)\n",
    "# #         self.chi_ls.append(chi_ls)\n",
    "# #         self.chi_s.append(chi_s)\n",
    "# #         #self.tot_chi_ls = self.tot_chi_ls+chi_ls\n",
    "# #         #self.tot_chi_s = self.tot_chi_s+chi_s\n",
    "# #         print(self.meta.name+' chi_ls='+str(self.chi_ls))\n",
    "# #         print(self.meta.name+' chi_s='+str(self.chi_s))\n",
    "#         return(var)\n",
    "\n",
    "# class stat_probs(calcstats):\n",
    "#     \"\"\"\n",
    "#     calculates statistics requiring only probabilities:  log posterior probability for alternatives, variance of probabilities\n",
    "#     \"\"\"\n",
    "#     def __init__(self, meta):\n",
    "#         calcstats.__init__(self, meta)\n",
    "#         #self.summary = 0\n",
    "#         self.name = 'probs'\n",
    "\n",
    "# #         # calculating log likelihood ratio test statistic for each relative to truth (and true relative to prior)\n",
    "#         if self.meta.logtruNz is not None:\n",
    "#             self.lp_tru = self.meta.postdist.lnprob(self.meta.logtruNz)\n",
    "#             self.lik_tru = self.meta.postdist.lnlike(self.meta.logtruNz)\n",
    "#         else:\n",
    "#             self.lp_tru = self.meta.postdist.lnprob(self.meta.mean)\n",
    "#             self.lik_tru = self.meta.postdist.lnlike(self.meta.mean)\n",
    "\n",
    "#         self.lp_int = self.meta.postdist.lnprob(self.meta.logintNz)\n",
    "#         self.lp_stk = self.meta.postdist.lnprob(self.meta.logstkNz)\n",
    "#         self.lp_map = self.meta.postdist.lnprob(self.meta.logmapNz)\n",
    "#         self.lp_exp = self.meta.postdist.lnprob(self.meta.logexpNz)\n",
    "#         self.lp_mml = self.meta.postdist.lnprob(self.meta.logmmlNz)\n",
    "\n",
    "#         self.var_y = []\n",
    "\n",
    "#         outdict = {'var_y': self.var_y,\n",
    "#                    'lp_tru': self.lp_tru,\n",
    "#                    'lp_int': self.lp_int,\n",
    "#                    'lp_stk': self.lp_stk,\n",
    "#                    'lp_map': self.lp_map,\n",
    "#                    'lp_exp': self.lp_exp,\n",
    "#                    'lp_mml': self.lp_mml\n",
    "#                   }\n",
    "\n",
    "#         if self.meta.plotonly == False:\n",
    "#             with open(os.path.join(self.meta.topdir,'stat_probs.p'),'wb') as statprobs:\n",
    "#                 cpkl.dump(outdict,statprobs)\n",
    "\n",
    "#     def compute(self, ydata):\n",
    "#         y = np.swapaxes(ydata,0,1).T\n",
    "#         var_y = sum([statistics.variance(y[w]) for w in xrange(self.meta.nwalkers)])/self.meta.nwalkers\n",
    "#         #self.llr_smp.append((2.*np.max(lik_y)-2.*self.ll_tru))\n",
    "#         self.var_y.append(var_y)\n",
    "#         # self.summary = self.summary+var_y\n",
    "\n",
    "#         with open(os.path.join(self.meta.topdir,'stat_probs.p'),'rb') as indict:\n",
    "#             outdict = cpkl.load(indict)\n",
    "\n",
    "#         outdict['var_y'] = self.var_y\n",
    "\n",
    "#         with open(os.path.join(self.meta.topdir,'stat_probs.p'),'wb') as statprobs:\n",
    "#             cpkl.dump(outdict,statprobs)\n",
    "\n",
    "# #         return { #'summary': self.summary,\n",
    "# #                  'var_y': self.var_y,\n",
    "# #                  'lp_tru': self.lp_tru,\n",
    "# # #                  'lp_stk': self.lp_stk,\n",
    "# # #                  'lp_map': self.lp_map,\n",
    "# # # #                  'lp_exp': self.lp_exp\n",
    "# #                  'lp_mml': self.lp_mml\n",
    "# #                }\n",
    "\n",
    "# # class stat_fracs(calcstats):\n",
    "# #     \"\"\"\n",
    "# #     calculates summary statistics on acceptance fractions\n",
    "# #     \"\"\"\n",
    "# #     def __init__(self, meta):\n",
    "# #         calcstats.__init__(self, meta)\n",
    "# #         self.var_y = []\n",
    "# #         self.name = 'fracs'\n",
    "# #     def compute(self, ydata):\n",
    "# #         y = ydata.T\n",
    "# #         var_y = statistics.variance(y)\n",
    "# #         self.var_y.append(var_y)\n",
    "# #         return {'var_y': self.var_y}\n",
    "\n",
    "# class stat_times(calcstats):\n",
    "#     \"\"\"\n",
    "#     calculates summary statistics on autocorrelation times\n",
    "#     \"\"\"\n",
    "#     def __init__(self, meta):\n",
    "#         calcstats.__init__(self, meta)\n",
    "#         self.var_y = []\n",
    "#         self.name = 'times'\n",
    "#     def compute(self, ydata):\n",
    "#         y = ydata.T\n",
    "#         var_y = np.var(y)\n",
    "#         self.var_y.append(var_y)\n",
    "#         return {'var_y': self.var_y}\n",
    "\n",
    "# def cft(xtimes,lag):\n",
    "#     \"\"\"calculate autocorrelation times since emcee sometimes fails\"\"\"\n",
    "#     lent = len(xtimes)-lag\n",
    "#     allt = xrange(lent)\n",
    "#     ans = np.array([xtimes[t+lag]*xtimes[t] for t in allt])\n",
    "#     return ans\n",
    "\n",
    "# def cf(xtimes):#xtimes has ntimes elements\n",
    "#     cf0 = np.dot(xtimes,xtimes)\n",
    "#     allt = xrange(len(xtimes)/2)\n",
    "#     cf = np.array([sum(cft(xtimes,lag)[len(xtimes)/2:]) for lag in allt])/cf0\n",
    "#     return cf\n",
    "\n",
    "# def cfs(x,mode):#xbinstimes has nbins by ntimes elements\n",
    "#     if mode == 'walkers':\n",
    "#         xbinstimes = x\n",
    "#         cfs = np.array([sum(cf(xtimes)) for xtimes in xbinstimes])/len(xbinstimes)\n",
    "#     if mode == 'bins':\n",
    "#         xwalkerstimes = x\n",
    "#         cfs = np.array([sum(cf(xtimes)) for xtimes in xwalkerstimes])/len(xwalkerstimes)\n",
    "#     return cfs\n",
    "\n",
    "# def acors(xtimeswalkersbins,mode):\n",
    "#     if mode == 'walkers':\n",
    "#         xwalkersbinstimes = np.swapaxes(xtimeswalkersbins,1,2)#nwalkers by nbins by nsteps\n",
    "#         taus = np.array([1. + 2.*sum(cfs(xbinstimes,mode)) for xbinstimes in xwalkersbinstimes])#/len(xwalkersbinstimes)# 1+2*sum(...)\n",
    "#     if mode == 'bins':\n",
    "#         xbinswalkerstimes = xtimeswalkersbins.T#nbins by nwalkers by nsteps\n",
    "#         taus = np.array([1. + 2.*sum(cfs(xwalkerstimes,mode)) for xwalkerstimes in xbinswalkerstimes])#/len(xwalkersbinstimes)# 1+2*sum(...)\n",
    "#     return taus\n",
    "\n",
    "# def calckl(difs,lqn,lpn):\n",
    "#     \"\"\"KL Divergence test\"\"\"\n",
    "#     pn = np.exp(lpn)*difs\n",
    "#     qn = np.exp(lqn)*difs\n",
    "#     p = pn/np.sum(pn)\n",
    "#     q = qn/np.sum(qn)\n",
    "#     logp = um.safelog(p)\n",
    "#     logq = um.safelog(q)\n",
    "#     klpq = np.sum(p*(logp-logq))\n",
    "#     klqp = np.sum(q*(logq-logp))\n",
    "#     return(klpq,klqp)\n",
    "\n",
    "# def calcbfe(samples):\n",
    "#     with open(samples,'rb') as csvfile:\n",
    "#         tuples = (line.split(None) for line in csvfile)\n",
    "#         alldata = [[float(pair[k]) for k in range(0,len(pair))] for pair in tuples][1:]\n",
    "#         nbins = len(alldata[0])\n",
    "#         alldata = np.array(alldata).T\n",
    "\n",
    "#     locs,scales = [],[]\n",
    "#     for k in xrange(nbins):\n",
    "#         y_all = alldata[k].flatten()\n",
    "#         loc,scale = sp.stats.norm.fit_loc_scale(y_all)\n",
    "#         locs.append(loc)\n",
    "#         scales.append(scale)\n",
    "#     locs = np.array(locs)\n",
    "#     scales = np.array(scales)\n",
    "#     return(locs,scales)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
