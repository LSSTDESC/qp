{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kullback-Leibler Divergence\n",
    "\n",
    "In this notebook, we try and gain some intuition about the magnitude of the KL divergence by computing its value between two Gaussian PDFs as a function of the \"tension\" between them.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You'll need the `qp` package and its dependencies (notably `scipy` and matplotlib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The Kullback-Leibler divergence between probability distributions $P$ and $Q$ is:\n",
    "\n",
    "$D(P||Q) = \\int_{-\\infty}^{\\infty} \\log \\left( \\frac{P(x)}{Q(x)} \\right) P(x) dx$\n",
    "\n",
    "The wikipedia page for the KL divergence gives the following useful interpretation of the KLD:\n",
    "\n",
    "> KL divergence is a measure of the difference between two probability distributions P and Q. It is not symmetric in P and Q. In applications, P typically represents ... a precisely calculated theoretical distribution, while Q typically represents ... [an] approximation of P.\n",
    ">\n",
    "> Specifically, the Kullback–Leibler divergence from Q to P, denoted DKL(P‖Q), is a measure of the information gained when one revises one's beliefs from ... Q to ... P. In other words, it is the amount of information lost when Q is used to approximate P.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Gaussian Illustration\n",
    "\n",
    "\"Information\" is not a terribly familiar quantity to most of us, so lets compute the KLD between two Gaussians:\n",
    "\n",
    "* The \"True\" 1D Gaussian PDF, $P(x)$, of unit width and central value 0\n",
    "\n",
    "* An \"approximating\" 1D Gaussian PDF, $Q$, of width $\\sigma$ and centroid $x_0$\n",
    "\n",
    "How does the KLD between these PDFs vary with offset $x_0$ and width $\\sigma$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import qp\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = qp.PDF(truth=sps.norm(loc=0.0, scale=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x0, sigma = 2.0, 1.0\n",
    "Q = qp.PDF(truth=sps.norm(loc=x0, scale=sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infinity = 100.0\n",
    "D = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "print np.round(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. Two equal-width Gaussians overlapping at their 1-sigma points have a KLD of 2 nats. \n",
    "\n",
    "> The unit of information here is a \"nat\" rather than a \"bit\" because `qp` uses a natural logarithm in its KLD calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the two Gaussians are perfectly aligned, but the approximation is broader than the truth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x0, sigma = 0.0, 4.37\n",
    "Q = qp.PDF(truth=sps.norm(loc=x0, scale=sigma))\n",
    "D = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "print D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. Two concentric Gaussian PDFs differing in width by a factor of 4.37 have a KLD of 1 nat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tension between PDFs\n",
    "\n",
    "Two measurements that disagree with each other will lead to parameter PDFs that have different cenrtroids. Let's tabulate the KLD for a range of distribution offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "separations = np.linspace(0.0,15.0,16)\n",
    "D = np.empty_like(separations)\n",
    "D = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "print np.round(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma = 1.0\n",
    "infinity = 100.0\n",
    "\n",
    "for k,x0 in enumerate(separations):\n",
    "    Q = qp.PDF(truth=sps.norm(loc=x0, scale=sigma))\n",
    "    D[k] = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "    \n",
    "print zip(separations, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(separations, D, color='k', linestyle='-', lw=2.0, alpha=1.0, label='Width=1.0')\n",
    "plt.xlabel('Separation between Gaussians')\n",
    "plt.ylabel('KL divergence (nats)')\n",
    "l = plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For separations greater than about 7 sigma, numerical precision starts to matter: the overlap integral out here is smaller than machine precision. `qp` uses a `safelog` function that replaces values smaller than the system threshold value with that threshold; the log of that threshold is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print np.log(sys.float_info.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presumably what matters is the \"tension\" between the two distributions: the separation in units of the combined width, $\\sigma_0^2 + \\sigma^2$. This quantity comes up often in discussions of dataset combination, where the difference in centroids of two posterior PDFs needs to be expressed in terms of their widths: the quadratic sum makes sense in this context, since it would appear in the product of the two likelihoods were they to be combined.\n",
    "\n",
    "For a few different widths, let's plot KLD vs tension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infinity = 100.0\n",
    "widths = np.array([1.0,1.5,2.0,2.5,3.0,3.5,4.0]) \n",
    "separations = np.linspace(0.0,7.0,15)\n",
    "\n",
    "D = np.zeros([7,len(separations)])\n",
    "tensions = np.empty_like(D)\n",
    "\n",
    "for j,sigma in enumerate(widths):\n",
    "    \n",
    "    for k,x0 in enumerate(separations):\n",
    "        Q = qp.PDF(truth=sps.norm(loc=x0, scale=sigma))\n",
    "        D[j,k] = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "        tensions[j,k] = x0 / np.sqrt(sigma*sigma + 1.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tensions[0,:]\n",
    "y = x**2\n",
    "plt.plot(x, y, color='gray', linestyle='-', lw=8.0, alpha=0.5, label='$t^2$')\n",
    "\n",
    "plt.plot(tensions[0,:], D[0,:], color='black', linestyle='-', lw=2.0, alpha=1.0, label='Width=1.0')\n",
    "plt.plot(tensions[1,:], D[1,:], color='violet', linestyle='-', lw=2.0, alpha=1.0, label='Width=1,5')\n",
    "plt.plot(tensions[2,:], D[2,:], color='blue', linestyle='-', lw=2.0, alpha=1.0, label='Width=2.0')\n",
    "plt.plot(tensions[3,:], D[3,:], color='green', linestyle='-', lw=2.0, alpha=1.0, label='Width=2.5')\n",
    "plt.plot(tensions[4,:], D[4,:], color='yellow', linestyle='-', lw=2.0, alpha=1.0, label='Width=3.0')\n",
    "plt.plot(tensions[5,:], D[5,:], color='orange', linestyle='-', lw=2.0, alpha=1.0, label='Width=3.5')\n",
    "plt.plot(tensions[6,:], D[6,:], color='red', linestyle='-', lw=2.0, alpha=1.0, label='Width=4.0')\n",
    "plt.xlabel('Tension between Gaussians, $t$ (sigma)')\n",
    "plt.ylabel('KL divergence (nats)')\n",
    "l = plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The simple numerical experiment in this notebook suggests that:\n",
    "\n",
    "The KL divergence, in nats, between an approximating Gaussian and a true Gaussian is _approximately_ equal to the square of the tension between the two distributions, where tension $t$ is defined as\n",
    "\n",
    "## $t = \\frac{\\Delta x}{\\sqrt{\\left(\\sigma_0^2 + \\sigma^2\\right)}}$\n",
    "\n",
    "and has, in some sense, \"units\" of \"sigma\". The KLD is the information lost when using the approximation: the information loss rises in proprtion to the tension squared. An analytic derivation of this result would be welcome!\n",
    "\n",
    "We can perhaps take the KL divergence to be a generalized quantification of tension: the square root of the KLD between a PDF and its approximation, in nats, gives an approximate sense of the tension between the two distributions, in \"units\" of \"sigma\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
