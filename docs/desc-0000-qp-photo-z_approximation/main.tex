\RequirePackage{docswitch}
\setjournal{\flag}

\documentclass[\docopts]{\docclass}


\usepackage{lsstdesc_macros}

\usepackage{graphicx}
\graphicspath{{./}{./figures/}{.logos}}
\bibliographystyle{apj}


\newcommand{\textul}{\underline}
\newcommand{\qp}{\texttt{qp}}
\newcommand{\pz}{photo-$z$ PDF}
\newcommand{\Pz}{Photo-$z$ PDF}


\begin{document}

\title{ Approximating photo-z PDFs for large surveys }


\begin{abstract}

Upcoming and ongoing galaxy surveys will produce redshift probability 
distribution functions (PDFs) in addition to traditional photometric redshift 
(photo-$z$) point estimates.  However, the storage of \pz s may present a 
challenge with increasingly large catalogs, as we face a trade-off between the 
accuracy of subsequent science measurements and the storage cost.  This paper 
presents \qp, a Python package facilitating manipulation of approximations of 
1-dimensional PDFs, as suitable for \pz s.  We use \qp\ to investigate the 
performance of three PDF formats on two realistic datasets representative of 
upcoming surveys with different data qualities as a function of the number of 
stored parameters per \pz, using metrics of both individual \pz s and an 
estimator of the overall redshift distribution function.

\end{abstract}

\dockeys{methods: data analysis, catalogs, surveys}

\maketitlepost





\section{Introduction}
\label{sec:intro}


Ongoing and upcoming photometric galaxy surveys such as the Large Synoptic 
Survey Telescope (LSST) will observe tens of billions of galaxies without 
spectroscopic follow-up to obtain redshifts necessary for studies of cosmology 
and galaxy evolution, instead relying on the methods of photometric redshift 
(photo-$z$) estimation.  Photo-$z$s are subject to a number of systematic 
errors, some caused by the data analysis procedures and others intrinsic to the 
data itself.  Due to these issues, the photo-$z$ community has come to favor 
estimation of redshift probability distribution functions (PDFs), or \pz s, 
that include information about the potential for such systematic errors for 
each galaxy in the survey.  \Pz s are interim posterior distributions, as they 
are estimates of the probability of a galaxy's redshift conditioned on its 
photometric data and any assumptions made by the method producing it, though 
they are commonly written simply as $p(z)$.

Given the tremendous size of the surveys in question, storage of these 
probability distributions involves making difficult decisions.  Each survey 
seeks to create a catalog of \pz s balancing accuracy against storage 
footprint.  For example, the \pz\ catalog that LSST will release is limited to 
200 floating point numbers per galaxy, with plans to store \pz s derived by 
multiple methods.  \citep{juric_data_2017}
These matters were first addressed in \citet{carrasco_kind_sparse_2014} in the 
context of a single galaxy survey, a limited set of storage formats, and 
metrics restricted to that survey's intended \pz\ use case.  The number of 
stored parameters and the parametrization optimizing this choice, however, 
depend on the science requirements of the survey and the characteristics of the 
data.  This paper answers the question of \textit{how} these choices should be 
made in general, demonstrating the approach on diverse mock data and using 
mathematically motivated metrics of both the individual \pz s and a science 
application.  The publicly available \qp\ Python package is also presented here 
as a tool to enable each survey to optimize these choices.

In Sec. \ref{sec:methods}, we outline how \qp\ can be used to optimize the 
choice of storage format and catalog allocation.  In Sec. \ref{sec:data}, we 
describe the mock datasets on which we perform such an analysis.  We present 
the results of this procedure in Sec. \ref{sec:results} and make 
recommendations for the use of \qp\ by the photo-$z$ community in 
\ref{sec:conclusions}.








\section{Methods}
\label{sec:methods}



We have developed the \qp\ Python package to facilitate manipulation of \pz s.  
A \texttt{qp.PDF} object is defined by its parametrizations.  By way of 
interpolation, \qp\ can convert a representation of a \pz\ under one 
parametrization to a representation of that \pz\ under a different 
parametrization.  The currently supported parametrizations are described in 
Sec. \ref{sec:approx}.  \qp\ also includes a few built-in metrics of the 
accuracy of a representation of a \pz\ if its value under a given 
parametrization is designated as "true."  The currently implemented metrics are 
described in Sec. \ref{sec:metric}.

\subsection{Approximation Methods}
\label{sec:approx}

First, we establish a vocabulary for the definitions of approximation methods.  
Each \textit{parametrization} of a \pz\ is defined in terms of the 
\textit{format} function $\mathcal{F}$, \textit{metaparameters} comprising 
$\vec{C}$, and \textit{parameters} comprising $\vec{c}$.  Each parametrization 
in turn corresponds to a \textit{representation}
\begin{align}
  \label{eq:definition}
  \hat{p}_{\mathcal{F}, \vec{C}, \vec{c}}(z) &\equiv \mathcal{F}_{\vec{C}}(z; 
\vec{c})
\end{align}
of the \pz, denoted as $\hat{p}(z)$ for brevity.  We often employ interpolation 
schemes with a generic interpolator functions $F_{\vec{C}'}(z; z', p')$ that 
comes with its own metaparameters $\vec{C}'$.  The choice of interpolator may 
be made on a survey-by-survey basis.  \qp\ supports all interpolation options 
available to the \texttt{scipy.interpolate.interp1d} function.  However, we 
have chosen a default interpolation scheme for each format to maximize its 
performance, demonstrating the significance of the choices made in 
reconstructing a \pz\ from stored parameters.

\qp\ supports conversion of \pz\ catalogs between five formats: step functions, 
samples, quantiles, evaluations, and mixture model components.  These formats 
may be associated with any number $N_{f}$ of stored parameters $c_{i}$ per \pz, 
which are presumed to be floating point numbers unless otherwise specified.  
The metaparameters are a set of numbers $C_{i}$ necessary to convert the stored 
\pz\ parameters $\vec{c}$ into a probability distribution function over 
redshift.  Special cases of three of these formats are considered candidates 
for large survey \pz\ catalog storage: regular binning (Sec. \ref{sec:bins}), 
random samples (Sec. \ref{sec:samples}), and regular quantiles (Sec. 
\ref{sec:quantiles}), while the other two, evaluations on a grid and mixture 
model components, are used solely for internal manipulations within \qp.

The different \qp\ formats are illustrated in Fig. \ref{fig:qp} on a multimodal 
\pz\ with stored parameters.
\begin{figure}
  \includegraphics[width=0.9\columnwidth]{figures/demo_pz.png}
  \caption{\qp\ can approximate a continuous 1D PDF (solid black line) using 
the step function (orange dotted line), random sample (green dash-dotted line), 
and quantile formats (purple dashed line) with a specified number of stored 
parameters ($N_{f}=20$ in this case).
  \label{fig:qp}}
\end{figure}
For each format, we address the following questions:
\begin{itemize}
  \item When/where has this format appeared in the literature as a published 
catalog format, native \pz\ code output format, and science application input 
format?
  \item What exactly is stored under this format, per galaxy (the parameters) 
and per catalog (the metaparameters)?
  \item Beyond fidelity to the original \pz, what are the a priori strengths 
and weaknesses of this format?
\end{itemize}

Before delving into the formats compared in this study, we must address why we 
do not include the \texttt{SparsePz} sparse basis representation of 
\citet{carrasco_kind_sparse_2014}, which uses a mixture model of $N_{f}$ 
members of a library of $\sim10^{4}$ functions and has impressive compression 
properties.  However, a mixture model is only appropriate when the true \pz\ is 
in fact a mixture of the functions in the model, which we cannot in general 
check with real data.  Furthermore, decomposition with \texttt{SparsePZ} in 
particular does not enforce that the stored parametrization be a probability 
distribution in the mathematical sense of integrating to unity and, especially, 
always being positive semidefinite.  While normalizing a positive semidefinite 
function to integrate to unity is always possible (if the endpoints of 
integration are specified), one can motivate multiple schemes for enforcing 
nonnegativity that result in different reconstructions $\hat{p}(z)$.  To 
discourage misuse of \qp, we do not permit mixtures of functions that can take 
negative values at this time.  We anticipate including the sparse basis 
representation in \qp\ in the future when we can justify a choice of how to 
handle negative values.

\subsubsection{Regular Binning}
\label{sec:bins}

By far the most popular format for \pz s is that of a piecewise constant step 
function, also called a histogram binning.  It is the only format that has been 
used for public release of \pz\ catalogs \citep{tanaka_photometric_2017, 
sheldon_photometric_2012}; it is unclear whether this is a consequence or cause 
of the fact that it is the most common format for using \pz s in cosmological 
inference, as tomographic binning is a universal step between the \pz\ catalog 
and calculation of any two-point correlation function.

The metaparameters of the binned parametrization are the ordered list of 
redshifts $\vec{C} = (z_{1}, z_{2}, \dots, z_{N_{f}}, z_{N_{f}+1})$ serving as 
bin endpoints shared by all galaxies in the catalog, each adjacent pair of 
which is associated with a parameter $c_{i}=\int_{C_{i}}^{C_{i+1}}\ p(z)dz$.  
The histogram format assumes $p(z)=0$ when $z<C_{1}$ or $z>C_{N_{f}+1}$ leading 
to the normalization condition $\sum_{i} c_{i}(C_{i+1}-C_{i})) = 1$.  Note that 
this is not generally equivalent to the erroneous normalization condition 
$\sum_{i} c_{i} = 1$ commonly enforced in public catalogs.  The histogram 
format function $\mathcal{F}^{h}$ is the sum of a set of $N_{f}$ step 
functions, making the reconstructed estimator of the \pz
\begin{align}
  \label{eq:binned}
  \hat{p}^{h}(z) &= \sum_{i}^{N_{f}}\ c_{i} 
\left\{\begin{tabular}{cc}$1$&$C_{i}<z<C_{i+1}$\\
0&$z < C_{i}$\ or\ $z > C_{i+1}$\end{tabular}\right\},
\end{align}
where the step functions may be considered their own interpolators.  Here we 
only consider a regular binning, with $C_{i+1}=C_{i}+\delta_{f}$ for a constant 
$\Delta$, as this is the only type of binning that has been used in the 
literature.

The histogram format may be considered wasteful in terms of data storage: a 
\pz\ with a very compact (broad) probability distribution may have many 
parameters taking the same value $c_{i}\approx0$ 
($c_{i}\approx(C_{N_{f}+1}-C_{1})\delta_{f}^{-1}$) that are redundant in 
storage.  It also requires the researcher to choose the minimum and maximum 
possible redshifts $C_{1}$ and $C_{N_{f}+1}$ of the galaxy sample, which are 
unknown quantities, so it would be preferable to leave them unconstrained when 
setting up the catalog.

\subsubsection{Random Samples}
\label{sec:samples}

Samples are the native output format of many machine learning algorithms 
dependent on random choices, such as random forests.
Such approaches typically produce large numbers of samples, far more than can 
realistically be stored by any survey, so a subsample is commonly stored.

The parameters of the samples format are the set of $N_{f}$ samples 
$\vec{c}=(z_{1}, z_{2}, \dots, z_{N_{f}-1}, z_{N_{f}})$, where $C=N_{f}$ is an 
implicit metaparameter.  Though it is possible to construct a catalog where $C$ 
is not uniform over the catalog, we leave its investigation to future work, as 
it has not yet appeared in the literature.  The format function 
$\mathcal{F}^{s}$ is simply the interpolation scheme $F$.  In the tests 
presented here, $F$ is the Gaussian kernel density estimate (KDE) of 
\texttt{scipy.stats.gaussian\_kde} with the smoothing bandwidth $C'$ as a 
metaparameter.  The samples representation is then
\begin{align}
  \label{eq:sampled}
  \hat{p}^{s}(z) &= F_{C'}(z; \vec{c}).
\end{align}

Though samples are an obvious choice for \pz s with narrow features with high 
amplitude, a small number of samples from a broad \pz\ may boost the variance 
across the catalog.  The researcher must also choose an interpolation method to 
reconstruct a \pz\ from samples.

\subsubsection{Regular Quantiles}
\label{sec:quantiles}

One parametrization that has not previously been investigated is that of 
quantiles, which are defined in terms of the cumulative distribution function 
(CDF).  Under the quantile format, a \pz\ catalog shares $N_{f}$ ordered CDFs 
$\vec{C}=(q_{1}, q_{2}, \dots, q_{N_{f}-1}, q_{N_{f}})$.  Each galaxy's catalog 
entry is the vector of redshifts $\vec{c}=(z_{1}, z_{2}, \dots, z_{N_{f}-1}, 
z_{N_{f}})$ satisfying $CDF(c_{i})=C_{i}$, so the quantile format function 
$\mathcal{F}_{q}$ is the derivative of an interpolation of the inverse CDF 
$CDF^{-1}(C_{i})=c_{i}$ under the interpolation scheme $F$.  In this study, we 
test regular quantiles $C_{i}\equiv i(N_{f}+1)^{-1}$ using a linear 
interpolation scheme.  The format function is then the convolution 
$\mathcal{F}^{q}=F(CDF^{-1})$, making the quantile representation
\begin{align}
  \label{eq:quantiles}
  \hat{p}_{q}(z) &= F(z; \vec{c}, \frac{\Delta\vec{C}}{\Delta\vec{c}}).
\end{align}
In our tests, the quantiles are regular, such that $C_{i}\equiv i\bar{C}$, 
where $\bar{C}\equiv(N_{f}+1)^{-1}$, but \qp\ does not require that this be so. 
 Additionally, in the tests presented here, we use a linear interpolator by 
default for the quantile format.


We suggest that the quantile format is appropriate for \pz\ storage because it 
allocates storage evenly in the space of probability density, as opposed to the 
histogram format storing data evenly spaced in redshift and the samples format 
storing data randomly in probability density.  As with the samples 
representation, an interpolation function must be chosen for reconstructing the 
\pz\ from the stored parameters.  Depending on the native \pz\ output format, 
converting to the quantile format may require $N_{f}$ numerical optimizations.





\subsection{Comparison Metrics}
\label{sec:metric}

In contrast to popular \pz\ metrics in the literature, we aim to probe how 
closely a PDF reconstructed from a limited set of parameters approximates a 
true PDF without reference to a galaxy's true redshift.  (For a demonstration 
of how one might approach the different problem of evaluating the accuracy of a 
\pz\ relative to a true redshift, see Schmidt, et al. (in prep.).)  To quantify 
how much more information is in a true PDF $P(z)$ than an approximate PDF 
$\hat{P}(z)$, we use the Kullback-Leibler divergence (KLD), defined as
\begin{align}
  \label{eq:kld}
  KLD[P(z) || \hat{P}(z)] &= \int_{-\infty}^{\infty}\ P(z)\ 
\log\left[\frac{P(z)}{\hat{P}(z)}\right]\ dz\\
  &\approx \delta_{ff}\sum_{z=z_{1}}^{z_{N_{ff}}}\ P(z)\ 
\log\left[\frac{P(z)}{\hat{P}(z)}\right],
\end{align}
where $\log$ is the natural logarithm throughout this paper.  Because there is 
in general no closed-form expression for the KLD, we calculate the KLDs in this 
paper using evaluations of the PDF under each format on a very fine, regular 
grid $(z_{1}, z_{2}, \dots, z_{N_{ff}-1}, z_{N_{ff}})$ with resolution 
$\delta_{ff}\ll\delta_{f}$.  We review the properties of the KLD and deepen 
intuition for it in App. \ref{sec:kld}.

\Pz s have thus far been used almost exclusively to estimate the redshift 
distribution function $n(z)$ necessary for calculating the correlation 
functions used by many cosmological probes.  The most common way to estimate 
the redshift distribution function is to sum the \pz s according to
\begin{align}
  \label{eq:nz}
  \hat{n}(z) &\equiv \frac{1}{N_{g}}\ \sum_{k=1}^{N_{g}}\ \hat{p}_{k}(z),
\end{align}
in terms of the number $N_{g}$ of galaxies $k$ in the catalog, where the 
estimator is normalized so that it, too, is a probability distribution (though 
in the literature it is often subject to the fallacy conflating a sum and an 
integral, first mentioned in Sec. \ref{sec:bins}).  We do not recommend this 
approach to estimating the redshift distribution (see Malz and Hogg, et al. (in 
prep.) for a mathematically principled alternative), however, we use it under 
the assumption that any metric calculated on a more principled estimator of the 
redshift distribution function will have similar behavior with respect to the 
parametrization of the \pz\ catalog as that of the stacked estimator.  Our 
primary metric is the KLD from the stacked estimator of a catalog of 
evaluations of reconstructed \pz s to the stacked estimator of a catalog of 
evaluations of the true \pz s.

For most galaxy surveys producing \pz\ catalogs, there is potential value not 
only in obtaining $\hat{n}(z)$ but also in recovering individual \pz s that, 
for example, may be used as the basis for targeting spectroscopic follow up for 
a variety of science applications.  For this purpose, we also calculate the KLD 
for each individual \pz\ in our catalogs and characterize the PDF of KLD values 
$\hat{p}(KLD)$ by their first, second, and third moments, or, the mean, 
variance, and kurtosis.  We use these aggregate statistics to observe how the 
approximate \pz s for some dataset vary with the choice of parametrization.





\section{Photo-z Test Data}
\label{sec:data}

With the expectation that \qp\  may indicate a different optimal result for 
different datasets, we apply it to two mock datasets with different data 
quality properties.  For example, one can expect storing a small number of 
samples to be sufficient for unimodal \pz s with low variance but disastrous 
for broader \pz s, or storing the components of a Gaussian mixture model to 
only be appropriate.  For this reason, we demonstrate a procedure for making a 
good choice of format and number of parameters on a pair of mock datasets 
intended to be realistic projections of anticipated data from upcoming surveys 
with different data quality.

Both datasets were fit using the publicly available Bayesian Photometric 
Redshift (BPZ) code \citep{benitez_bayesian_2000}, which employs spectral 
energy distribution (SED) fitting to a template library.  However, the choice 
of \pz\ estimation method is not relevant to this study; so long as the mock 
\pz s are realistically complex, meaning they take shapes like those we expect 
to see in accurate \pz s from real datasets with similar photometric 
properties, it does not matter how accurately they describe the probability 
distribution of galaxy redshifts given their photometric data.  We only seek to 
optimize the fidelity of the stored \pz\ relative to the \pz\ output by a 
representative \pz\ fitting code.  (See \citet{tanaka_photometric_2017}, 
Schmidt, et al. (in prep.) for other work comparing the accuracy of \pz s 
produced by different methods.)  As BPZ is a widely used and well established 
method, we assume that the \pz s produced by it are of realistic complexity.

The mock datasets considered here were obtained in a gridded parametrization, 
the default for BPZ output, with $N_{ff}>200$, the maximum number of floating 
point numbers available for characterizing all photo-z information obtained by 
LSST.  \citep{juric_data_2017}  Because we believe that each galaxy has an 
underlying redshift interim posterior probability distribution that is a 
continuous function, to which the output of this \pz\ code is itself a 
high-resolution approximation, we fit each \pz\ in the catalog of densely 
evaluated gridded evaluations with a Gaussian mixture model and designate that 
as the true \pz\ $p(z)$.

\begin{figure}
  \includegraphics[width=0.9\columnwidth]{figures/lsst_pzs.png}\\
  \includegraphics[width=0.9\columnwidth]{figures/euclid_pzs.png}
  \caption{The shapes of the \pz s\ in the two datasets considered vary 
substantially despite using the same \pz\ method, due to the differences in the 
quality of the photometry.  Different parametrizations may perform better on 
some \pz\ shapes than others.  Top panel: The lower quality photometry of the 
ground-based optical photometry catalog yields broader, multimodal \pz s. 
Bottom panel: The higher quality photometry of the space-based optical + IR 
photometry catalog yields narrow, unimodal \pz s.
  \label{fig:pzs}}
\end{figure}

\subsection{Ground-based optical mock data}
\label{sec:LSST}


In order to simulate the expected LSST galaxy sample, we use the 
Buzzard-highres-v1.0 mock galaxy catalog
, which adds galaxies with SEDs drawn from an empirical library of 
$\sim500,000$ SEDs from the Sloan Digital Sky Survey  (SDSS)
Given an SED, redshift, and absolute r-band magnitude for each galaxy, we 
compute the expected apparent magnitudes and magnitude errors in the six 
broadband LSST filters (ugrizy), assuming full 10-year depth of the survey 
using the simple model of \citet{ivezic_lsst:_2008}.  The catalog contains 
111,171 galaxies to a depth of $i<26.9$, 1.5 magnitudes deeper than the 
expected "LSST Gold sample'' of galaxies that will have S/N$\gtrsim$30 in 
multiple bands.

In implementing BPZ, we create a custom Bayesian prior using a subset of the 
Buzzard-highres-v1.0 catalog, and create a spanning template set via a simple 
k-means clustering algorithm, selection 100 of the SDSS SEDs used in creating 
the Buzzard catalog.  BPZ outputs the marginalized redshift posterior on a 
regular grid of test redshifts, in this case we have 211 points spanning 
$0.005\leq z\leq2.105$.

Even with six filters spanning the optical, there are known degeneracies 
(e.~g.~the Lyman/Balmer break degeneracy) that lead us to expect the presence 
of multi-modal \pz's; some examples are shown in the top panel of 
Fig.~\ref{fig:pzs}.  We produce "true" underlying \pz s by fitting a 
five-component Gaussian mixture model to each \pz\ output by BPZ.


\subsection{Space-based optical + IR mock data}
\label{sec:Euclid}


We start with a 30,000 object subset of the same simulated galaxy catalog used 
for LSST photometric redshift experiments by Graham, et al. (in prep.), which 
is based on the Millennium simulation \citep{springel_simulations_2005}, in 
particular the LC DEEP Gonzalez2014a
catalog based on the galaxy formation models of \cite{gonzalez-perez_how_2014}, 
and was created using the lightcone construction techniques described by 
\cite{merson_lightcone_2013}.  We limit the sample to galaxies with a catalog 
$i$-band magnitude of $i<25$ and true redshifts $z<3.5$. As in Graham, et al. 
(in prep.) we simulate observed apparent magnitudes from the true catalog 
magnitudes by adding a normal random scatter with a standard deviation equal to 
the predicted magnitude error for each galaxy (from Section 3.2.1. of 
\citealt{ivezic_lsst:_2008}, using the software of 
\citealt{connolly_end--end_2014}, assuming a mean airmass of 1.2 and a 10-year 
accumulation of 56, 80, 184, 184, 160, and 160 visits in filters $ugrizy$, 
respectively).  We also ignore any magnitudes fainter than the predicted 
10-year limiting magnitudes in each filter, $u<26.1$, $g<27.4$, $r<27.5$, 
$z<26.1$, and $y<24.9$, as a realistic simulation of non-detections.

The \pz s for this simulated catalog use the CFHTLS set of spectra 
\citep{ilbert_accurate_2006} and all of the default parameter settings for BPZ, 
except that we impose a maximum photometric redshift of 3.5 and allow BPZ to 
use the $i$-band as a magnitude prior during the photo-$z$ fit. The \pz s from 
BPZ are in the form of $N_{ff} = 351$ floating point numbers represing the 
probability on a regular grid of redshifts $0.01 < z_{\rm phot} < 3.51$.  We 
produce "true" underlying \pz s by fitting a three-component Gaussian mixture 
model to each \pz output by BPZ.



\section{Results \& Discussion}
\label{sec:results}


In this study, we perform tests comparing the parametrizations of Sec. 
\ref{sec:approx} as a function of the number of parameters per galaxy.  The 
tests are conducted using the functionality of the \texttt{qp.Ensemble} class 
that is a wrapper for collections of \texttt{qp.PDF} objects.


\subsection{Individual \pz s}
\label{sec:individual}

Parametrizations are compared on the basis of the distributions of the KLD of 
Sec. \ref{sec:metric} calculated over all \pz s in the ensemble.  An example of 
the distribution of the KLD is shown in Fig. \ref{fig:individual}.

\begin{figure}
  \includegraphics[width=0.9\columnwidth]{figures/lsst_individual.png}
  \caption{Here one can see the general shape of the distribution of log-KLD 
values for $N_{g}=100$ \pz s from the Optical dataset and one number of 
parameters $N_{f}=10$ over the quantiles (purple with dashed border), samples 
(green with dash-dotted border), and histogram (orange with dotted border) 
formats.  We note that the quantile format has a narrow distribution with a 
higher KLD, whereas the samples and histogram formats have broader 
distributions with generally lower KLDs.
  \label{fig:individual}}
\end{figure}

To distill what is observed in plots like Fig. \ref{fig:individual} for both 
datasets and all numbers of parameters, we compare the moments of the 
distributions of metric values for the individual \pz s under each 
parametrization, summarized in Fig. \ref{fig:moments}.

\begin{figure}
  \includegraphics[width=0.9\columnwidth]{lsst_moments.png}\\
  \includegraphics[width=0.9\columnwidth]{euclid_moments.png}
  \caption{The mean (star), variance ($+$), and kurtosis ($\times$) of the KLD 
distributions are plotted for each dataset over all parametrizations (quantiles 
in purple, samples in green, and histogram in orange).  Top panel: The optical 
dataset has\dots.  Bottom panel: The optical+IR dataset has\dots.
  \label{fig:moments}}
\end{figure}

While it is obvious that one would like the mean (first moment) of the KLD 
distribution to be low, interpretation of higher-order moments is less clear.  
In a science application that is robust to \pz\ outliers, a parametrization 
with a high variance (second moment) may be acceptable, whereas in another 
science application that simply requires well-characterized errors could 
tolerate a higher mean in exchange for a lower variance.  To meaningfully 
interpret the KLD of individual \pz s, it will be necessary for those using \pz 
s in their science to calculate the requirements on the acceptable degree of 
information loss.

\subsection{Stacked $\hat{n}(z)$ estimator}
\label{sec:stacked}

Parametrizations are also compared by the accuracy of their stacked redshift 
distribution estimator $\hat{n}(z)$ relative to that of the \pz s in their 
original format.  Fig. \ref{fig:stacked} shows a typical example of the stacked 
estimator based on \pz s reconstructed from a set number of stored parameters 
under each format, as well as the KLD for each format.

\begin{figure}
  \includegraphics[width=0.9\columnwidth]{figures/euclid_stacked.png}
  \caption{We show an example of the stacked estimator of the redshift 
distribution for the Optical+IR dataset with one number of parameters 
$N_{f}=10$ across all formats along with the KLD for each format.  The most 
striking characteristic of $\hat{n}(z)$ with a relatively small number of 
parameters on a small number of galaxies is the coarseness of the histogram 
format (orange dotted line) relative to the quantile format (purple dashed 
line) and samples format (green dash-dotted line), both of which are fairly 
close to $\hat{n}(z)$ derived from evaluating the true \pz s (thick gray line).
  \label{fig:stacked}}
\end{figure}

The KLD values based on plots like Fig. \ref{fig:stacked} over all 
parametrizations are collected and plotted in Fig. \ref{fig:kld}.  Error 
regions are based on $10$ subsamples of $N_{g}=1000$ \pz s from the full 
catalogs of Sec. \ref{sec:data}.  The results can be interpreted in terms of 
absolute accuracy of the redshift distribution estimator and its convergence 
behavior.

\begin{figure}
  \includegraphics[width=0.9\columnwidth]{figures/lsst_kld.png}\\
  \includegraphics[width=0.9\columnwidth]{figures/euclid_kld.png}
  \caption{We show the behavior of the KLD as a function of number of 
parameters over the quantiles (purple dashed line), samples (green dash-dotted 
line), and histogram (orange dotted line) formats.  Top panel: The Optical 
dataset of Sec. \ref{sec:LSST} shows\dots.  Bottom panel: The Optical+IR 
dataset of Sec. \ref{sec:Euclid} shows\dots.
  \label{fig:kld}}
\end{figure}

A survey may be limited by a required accuracy on $\hat{n}(z)$ for its primary 
science mission or by a hard limit on the available storage $N_{f}$ (or both).  
If the storage is the strongest constraint, one may consider a vertical line in 
Fig. \ref{fig:kld}; the best parametrization will correspond to the format that 
minimizes the KLD at the limiting value of $N_{f}$.

Further recommendations can be made if the requirements on the science products 
derived from the \pz\ catalog are known in terms of the acceptable degree of 
information loss.  If the information loss tolerance on $n(z)$ for a given 
survey mission is specified, it would correspond to a horizontal line in Fig. 
\ref{fig:stacked}; the optimal format would be the one that drops below that 
line at the lowest value of $N_{f}$, which would be the optimal number of 
parameters to store.  We encourage the community to specify the requirements on 
estimators of science products like $n(z)$ in terms of information theoretic 
quantities such as entropy, as these are naturally relatable to \pz s.

If there is some flexibility in the acceptable degree of information loss on 
$\hat{n}(z)$ and/or the allocation of storage for \pz s, as is the case for 
LSST, it may be best to examine the asymptotic behavior of the KLD as a 
function of $N_{f}$ for each format considered to make a choice based on a 
marginal difference.  For example, if the KLD can be significantly reduced with 
a slightly larger $N_{f}$, it may be possible to request additional storage 
capacity for the survey's \pz s.  We note that for these datasets, the KLD 
achieves its asymptote at different values of $N_{f}$ for each format, with the 
histogram format taking far larger $N_{f}$ than the other formats.





\section{Conclusions \& Future Directions}
\label{sec:conclusions}


This work develops a principled approach to choosing a storage format and 
resolution for storing a catalog of \pz s from a survey of known data quality 
to balance the accuracy of \pz s and science products thereof reconstructed 
from the stored parameters against the available storage constraints.  We 
demonstrate the recommended method on two realistic mock datasets 
representative of upcoming \pz\ catalogs and draw the following conclusions:
\begin{itemize}
  \item
  \item As indicated by the KLD, the quantile parametrization is a promising 
option for minimizing loss of information in \pz\ storage.
  \item Given the constraint that LSST will be able to store 200 floating point 
numbers to quantify the redshift of each galaxy and intends to include several 
\pz\ codes, we can safely say that LSST can store the output of more than one 
\pz\ code without any significant risk of loss of information.
\end{itemize}

We also make publicly available the \qp\ Python package central to this 
procedure and invite the community to contribute additional formats and metrics 
to the project.  \qp\ is a tool that can be used to optimize the choice of 
stored parametrization and number of stored parameters of a catalog of \pz s 
based on the accuracy needs of the use cases of the catalog.  Further 
applications of \qp\ functionality for manipulations of \pz s is demonstrated 
in the LSST-DESC PZ DC1 paper (in prep.).

We do not advocate for a one-size-fits-all solution to the problem and 
emphasize that the optimal choice must depend on the requirements of the 
science metric(s) and characteristics of the underlying \pz\ catalog.  
Furthermore, this procedure does not address the computational resources that 
may be necessary to perform the storage operation and to reconstruct \pz s into 
the format necessary for science calculations.

\dots


\subsection*{Appendix}
\label{sec:kld}

The KLD measures the loss of information, measured in nats, due to using an 
approximation of a distribution.  The most important quirk of the KLD is its 
asymmetry; it is not a distance, like the root mean square error, that is the 
same from $P(z)$ to $P'(z)$ as it is from $P'(z)$ to $P(z)$ but a 
\textit{divergence} in the information lost when using $P'(z)$ to approximate 
$P(z)$.  The KLD requires not only that both functions $P(z)$ and $P'(z)$ be 
true probability distributions (always positive semidefinite and integrating to 
unity) but also that there must be some notion of a true reference 
distribution.  The latter is in general not true for real data but can be 
checked for mock data, and the former may need to be explicitly enforced for 
formats other than quantiles.  It is also obvious that the KLD is always 
positive, and a smaller value indicates better agreement between the 
approximation and the truth.

To develop some intuition, consider the simple example of a Gaussian 
$P(z)=\mathcal{N}(\mu_{0}, \sigma_{0}^{2})$ being approximated by a Gaussian 
$P'(z)=\mathcal{N}(\mu, \sigma^{2})$, whose KLD is
\begin{align}
  \label{eq:gaussian}
  KLD &= \frac{1}{2}\left(\log\left[\frac{\sigma^{2}}{\sigma_{0}^{2}}\right] + 
\frac{\sigma_{0}^{2}}{\sigma^{2}} + \frac{(\mu-\mu_{0})^{2}}{\sigma^{2}} - 
1\right)
\end{align}
To get a sense of the units of information, we can calculate the KLD in some 
limiting cases.  If $\sigma=\sigma_{0}$ but $\mu=\mu_{0}+1$, we obtain 
$KLD=\frac{1}{2}$ in units of nats -- if the mean of the approximation is wrong 
by an additive factor of $1\sigma$, half a nat of information is lost.  If 
$\mu=\mu_{0}$ but $\sigma=\sqrt{2\pi}\sigma_{0}$, we find 
$KLD\approx\frac{1}{2}$ in units of nats -- half a nat of information is also 
lost if the variance of the approximation is off by a multiplicative factor of 
$2\pi$.

We can use the KLD to identify notions of imprecision and inaccuracy.  
Intuitively, precision must be related to how close $\sigma$ is to $\sigma_{0}$ 
and accuracy must be related to how close $\mu$ is to $\mu_{0}$.  If 
$\mu\approx\mu_{0}$, we can say $KLD\sim\log[r] + \frac{1}{2}r^{-2} - 
\frac{1}{2}$ in terms of the "precision" 
$r^{-1}\equiv\frac{\sigma_{0}}{\sigma}$, whose behavior is illustrated in Fig. 
\ref{fig:precision}.
\begin{figure}
  \includegraphics[width=0.9\columnwidth]{figures/kld_precision.png}
  \caption{The KLD (solid black line) is proportional to the log of the inverse 
precision $r$ for $\sigma>\sigma_{0}$ but is more complicated for 
$\sigma<\sigma_{0}$.
  \label{fig:precision}}
\end{figure}
We observe that an overestimated variance increases the KLD as the log of the 
square root of the ratio of the estimated variance to the true variance.  When 
$\sigma\approx\sigma_{0}$, $KLD\sim t^{2}$ in terms of the "tension" 
$t\equiv\frac{(\mu-\mu_{0})^{2}}{\sigma^{2}}$, whose behavior is illustrated in 
Fig. \ref{fig:tension}.
\begin{figure}
  \includegraphics[width=0.9\columnwidth]{figures/kld_tension.png}
  \caption{The KLD (solid black line) is equal to the square of the tension 
$t$, with a small additive offset when $r\neq1$.
  \label{fig:tension}}
\end{figure}

\subsection*{Acknowledgments}


SJS was partially supported by the National Science Foundation under grant 
N56981CC.


\input{acknowledgments}

\input{contributions}


\bibliography{lsstdesc,main}

\end{document}
