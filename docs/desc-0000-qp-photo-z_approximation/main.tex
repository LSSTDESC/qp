\RequirePackage{docswitch}
\setjournal{\flag}

\documentclass[\docopts]{\docclass}


\usepackage{lsstdesc_macros}

\usepackage{graphicx}
\graphicspath{{./}{./figures/}{.logos}}
\bibliographystyle{apj}


\newcommand{\textul}{\underline}
\newcommand{\qp}{\texttt{qp}}
\newcommand{\pz}{photo-$z$ PDF}
\newcommand{\Pz}{Photo-$z$ PDF}


\begin{document}

\title{ Approximating photo-z PDFs for large surveys }


\begin{abstract}

Upcoming and ongoing galaxy surveys will produce redshift probability 
distribution functions (PDFs) in addition to traditional photometric redshift 
(photo-$z$) point estimates.  However, the storage of \pz s may present a 
challenge as the dataset size increases, as we face a trade-off between the 
accuracy of subsequent science measurements and the storage cost. We 
investigate a number of different PDF approximations, using metrics that 
quantify performance in both large scale structure and weak gravitational 
lensing studies. In the process, we present \qp, a Python library enabling the 
evaluation of various approximations of 1-dimensional PDFs, as suitable for 
photometric redshifts.

\end{abstract}

\dockeys{methods: data analysis, catalogs, surveys}


\FIXME{There's a bug with \texttt{start\_paper} causing the 
title/authors/abstract to not be rendered.  Also, it is a problem that it 
requires internet access to compile.}



\section{Introduction}
\label{sec:intro}


Ongoing and upcoming photometric galaxy surveys such as the Large Synoptic 
Survey Telescope (LSST) will observe tens of billions of galaxies without 
spectroscopic follow-up to obtain redshifts necessary for studies of cosmology 
and galaxy evolution.  Such surveys rely on the methods of photometric redshift 
(photo-$z$) estimation.  Photo-$z$s are subject to a number of systematic 
errors, some caused by the data analysis procedures and others intrinsic to the 
data itself.  Due to these issues, the photo-$z$ community favors estimation of 
redshift probability distributions, or \pz s, that include information about 
the potential for such systematic errors for each galaxy in the survey.

Given the tremendous size of the surveys in question, storage of these 
probability distributions raises a number of nontrivial questions.  Previous 
treatments of photometric redshifts resulted in each galaxy's catalog entry 
having one additional floating point number to store, or perhaps a few based on 
a handful of photo-$z$ codes; how many numbers are necessary to characterize a 
\pz\ to the degree of precision dictated by the survey's science goals?  \Pz\  
codes do not all produce outputs in the same parametrization; what storage 
format best preserves the characteristics of a \pz?

Little attention has been paid to these matters, with a few exceptions.  
\citep{carrasco_kind_sparse_2014}

In this work, we outline a method by which a survey team may optimize the 
choice of parametrization and number of stored parameters for an anticipated 
catalog of \pz s.  We also present the publicly available \qp\ Python package 
for performing this optimization for an arbitrary survey.  The approach is 
demonstrated for LSST mock data.








\section{Methods}
\label{sec:methods}



We have developed the \qp\ Python library to facilitate manipulation of \pz s.  
A \texttt{qp.PDF} object is defined by its parametrizations.  By way of 
interpolation, \qp\ can convert a representation of a \pz\ under one 
parametrization to a representation of that \pz\ under a different 
parametrization.  The supported parametrizations are described in Sec. 
\ref{sec:approx}.  \qp\ also includes a few built-in metrics of the accuracy of 
a representation of a \pz\ if its value under a given parametrization is 
designated as "true."  The included metrics are described in Sec. 
\ref{sec:metrics}.

\subsection{Approximation Methods}
\label{sec:approx}


First, we establish a vocabulary for the definitions of approximation methods.  
Each \textit{parametrization} of a \pz\ is defined in terms of the 
\textit{format} $\mathcal{F}$, \textit{metaparameters} comprising $\vec{C}$, 
and \textit{parameters} comprising $\vec{c}$.  Each parametrization in turn 
corresponds to a \textit{representation}
\begin{align}
  \hat{p}_{\mathcal{F}, \vec{C}}(z) &\equiv \mathcal{F}(z;\vec{C}, \vec{c})
\end{align}
of the \pz, denoted as $\hat{p}(z)$ for brevity.

\qp\ is capable of converting a \pz\ between the following five formats: step 
functions, samples, grid evaluations, mixture models, and quantiles.  These 
formats are described below in terms of the number $N_{f}$ of stored parameters 
$c_{i}$ per \pz, which are presumed to be floating point numbers.  The 
metaparameters are set of $N_{M}$ numbers $C_{m}$ necessary to convert the 
stored \pz\ parameters $\vec{c}$ into a probability distribution function over 
redshift.  Because the metaparameters for a catalog of \pz s released by a 
survey will only need to be stored once, it does not matter how large $N_{M}$ 
is.  For each format, we address the following questions:

\begin{itemize}
  \item When/where has this format appeared in the literature as a published 
catalog format, native \pz code output format, and science application input 
format?
  \item What exactly is stored under this format, per galaxy (the parameters) 
and per catalog (the metaparameters)?
  \item In what ways is this format advantageous, and what are its weaknesses?
\end{itemize}

\subsubsection{Regular Binning}
\label{sec:bins}

By far the most popular format for \pz s is that of a piecewise constant step 
function, also called a histogram binning.  It is the only format that has been 
used for public release of \pz\ catalogs \citep{tanaka_photometric_2017, 
sheldon_photometric_2012}; it is unclear whether this is a consequence or a 
cause of the fact that it is the most common format for using \pz s in 
cosmological inference, as tomographic binning is a universal step between the 
\pz\ catalog and calculation of any two-point correlation function.

The metaparameters of the binned parametrization are the ordered list of 
$N_{M}=N_{f}+1$ redshifts $(z_{1}, z_{2}, \dots, z_{N_{M}-1}, z_{N_{M}})$ with 
$z_{1} < z_{2} < \dots < z_{N_{M}-1} < z_{N_{M}}$ and 
$z_{m+1}=z_{m}+\Delta_{m}$ serving as endpoints shared by all galaxies in the 
catalog, each adjacent pair of which is associated with a parameter 
$c_{i}=\int_{z_{i}}^{z_{i+1}}\ p(z)\ dz$.  If the binning is "regular," then 
$z_{i+1}=z_{i}+\Delta$ for some constant scalar $\Delta\equiv$.  As this is the 
only type of binning that has been used in the literature, it is the only one 
we consider.

The standard assumption that $p(z)=0$ when $z<z_{1}$ or $z>z_{N_{M}}$ implies 
that the $c_{i}$ are normalized according to $\Delta \sum_{i} c_{i} = 1$.  Note 
that this is not equivalent to the erroneous normalization condition $\sum_{i} 
c_{i} = 1$, that commonly appears in the literature.

All known native \pz\ formats are easy to convert to the histogram 
representation.  However, the corrolary to this statement is that it is an 
inherently lossy format.  A \pz\ with features smaller than the bin width may 
not accurately represent the underlying probability distribution, as 
significant structure may be stored as a single bin value.  This problem may be 
particularly troubling as the quality of the approximation will not be unbiased 
among catalog entries; rather, it is likely to correlate with the properties of 
the \pz\ in question.

The binned parametrization may also be considered wasteful in terms of data 
storage.  A \pz\ with a compact probability distribution, with much of the 
probability contained within a region far narrower than the redshift range 
$z_{N_{M}} - z_{1}$, may have many of its catalog entries $c_{i}$ being 
identically zero.  The storage footprint of this data is wasted in that it is 
redundant, with each $c_{i}$ requiring the same space even though 
$c_{i}=c_{i'}$ for many pairs $(i, i')$.

Finally, the binned parametrization requires the researcher to choose the 
minimum and maximum possible redshifts of the galaxy sample.  These are 
physical quantities that are unknown, so it would be preferable to not have to 
choose them at the stage of producing the \pz\ catalog.

\subsubsection{Samples}
\label{sec:samples}

Samples $(z_{1}, z_{2}, \dots, z_{N_{f}-1}, z_{N_{f}})$ are another common 
storage format for \pz s.  \COMMENT{Cite upcoming DES paper.}  Samples are the 
native output format of many machine learning algorithms dependent on random 
choices, such as random forests.

Such approaches typically produce large numbers of samples, far more than can 
realistically be stored by any survey, so a subsample is commonly stored.  A 
small number of samples from a broad \pz may not be representative of the 
overall shape, but it can be appropriate for \pz s with narrow features.  As in 
the case of the histogram parametrization, there is a significant risk of bias 
in the quality of the approximation with respect to properties of the \pz.

Though it is possible to construct a catalog where each galaxy has a different 
number $N_{f}$ of stored samples, optimizing the choice of $N_{f}$ given the 
shape of the \pz\ in its native format is nontrivial.  As this has not been 
done in the literature, we leave its investigation to future work.

\subsubsection{Evaluation on a Regular Grid}
\label{sec:grid}

Another storage option is evaluations of a continuous functional form of a \pz\ 
at $N_{f}$ redshifts $z_{1}, z_{2}, \dots, z_{N_{f}-1}, z_{N_{f}}$.  Thus far, 
only a regular grid of redshifts (i.e. $z_{i+1}=z_{i}+\Delta$ for a constant 
$\Delta$) shared among the entire catalog have been used in the literature, so 
we do not consider irregular grids or grids that are not homogeneous over the 
catalog.   In some cases, this is the native output format of a \pz\ code.  
\COMMENT{Apparently BPZ does this because that's what Melissa and Sam gave me, 
but I don't have anything to cite saying this.}

The gridded parametrization is in some ways similar to the histogram format of 
\ref{sec:bins}, suffering from the same risks of losing small-scale features 
(and the corresponding risk of systematic bias in the quality of the 
approximation), wasting a substantial fraction of the $N_{f}$ allocated 
parameters, and choosing the grid endpoints.  Unlike the piecewise constant 
parametrization, the grid evaluations are not necessarily normalized, which can 
be a challenge for null tests and science use cases.  Additionally, it is not 
in general easy to convert to this format from the native format of a \pz\ 
code, as it requires a continuous function for the \pz.

\subsubsection{Mixture Model}
\label{sec:mm}

There is some history of using a mixture model parametrization for \pz s.  
\COMMENT{I'm still seeking something to cite here -- I thought the native 
output of BPZ was the means and standard deviations of the top three Gaussian 
components, but this does not appear to be so.}  In this work, we consider only 
the common Gaussian mixture model, though the \qp\ framework can accommodate 
mixtures of all probability distribution functions that have been implemented 
as \texttt{scipy.stats.rv\_continous} objects.

A Gaussian mixture model may be a natural choice for the native output of a 
template-fitting code or one based on a distance metric in the space of 
photometry, and it is easy to convert it to the previously discussed 
parametrizations.  However, it may be difficult to fit a mixture model to \pz s 
in other native formats, and the mixture model is only an accurate 
approximation of the \pz s are actually comprised of components of the included 
parametrizations.

Besides the mixture of Gaussians, other functions have been investigated before 
in the sparse basis representation of \citet{carrasco_kind_sparse_2014}.  
Though it has promising compression properties, we do not consider it in this 
work for several reasons.  Decomposition with \texttt{SparsePZ} does not 
guarantee that the stored parametrization be a probability distribution in the 
mathematical sense of always being positive definite and integrating to unity, 
which we consider a necessary condition both for use of \pz s in research and 
for comparison to other methods using the \qp\ metrics.   The sparse basis 
representation also assumes that the native format of a \pz\ is evaluations on 
a grid; if this is not true, then the \pz\ may undergo additional conversions 
that introduce loss of information with each approximation.

\COMMENT{I also thought it required extensive computational resources, but it 
appears to be a lot faster than \qp\ making quantiles with the same number of 
parameters -- recall that calculating quantiles generally requires an 
optimization.  My reasons are pretty weak; if I had more time I'd have \qp\ 
employ the \texttt{SparsePz} method as another parametrization, but I don't 
know when I'll be able to do it!}

\subsubsection{Regular Quantiles}
\label{sec:quantiles}

One parametrization that has not previously been implemented is that of 
quantiles, which are defined in terms of the cumulative distribution function 
(CDF)
\begin{align}
  \label{eq:cdf}
  CDF(z) &= \int_{-\infty}^{z}\ p(z)\ dz.
\end{align}
Under the quantile parametrization, an ensemble of \pz s shares a set of 
$N_{f}$ values $0<q_{1}<q_{2}<\dots<q_{N-1}<q_{N_{f}}<1$.  Each galaxy's 
catalog entry is the $N_{f}$ values $z_{i}$ satisfying $CDF(z_{i})=q_{i}$.  In 
our tests, the quantiles are regular, such that $q_{i}\equiv i\bar{q}$, where 
$\bar{q}\equiv(N_{f}+1)^{-1}$, but \qp\ does not require that this be so.

The quantile parametrization is the inspiration for this work and namesake of 
\qp.  Though it has not appeared in the \pz\ literature prior to this point, it 
is a natural choice for the compression of probability distributions because it 
keeps more information in areas of higher probability density, so there is 
inherently less waste in the information that is stored for each catalog entry 
and minimal risk of bias in the quality of the approximation across \pz\ 
shapes.  Storing quantiles is equivalent to storing piecewise constant data (or 
function evaluations) on an irregular binning (or irregular grid) optimized to 
have narrower bins (denser evaluations) in areas of high probability and wider 
bins (more diffuse evaluations) in areas of low probability, effectively 
performing the optimization in bin size (grid resolution) while still 
permitting an entire catalog to share a single set of metaparameters.  Unlike 
samples, it is guaranteed to be an equally good approximation regardless of the 
shape of the \pz.

The quantile parametrization is not without its drawbacks.  There is as yet no 
infrastructure for using such a format in a scientific application, but there 
is no reason to think that this cannot change if the quantile parametrization 
proves effective.  Furthermore, no known \pz\ method has quantiles as a native 
output format, and some native \pz\ output formats, like samples, are easy to 
convert to quantiles, while others, like piecewise constant functions, are not, 
requiring a numerical optimization for each parameter $i$.  Nonetheless, the 
quantile parametrization is a new option that merits careful consideration.

\subsection{Comparison Metrics}
\label{sec:metrics}


We use two metrics to quantify how well an approximation of a \pz\ extracted 
from a stored format represents the original \pz, characterized by many more 
parameters than are available for storage, before it was compressed.  Given the 
stored parameters, we perform an interpolation to evaluations of the 
approximated \pz\ $\hat{p}(z)$ on a fine grid $(z_{1}, z_{2}, \dots, 
z_{N_{ff}-1}, z_{N_{ff}})$ to calculate metrics against the original format 
$p_{0}(z)$.  The distributions of metric values for each parametrization are 
compared in Sec. \ref{sec:individual}.  The metrics are also calculated for a 
science application of \pz s in Sec. \ref{sec:stacked}.

\subsubsection{RMSE}
\label{sec:rms}

The root mean square error (RMSE) is a familiar measure of the difference 
between two functions $p(z)$ and $\hat{p}(z)$,
\begin{align}
  \label{eq:rmse}
  RMSE &= \frac{1}{N_{ff}}\ \sum_{i=1}^{N_{ff}} (p(z_{i}) - \hat{p}(z_{i}))^{2}.
\end{align}
The RMSE is symmetric in that it is simply about the difference between the 
functions, not some distance from one to the other.  The RMSE is also not 
specific to probability distributions.  The RMSE is always positive, and a 
smaller value indicates better agreement between the approximation and the 
truth.

\subsubsection{KLD}
\label{sec:kld}

The Kullback-Leibler divergence (KLD)
\begin{align}
  \label{eq:kld}
  D(p_{0}(z) || \hat{p}(z)) &= \int_{-\infty}^{\infty}\ p_{0}(z)\ 
\log\left[\frac{p_{0}(z)}{\hat{p}(z)}\right]\ dz
\end{align}
quantifies the loss of information of an approximation of a probability 
distribution from the true probability distribution.  Note that the KLD is not 
symmetric and requires not only that both functions be true probability 
distributions but also that there must be some notion of a true reference 
distribution.  The KLD is always positive, and a smaller value indicates better 
agreement between the approximation and the truth.


\section{Photo-z Test Data}
\label{sec:data}

With the expectation that \qp\  may suggest a different optimal result for 
different datasets, we apply it to two mock datasets with different data 
quality properties.  Both datasets were fit using Bayesian Photometric Redshift 
(BPZ) estimation \citep{benitez_bayesian_2000}, which employs spectral energy 
distribution (SED) fitting to a template set.  However, the choice of \pz\ 
estimation method is not relevant to this study; so long as the mock \pz s are 
of realistic complexity, it does not matter how accurately they describe the 
probability distribution of galaxy redshifts given their photometric data.  We 
only seek to optimize the fidelity of the stored \pz\ relative to the \pz\ 
output by a representative \pz\ fitting code.  (Other work has been done to 
compare the accuracy of \pz s produced by different methods; see Schmidt, et 
al. 2017 (in prep.), \citet{tanaka_photometric_2017}.)

As BPZ is a widely used and well established method, we assume that the \pz s 
produced by it are of realistic complexity, meaning they take shapes we expect 
to see in accurate \pz s from real datasets with similar photometric 
properties.  The mock datasets considered here have already been transformed 
into a gridded parametrization, as we did not run the \pz\ code ourselves to 
get the raw output.  To create a realistically complex testbed catalog, we fit 
these high-resolution, gridded \pz s with a Gaussian mixture model so that our 
catalog has a notion of "true" underlying \pz s.  \COMMENT{At this point, the 
only reason to do this is the suboptimal implementation of 
\texttt{qp.PDF.truth}.  It would not be impossible to eliminate this last step.}

\COMMENT{\citet{carrasco_kind_sparse_2014} uses $N_{g}=10^{6}$ galaxies.  How 
many should we use for the paper?}

\subsection{LSST mocks}
\label{sec:ss}

\COMMENT{Should we invite Sam to write this section?}

LSST will provide six-band optical photometry to a depth of 27.5 magnitudes.  
\COMMENT{Is this consistent with Buzzard?}  The Buzzard simulations The \pz s 
were provided in the form of $N_{ff}=211$ floating point numbers represing the 
probability on a regular grid of redshifts $0.005 < z < 2.11$.  Due to the 
small number of photometric filters, LSST-only \pz s are expected to be 
multimodal; some examples are shown in Fig. \ref{fig:ss}.  We produced true 
underlying PDFs by fitting a five-component Gaussian mixture model to each \pz.

\begin{figure}
  \caption{[single-panel plot with a few examples of what the LSST-only \pz s 
look like]
  \label{fig:ss}}
\end{figure}

\subsection{LSST+Euclid mocks}
\label{sec:mg}

\COMMENT{Should we invite Melissa to write this section?}

While LSST will provide six-band optical photometry to a depth of 27.5 
magnitudes, Euclid will provide three-band near infrared photometry to a depth 
of 24 magnitudes.  The \pz s were provided in the form of $N_{ff}=351$ floating 
point numbers represing the probability on a regular grid of redshifts $0.01 < 
z < 3.51$.  With even three additional photometric filters, \pz s derived from 
LSST+Euclid photometry are expected to be narrow and in most cases unimodal.  A 
few examples of those with nontrivial shapes are shown in Fig. \ref{fig:mg}, 
but these represent a small fraction of the galaxies in the sample.  We 
produced true underlying PDFs by fitting a two-component Gaussian mixture model 
to each \pz.

\begin{figure}
  \caption{[single-panel plot with a few examples of what the LSST+Euclid \pz s 
look like]
  \label{fig:mg}}
\end{figure}


\section{Science Metrics}
\label{sec:science}

Though the use of \pz s could potentially extend to all areas of astronomy in 
which redshifts are used, \pz s have thus far been used almost exclusively to 
estimate the redshift distribution function $n(z)$ necessary for calculating 
the correlation functions used by many cosmological probes.
The most common way to estimate the redshift distribution function is to sum 
the \pz s according to
\begin{align}
  \label{eq:nz}
  \hat{n}(z) &\equiv \frac{1}{N_{g}}\ \sum_{j=1}^{N_{g}}\ \hat{p}_{j}(z),
\end{align}
where the estimator is normalized so that it, too, is a probability 
distribution.  Though we do not recommend this approach to estimating the 
redshift distribution (see Malz and Hogg, et al. (in prep.) for a 
mathematically consistent alternative), we also calculate the metrics of Sec. 
\ref{sec:metrics} on this "stacked estimator" of the redshift distribution 
function under the assumption that inaccuracy and imprecision of a \pz\ 
parametrization in th






\section{Results}
\label{sec:results}


In this study, we perform tests comparing the parametrizations of Sec. 
\ref{sec:approx} as a function of the number of parameters per galaxy.  The 
tests are conducted using the functionality of the \texttt{qp.Ensemble} class 
that is a wrapper for collections of \texttt{qp.PDF} objects.

\subsection{Individual \pz s}
\label{sec:individual}

Parametrizations are compared on the basis of the distributions of the metrics 
of Sec. \ref{sec:metrics} calculated over all galaxies in the ensemble.

\begin{figure}
  \caption{[histograms of each metric for different numbers of parameters, one 
panel per combination of metric (horizontal) and $N_{f}$ (vertical)]
  \label{fig:individual}}
\end{figure}

\subsection{Stacked $\hat{n}(z)$ estimator}
\label{sec:stacked}

Parametrizations are also compared by the accuracy of their stacked redshift 
distribution estimator $\hat{n}(z)$ relative to that of the \pz s in their 
original format.

\begin{figure}
  \caption{[number of stored floats $N_{f}$ vs. metric value, one curve per 
approximation method, one panel per combination of metric (horizontal) and 
$N_{f}$ (vertical)]
  \label{fig:stacked}}
\end{figure}





\section{Conclusions \& Future Directions}
\label{sec:conclusions}

We have presented a method for determining the most appropriate compression 
formats and number of stored parameters for large catalogs of \pz s produced by 
surveys with limited storage capacity.   In the case of well-behaved \pz s, we 
observe [RESULTS].  In the case of \pz s from lower quality data, we observe 
[RESULTS].  Given the constraint that LSST will be able to store 200 floating 
point numbers to quantify the redshift of each galaxy and intends to include 
several \pz\ codes, we recommend the [RECOMMENDATION] parametrization.



This work addresses the tradeoff between the accuracy of stored \pz s and the 
footprint of the data needed to encode the \pz s.  It does not address the 
computational resources necessary to perform the storage operation nor to 
unpack the stored information into the form necessary for science computations. 
 There may be other issues with the loss of information when extracting 
compressed \pz s for use in science, with impacts that may differ for each 
science case.

Further applications of \qp\ functionality for manipulations of \pz s is 
demonstrated in the LSST-DESC PZ DC1 paper (in prep.).

\subsection*{Acknowledgments}


We thank Melissa Graham and Sam Schmidt for providing the mock datasets.  This 
work was incubated at the 2016 LSST-DESC Hack Week.

\input{acknowledgments}

\input{contributions}


\bibliography{lsstdesc,main}

\end{document}
