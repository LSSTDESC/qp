\RequirePackage{docswitch}
\setjournal{\flag}

\documentclass[\docopts]{\docclass}


\usepackage{lsstdesc_macros}

\usepackage{graphicx}
\graphicspath{{./}{./figures/}{.logos}}
\bibliographystyle{apj}


\newcommand{\textul}{\underline}
\newcommand{\qp}{\texttt{qp}}
\newcommand{\pz}{photo-$z$ PDF}
\newcommand{\Pz}{Photo-$z$ PDF}


\begin{document}

\title{ Approximating photo-z PDFs for large surveys }


\begin{abstract}

Upcoming and ongoing galaxy surveys will produce redshift probability 
distribution functions (PDFs) in addition to traditional photometric redshift 
(photo-$z$) point estimates.  However, the storage of \pz s may present a 
challenge as the dataset size increases, as we face a trade-off between the 
accuracy of subsequent science measurements and the storage cost. We 
investigate a number of different PDF approximations, using metrics that 
quantify performance in both large scale structure and weak gravitational 
lensing studies. In the process, we present \qp, a Python library enabling the 
evaluation of various approximations of 1-dimensional PDFs, as suitable for 
photometric redshifts.

\end{abstract}

\dockeys{methods: data analysis, catalogs, surveys}


\FIXME{There's a bug with \texttt{start\_paper} causing the 
title/authors/abstract to not be rendered.  Also, it is a problem that it 
requires internet access to compile.}



\section{Introduction}
\label{sec:intro}


Ongoing and upcoming photometric galaxy surveys such as the Large Synoptic 
Survey Telescope (LSST) will observe tens of billions of galaxies without 
spectroscopic follow-up to obtain redshifts necessary for studies of cosmology 
and galaxy evolution.  Such surveys rely on the methods of photometric redshift 
(photo-$z$) estimation.  Photo-$z$s are subject to a number of systematic 
errors, some caused by the data analysis procedures and others intrinsic to the 
data itself.  Due to these issues, the photo-$z$ community favors estimation of 
redshift probability distributions, or \pz s, that include information about 
the potential for such systematic errors for each galaxy in the survey.

Given the tremendous size of the surveys in question, storage of these 
probability distributions raises a number of nontrivial questions.  Previous 
treatments of photometric redshifts resulted in each galaxy's catalog entry 
having one additional floating point number to store, or perhaps a few based on 
a handful of photo-$z$ codes; how many numbers are necessary to characterize a 
\pz\ to the degree of precision dictated by the survey's science goals?  \Pz\  
codes do not all produce outputs in the same parametrization; what storage 
format best preserves the characteristics of a \pz?

Little attention has been paid to these matters, with a few exceptions.  
\citep{carrasco_kind_sparse_2014}

In this work, we outline a method by which a survey team may optimize the 
choice of parametrization and number of stored parameters for an anticipated 
catalog of \pz s.  We also present the publicly available \qp\ Python package 
for performing this optimization for an arbitrary survey.  The approach is 
demonstrated for LSST mock data.








\section{Methods}
\label{sec:methods}



\qp\ has two main functionalities: converting between parametrizations of \pz s 
and computing metrics of parametrizations of \pz s relative to perfectly 
accurate representations thereof.

\subsection{Approximation Methods}
\label{sec:approx}


\qp\ is capable of converting a one-dimensional probability distribution 
between four parametrizations that have been used in the literature as well as 
one that has not: step functions, samples, grid evaluations, a mixture model, 
and quantiles.  These parametrizations will be described below in terms of the 
number $N_{f}$ of parameters $i$, which are presumed to be floating point 
numbers.  We will use "the native output format" of \pz\ codes to mean the 
natural product of the algorithm by which \pz s are derived from photometry by 
a given method.  For each, we address the following questions:

\begin{itemize}
  \item When/where has this format appeared in the literature as a published 
catalog format, native \pz code output format, and science application input 
format?
  \item What exactly is stored under this format, per galaxy (the parameters) 
and per catalog (the metaparameters)?
  \item In what ways is this format advantageous, and what are its weaknesses?
\end{itemize}

\subsubsection{Regular Binning}
\label{sec:bins}

By far the most popular parametrization of \pz s is that of a piecewise 
constant step function, also called a histogram binning.  It is the only format 
that has been used for public release of \pz\ catalogs 
\citep{tanaka_photometric_2017, sheldon_photometric_2012}; it is unclear 
whether this is a consequence or a cause of the fact that it is the most common 
format for using \pz s in cosmological inference, as tomographic binning is a 
universal step between the \pz\ catalog and calculation of any two-point 
correlation function.

The binned parametrization is defined by an ordered list of redshifts $(z_{1}, 
z_{2}, \dots, z_{N_{f}}, z_{N_{f}+1})$ with $z_{1} < z_{2} < \dots < z_{N_{f}} 
< z_{N_{f}+1}$ serving as endpoints shared by all galaxies in the catalog, each 
of which is associated with a list of values $\vec{p} = (p_{1}, p_{2}, \dots, 
p_{N_{f}-1}, p_{N_{f}})$ that are the integrated probabilities over the bin in 
question.  If the binning is "regular," then $z_{i+1}=z_{i}+\Delta$ for some 
constant scalar $\Delta$.  As this is the only type of binning that has been 
used in the literature, it is the only one we consider.  Note that the catalog 
entries integrate to unity but in general will not sum to unity, i.e. $\int 
p_{i} dz = 1$ and $\sum_{i}p_{i}\neq1$.

All known native \pz\ formats are easy to convert to the histogram 
representation.  However, the corrolary to this statement is that it is an 
inherently lossy format.  A \pz\ with features smaller than the bin width may 
not accurately represent the underlying probability distribution, as 
significant structure may be stored as a single bin value.  This problem may be 
particularly troubling as the quality of the approximation will not be unbiased 
among catalog entries; rather, it is likely to correlate with the properties of 
the \pz\ in question.

The binned parametrization may also be considered wasteful in terms of data 
storage.  A \pz\ with a compact probability distribution, with much of the 
probability contained within a region far narrower than the redshift range 
$z_{N_{f}+1} - z_{1}$, may have most of its catalog entries (the elements of 
$\vec{p}$) being trivial.

Finally, the binned parametrization requires the researcher to choose the 
minimum and maximum possible redshifts of the galaxy sample.  These are 
physical quantities that are unknown, so it would be preferable to not have to 
choose them at the stage of producing the \pz\ catalog.

\subsubsection{Samples}
\label{sec:samples}

Samples $(z_{1}, z_{2}, \dots, z_{N_{f}-1}, z_{N_{f}})$ are another common 
storage format for \pz s.  \COMMENT{Cite upcoming DES paper.}  Samples are the 
native output format of many machine learning algorithms dependent on random 
choices, such as random forests.

Such approaches typically produce large numbers of samples, far more than can 
realistically be stored by any survey, so a subsample is commonly stored.  A 
small number of samples from a broad \pz may not be representative of the 
overall shape, but it can be appropriate for \pz s with narrow features.  As in 
the case of the histogram parametrization, there is a significant risk of bias 
in the quality of the approximation with respect to properties of the \pz.

Though it is possible to construct a catalog where each galaxy has a different 
number $N_{f}$ of stored samples, optimizing the choice of $N_{f}$ given the 
shape of the \pz\ in its native format is nontrivial.  As this has not been 
done in the literature, we leave its investigation to future work.

\subsubsection{Evaluation on a Regular Grid}
\label{sec:grid}

Another storage option is evaluations of a continuous functional form of a \pz\ 
at $N_{f}$ redshifts $z_{1}, z_{2}, \dots, z_{N_{f}-1}, z_{N_{f}}$.  Thus far, 
only a regular grid of redshifts (i.e. $z_{i+1}=z_{i}+\Delta$ for a constant 
$\Delta$) shared among the entire catalog have been used in the literature, so 
we do not consider irregular grids or grids that are not homogeneous over the 
catalog.   In some cases, this is the native output format of a \pz\ code.  
\COMMENT{Apparently BPZ does this because that's what Melissa and Sam gave me, 
but I don't have anything to cite saying this.}

The gridded parametrization is in some ways similar to the histogram format of 
\ref{sec:bins}, suffering from the same risks of losing small-scale features 
(and the corresponding risk of systematic bias in the quality of the 
approximation), wasting a substantial fraction of the $N_{f}$ allocated 
parameters, and choosing the grid endpoints.  Unlike the piecewise constant 
parametrization, the grid evaluations are not necessarily normalized, which can 
be a challenge for null tests and science use cases.  Additionally, it is not 
in general easy to convert to this format from the native format of a \pz\ 
code, as it requires a continuous function for the \pz.

\subsubsection{Mixture Model}
\label{sec:mm}

There is some history of using a mixture model parametrization for \pz s.  
\COMMENT{I'm still seeking something to cite here -- I thought the native 
output of BPZ was the means and standard deviations of the top three Gaussian 
components, but this does not appear to be so.}  In this work, we consider only 
the common Gaussian mixture model, though the \qp\ framework can accommodate 
mixtures of all probability distribution functions that have been implemented 
as \texttt{scipy.stats.rv\_continous} objects.

A Gaussian mixture model may be a natural choice for the native output of a 
template-fitting code or one based on a distance metric in the space of 
photometry, and it is easy to convert it to the previously discussed 
parametrizations.  However, it may be difficult to fit a mixture model to \pz s 
in other native formats, and the mixture model is only an accurate 
approximation of the \pz s are actually comprised of components of the included 
parametrizations.

Besides the mixture of Gaussians, other functions have been investigated before 
in the sparse basis representation of \citet{carrasco_kind_sparse_2014}.  
Though it has promising compression properties, we do not consider it in this 
work for several reasons.  Decomposition with \texttt{SparsePZ} does not 
guarantee that the stored parametrization be a probability distribution in the 
mathematical sense of always being positive definite and integrating to unity, 
which we consider a necessary condition both for use of \pz s in research and 
for comparison to other methods using the \qp\ metrics.   The sparse basis 
representation also assumes that the native format of a \pz\ is evaluations on 
a grid; if this is not true, then the \pz\ may undergo additional conversions 
that introduce loss of information with each approximation.

\COMMENT{I also thought it required extensive computational resources, but it 
appears to be a lot faster than \qp\ making quantiles with the same number of 
parameters -- recall that calculating quantiles generally requires an 
optimization.  My reasons are pretty weak; if I had more time I'd have \qp\ 
employ the \texttt{SparsePz} method as another parametrization, but I don't 
know when I'll be able to do it!}

\subsubsection{Regular Quantiles}
\label{sec:quantiles}

One parametrization that has not previously been implemented is that of 
quantiles, which are defined in terms of the cumulative distribution function 
(CDF)
\begin{align}
  \label{eq:cdf}
  CDF(z) &= \int_{-\infty}^{z}\ p(z)\ dz.
\end{align}
Under the quantile parametrization, an ensemble of \pz s shares a set of 
$N_{f}$ values $0<q_{1}<q_{2}<\dots<q_{N-1}<q_{N_{f}}<1$.  Each galaxy's 
catalog entry is the $N_{f}$ values $z_{i}$ satisfying $CDF(z_{i})=q_{i}$.  In 
our tests, the quantiles are regular, such that $q_{i}\equiv i\bar{q}$, where 
$\bar{q}\equiv(N_{f}+1)^{-1}$, but \qp\ does not require that this be so.

The quantile parametrization is the inspiration for this work and namesake of 
\qp.  Though it has not appeared in the \pz\ literature prior to this point, it 
is a natural choice for the compression of probability distributions because it 
keeps more information in areas of higher probability density, so there is 
inherently less waste in the information that is stored for each catalog entry 
and minimal risk of bias in the quality of the approximation across \pz\ 
shapes.  Storing quantiles is equivalent to storing piecewise constant data (or 
function evaluations) on an irregular binning (or irregular grid) optimized to 
have narrower bins (denser evaluations) in areas of high probability and wider 
bins (more diffuse evaluations) in areas of low probability, effectively 
performing the optimization in bin size (grid resolution) while still 
permitting an entire catalog to share a single set of metaparameters.  Unlike 
samples, it is guaranteed to be an equally good approximation regardless of the 
shape of the \pz.

The quantile parametrization is not without its drawbacks.  There is as yet no 
infrastructure for using such a format in a scientific application, but there 
is no reason to think that this cannot change if the quantile parametrization 
proves effective.  Furthermore, no known \pz\ method has quantiles as a native 
output format, and some native \pz\ output formats, like samples, are easy to 
convert to quantiles, while others, like piecewise constant functions, are not, 
requiring a numerical optimization for each parameter $i$.  Nonetheless, the 
quantile parametrization is a new option that merits careful consideration.

\subsection{Comparison Metrics}
\label{sec:metrics}


We use several metrics to quantify how well an approximation of a \pz  
extracted from a stored format represents the original \pz , characterized by 
many more parameters than are available for storage, before it was compressed.

\COMMENT{\citet{carrasco_kind_sparse_2014} uses histograms and summary 
statistics of the RMSE distribution.  Should we also do that or is it 
sufficient to interpret the metrics on $\hat{n}(z)$ in terms of percent error?}

\COMMENT{\citet{carrasco_kind_sparse_2014} and others also consider metrics on 
point estimators derived from PDFs which I implement for the PZ DC1 paper.  
Should I include that in this paper?}

\subsubsection{RMSE}
\label{sec:rms}

The root mean square error (RMSE) is a familiar measure of the difference 
between two functions $P$ and $P'$,
\begin{align}
  \label{eq:rmse}
  RMSE &= (P - P')^{2}.
\end{align}

\subsubsection{KLD}
\label{sec:kld}

The Kullback-Leibler divergence quantifies the deviation of one distribution 
from another, which in our case is the distance to a true distribution $P$ from 
an approximation thereof $P'$, as in
\begin{align}
  \label{eq:kld}
  D(P' | P) &= \int_{-\infty}^{\infty}\ \log\left[\frac{P(z)}{P'(z)}\right]\ 
P(z)\ dz.
\end{align}




\section{Photo-z Test Data}
\label{sec:data}

With the expectation that \qp may suggest a different optimal result for 
different datasets, we apply it to two mock datasets with different data 
quality properties.  Both datasets were fit using Bayesian Photometric Redshift 
(BPZ) estimation \citep{benitez_bayesian_2000}, which employs spectral energy 
distribution (SED) fitting to a template set.  However, the choice of \pz  
estimation method is not relevant to this study; so long as the mock \pz s are 
of realistic complexity, it does not matter how accurately they describe the 
probability distribution of galaxy redshifts given their photometric data.  We 
only seek to optimize the fidelity of the stored \pz  relative to the \pz  
output by a representative \pz  fitting code.  (Other work has been done to 
compare the accuracy of \pz s produced by different methods; see Schmidt, et 
al. 2017 (in prep.), \citet{tanaka_photometric_2017}.)

As BPZ is a widely used and well established method, we assume that the \pz s 
produced by it are of realistic complexity, meaning they take shapes we expect 
to see in accurate \pz s from datasets with similar photometric properties.

The datasets considered here have already been transformed into a gridded 
parametrization, as we did not run the \pz  code ourselves to get the raw 
output.  To create a realistically complex testbed catalog, we fit these 
high-resolution, gridded \pz s with a Gaussian mixture model so that our 
catalog has a notion of "true" underlying \pz s.

\subsection{LSST+Euclid mocks}
\label{sec:mg}

\COMMENT{Should we invite Melissa to write this section?}

The \pz s were provided in the form of $N_{f}=351$ floating point numbers 
represing the probability on a regular grid of redshifts $0.01 < z < 3.51$.  We 
produced true underlying PDFs by fitting a two-component Gaussian mixture model 
to each \pz .

\subsection{LSST mocks}
\label{sec:ss}

\COMMENT{Should we invite Sam to write this section?}

The \pz s were provided in the form of $N_{f}=211$ floating point numbers 
represing the probability on a regular grid of redshifts $0.005 < z < 2.11$.  
We produced true underlying PDFs by fitting a five-component Gaussian mixture 
model to each \pz .














\section{Science Metrics}
\label{sec:science}

Though the use of \pz s could potentially extend to all areas of astronomy in 
which redshifts are used, \pz s have thus far been used almost exclusively to 
estimate the redshift distribution function $n(z)$ necessary for calculating 
correlation functions used by many cosmological probes.


The most common way to estimate the redshift distribution function is to sum 
the \pz s according to
\begin{align}
  \label{eq:nz}
  \hat{n}(z) &= \frac{1}{N_{g}}\ \sum_{j=1}^{N_{g}}\ p_{j}(z).
\end{align}
Though we do not recommend this approach to estimating the redshift 
distribution (see Malz and Hogg, et al. (in prep.) for a mathematically 
motivated alternative), we also calculate the metrics of Sec. \ref{sec:metrics} 
on this "stacked estimator" of the redshift distribution function.






\section{Results}
\label{sec:results}


\COMMENT{\citet{carrasco_kind_sparse_2014} uses $N_{g}=10^{6}$ galaxies.  How 
many should we use for the paper?}

In this study, we perform tests comparing the parametrizations of Sec. 
\ref{sec:approx} as a function of the number of parameters per galaxy.  We use 
an ensemble of $N_{g}=?$ galaxies

\subsection{Individual \pz s}
\label{sec:individual}

Parametrizations are compared on the basis of the distributions of the metrics 
of Sec. \ref{sec:metrics} calculated over all galaxies in the ensemble.

\begin{figure}
  \caption{[histograms of each metric for different numbers of parameters, one 
panel per combination of metric (horizontal) and $N_{f}$ (vertical)]
  \label{fig:individual}}
\end{figure}

\subsection{Stacked $\hat{n}(z)$ estimator}
\label{sec:stacked}

Parametrizations are also compared by the accuracy of their stacked redshift 
distribution estimator $\hat{n}(z)$ relative to that of the \pz s in their 
original format.

\begin{figure}
  \caption{[number of stored floats $N_{f}$ vs. metric value, one curve per 
approximation method, one panel per combination of metric (horizontal) and 
$N_{f}$ (vertical)]
  \label{fig:stacked}}
\end{figure}





\section{Conclusions \& Future Directions}
\label{sec:conclusions}

We have presented a method for determining the most appropriate compression 
formats and number of stored parameters for large catalogs of \pz s produced by 
surveys with limited storage capacity.   In the case of well-behaved \pz s, we 
observe [RESULTS].  In the case of \pz s from lower quality data, we observe 
[RESULTS].  Given the constraint that LSST will be able to store 200 floating 
point numbers to quantify the redshift of each galaxy and intends to include 
several \pz  codes, we recommend the [RECOMMENDATION] parametrization.



This work addresses the tradeoff between the accuracy of stored \pz s and the 
footprint of the data needed to encode the \pz s.  It does not address the 
computational resources necessary to perform the storage operation nor to 
unpack the stored information into the form necessary for science computations. 
 There may be other issues with the loss of information when extracting 
compressed \pz s for use in science, with impacts that may differ for each 
science case.

Further applications of \qp functionality for manipulations of \pz s is 
demonstrated in the LSST-DESC PZ DC1 paper (in prep.).

\subsection*{Acknowledgments}


We thank Melissa Graham and Sam Schmidt for providing the mock datasets.  This 
work was incubated at the 2016 LSST-DESC Hack Week.

\input{acknowledgments}

\input{contributions}


\bibliography{lsstdesc,main}

\end{document}
