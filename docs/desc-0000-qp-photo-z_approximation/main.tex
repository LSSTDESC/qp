\RequirePackage{docswitch}
\setjournal{\flag}

\documentclass[\docopts]{\docclass}


\usepackage{lsstdesc_macros}

\usepackage{graphicx}
\graphicspath{{./}{./figures/}{.logos}}
\bibliographystyle{apj}


\newcommand{\textul}{\underline}
\newcommand{\qp}{\texttt{qp}}


\begin{document}

\title{ Approximating photo-z PDFs for large surveys }


\begin{abstract}

Upcoming and ongoing galaxy surveys will produce redshift probability 
distribution functions (PDFs) in addition to traditional photometric redshift 
(photo-$z$) point estimates.  However, the storage of photo-$z$ PDFs may 
present a challenge as the dataset size increases, as we face a trade-off 
between the accuracy of subsequent science measurements and the storage cost. 
We investigate a number of different PDF approximations, using metrics that 
quantify performance in both large scale structure and weak gravitational 
lensing studies. In the process, we present \qp, a Python library enabling the 
evaluation of various approximations of 1-dimensional PDFs, as suitable for 
photometric redshifts.

\end{abstract}

\dockeys{methods: data analysis, catalogs, surveys}


\FIXME{There's a bug with \texttt{start\_paper} causing the 
title/authors/abstract to not be rendered.  Also, it is a problem that it 
requires internet access to compile.}



\section{Introduction}
\label{sec:intro}


Ongoing and upcoming photometric galaxy surveys such as the Large Synoptic 
Survey Telescope (LSST) will observe tens of billions of galaxies without 
spectroscopic follow-up to obtain redshifts necessary for studies of cosmology 
and galaxy evolution.  Such surveys rely on the methods of photometric redshift 
(photo-$z$) estimation.  Photo-$z$s are subject to a number of systematic 
errors, some caused by the data analysis procedures and others intrinsic to the 
data itself.  Due to these issues, the photo-$z$ community favors estimation of 
redshift probability distributions, or photo-$z$ PDFs, that include information 
about the potential for such systematic errors for each galaxy in the survey.

Given the tremendous size of the surveys in question, storage of these 
probability distributions raises a number of nontrivial questions.  Previous 
treatments of photometric redshifts resulted in each galaxy's catalog entry 
having one additional floating point number to store, or perhaps a few based on 
a handful of photo-$z$ codes; how many numbers are necessary to characterize a 
photo-$z$ PDF to the degree of precision dictated by the survey's science 
goals?  Photo-$z$ PDF codes do not all produce outputs in the same 
parametrization; what storage format best preserves the characteristics of a 
photo-$z$ PDF?

Little attention has been paid to these matters, with a few exceptions.  
\citep{carrasco_kind_sparse_2014}

In this work, we outline a method by which a survey team may optimize the 
choice of parametrization and number of stored parameters for an anticipated 
catalog of photo-$z$ PDFs.  We also present the publicly available \qp Python 
package for performing this optimization for an arbitrary survey.  The approach 
is demonstrated for LSST mock data.








\section{Methods}
\label{sec:methods}

[Describe possible interpolation schemes used in conversions for both 
approximations and metrics]


\qp has two main functionalities: converting between parametrizations of 
photo-$z$ PDFs and computing metrics of parametrizations of photo-$z$ PDFs 
relative to perfectly accurate representations thereof.

\subsection{Approximation Methods}
\label{sec:approx}


\qp is capable of converting a one-dimensional probability distribution between 
four parametrizations that have been used in the literature as well as one that 
has not: step functions, samples, grid evaluations, a mixture model, and 
quantiles.  These parametrizations will be described below in terms of the 
number $N_{p}$ of parameters $i$.

\COMMENT{I need to hunt down citations for the assertion that these formats 
have been used before!}

\subsubsection{Regular Binning}
\label{sec:bins}

By far the most common parametrization of photo-$z$ PDFs is that of a piecewise 
constant step function, also called a histogram binning.  
\citep{tanaka_photometric_2017, sheldon_photometric_2012}

\subsubsection{Samples}
\label{sec:samples}

Some surveys store samples of a photo-$z$ PDF.  \COMMENT{Cite upcoming DES 
paper.}

\subsubsection{Evaluation on a Regular Grid}
\label{sec:grid}

\subsubsection{Mixture Model}
\label{sec:mm}

There is some history of using a mixture model parametrization for photo-$z$ 
PDFs.  Though mixtures of Gaussian distributions have been used more often, 
other functions have been investigated as well 
\citep{carrasco_kind_sparse_2014}.

In this work, we consider only the common Gaussian mixture model, though the 
\qp framework can accommodate mixtures of all probability distribution 
functions that have been implemented as \texttt{scipy.stats.rv_continous} 
objects.

Though the sparse basis representation of \citet{carrasco_kind_sparse_2014} has 
promising compression properties, we do not consider it in this work for 
several reasons.  The sparse basis representation of 
\citet{carrasco_kind_sparse_2014} does not guarantee that the stored 
parametrization be a probability distribution in a mathematical sense, which we 
consider a necessary condition both for use of photo-$z$ PDFs in research and 
for comparison to other methods using the \qp metrics.   The sparse basis 
representation also assumes that the native format of a photo-$z$ PDF is 
evaluations on a grid; if this is not true, then the photo-$z$ PDF may undergo 
additional conversions that introduce loss of information with each 
approximation.

\COMMENT{I also thought it required extensive computational resources, but it 
appears to be a lot faster than \qp making quantiles with the same number of 
parameters -- recall that calculating quantiles generally requires an 
optimization.  My reasons are pretty weak; if I had more time I'd have \qp 
employ the \texttt{SparsePz} method as another parametrization, but I don't 
know when I'll be able to do it!}

\subsubsection{Regular Quantiles}
\label{sec:quantiles}

One parametrization that has not previously been implemented is that of 
quantiles, which are defined in terms of the cumulative distribution function 
(CDF)
\begin{align}
  \label{eq:cdf}
  CDF &= \int_{-\infty}^{z}\ p(z)\ dz.
\end{align}
Under the quantile parametrization, an ensemble of photo-$z$ PDFs shares a set 
of $N$ values $0<q_{1}<q_{2}<\dots<q_{N-1}<q_{N_{p}}<1$.  Each galaxy's catalog 
entry is the $N_{p}$ values $z_{i}$ satisfying $CDF=q_{i}$.  In our tests, the 
quantiles obey a regular binning $q_{i}\equiv i\bar{q}$, where 
$\bar{q}\equiv(N_{p}+1)^{-1}$, but there is no requirement that this be so.

\subsection{Comparison Metrics}
\label{sec:metrics}


We use several metrics to quantify how well an approximation of a photo-$z$ PDF 
extracted from a stored format represents the original photo-$z$ PDF, 
characterized by many more parameters than are available for storage, before it 
was compressed.

\COMMENT{\citet{carrasco_kind_sparse_2014} uses histograms and summary 
statistics of the RMSE distribution.  Should we also do that or is it 
sufficient to interpret the metrics on $\hat{n}(z)$ in terms of percent error?}

\COMMENT{\citet{carrasco_kind_sparse_2014} and others also consider metrics on 
point estimators derived from PDFs which I implement for the PZ DC1 paper.  
Should I include that in this paper?}

\subsubsection{RMSE}
\label{sec:rms}

The root mean square error (RMSE) is a familiar measure of the difference 
between two functions $P$ and $P'$,
\begin{align}
  \label{eq:rmse}
  RMSE &= (P - P')^{2}.
\end{align}

\subsubsection{KLD}
\label{sec:kld}

The Kullback-Leibler divergence quantifies the deviation of one distribution 
from another, which in our case is the distance to a true distribution $P$ from 
an approximation thereof $P'$, as in
\begin{align}
  \label{eq:kld}
  D(P' | P) &= \int_{-\infty}^{\infty}\ \log\left[\frac{P(z)}{P'(z)}\right]\ 
P(z)\ dz.
\end{align}




\section{Photo-z Test Data}
\label{sec:data}

With the expectation that \qp may suggest a different optimal result for 
different datasets, we apply it to two mock datasets with different data 
quality properties.  Both datasets were fit using Bayesian Photometric Redshift 
(BPZ) estimation \citep{benitez_bayesian_2000}, which employs spectral energy 
distribution (SED) fitting to a template set.  However, the choice of photo-$z$ 
PDF estimation method is not relevant to this study; so long as the mock 
photo-$z$ PDFs are of realistic complexity, it does not matter how accurately 
they describe the probability distribution of galaxy redshifts given their 
photometric data.  We only seek to optimize the fidelity of the stored 
photo-$z$ PDF relative to the photo-$z$ PDF output by a representative 
photo-$z$ PDF fitting code.  (Other work has been done to compare the accuracy 
of photo-$z$ PDFs produced by different methods; see Schmidt, et al. 2017 (in 
prep.), \citet{tanaka_photometric_2017}.)

As BPZ is a widely used and well established method, we assume that the 
photo-$z$ PDFs produced by it are of realistic complexity, meaning they take 
shapes we expect to see in accurate photo-$z$ PDFs from datasets with similar 
photometric properties.

The datasets considered here have already been transformed into a gridded 
parametrization, as we did not run the photo-$z$ PDF code ourselves to get the 
raw output.  To create a realistically complex testbed catalog, we fit these 
high-resolution, gridded photo-$z$ PDFs with a Gaussian mixture model so that 
our catalog has a notion of "true" underlying photo-$z$ PDFs.

\subsection{LSST+Euclid mocks}
\label{sec:mg}

\COMMENT{Should we invite Melissa to write this section?}

The photo-$z$ PDFs were provided in the form of $N_{p}=351$ floating point 
numbers represing the probability on a regular grid of redshifts $0.01 < z < 
3.51$.  We produced true underlying PDFs by fitting a two-component Gaussian 
mixture model to each photo-$z$ PDF.

\subsection{LSST mocks}
\label{sec:ss}

\COMMENT{Should we invite Sam to write this section?}

The photo-$z$ PDFs were provided in the form of $N_{p}=211$ floating point 
numbers represing the probability on a regular grid of redshifts $0.005 < z < 
2.11$.  We produced true underlying PDFs by fitting a five-component Gaussian 
mixture model to each photo-$z$ PDF.














\section{Science Metrics}
\label{sec:science}

Though the use of photo-$z$ PDFs could potentially extend to all areas of 
astronomy in which redshifts are used, photo-$z$ PDFs have thus far been used 
almost exclusively to estimate the redshift distribution function $n(z)$ 
necessary for calculating correlation functions used by many cosmological 
probes.


The most common way to estimate the redshift distribution function is to sum 
the photo-$z$ PDFs according to
\begin{align}
  \label{eq:nz}
  \hat{n}(z) &= \frac{1}{N_{g}}\ \sum_{j=1}^{N_{g}}\ p_{j}(z).
\end{align}
Though we do not recommend this approach to estimating the redshift 
distribution (see Malz and Hogg, et al. (in prep.) for a mathematically 
motivated alternative), we also calculate the metrics of Sec. \ref{sec:metrics} 
on this "stacked estimator" of the redshift distribution function.






\section{Results}
\label{sec:results}


\COMMENT{\citet{carrasco_kind_sparse_2014} uses $N_{g}=10^{6}$ galaxies.  How 
many should we use for the paper?}

In this study, we perform tests comparing the parametrizations of Sec. 
\ref{sec:approx} as a function of the number of parameters per galaxy.  We use 
an ensemble of $N_{g}=?$ galaxies

\subsection{Individual photo-$z$ PDFs}
\label{sec:individual}

Parametrizations are compared on the basis of the distributions of the metrics 
of Sec. \ref{sec:metrics} calculated over all galaxies in the ensemble.

\begin{figure}
  \caption{[histograms of each metric for different numbers of parameters, one 
panel per combination of metric (horizontal) and $N_{p}$ (vertical)]
  \label{fig:individual}}
\end{figure}

\subsection{Stacked $\hat{n}(z)$ estimator}
\label{sec:stacked}

Parametrizations are also compared by the accuracy of their stacked redshift 
distribution estimator $\hat{n}(z)$ relative to that of the photo-$z$ PDFs in 
their original format.

\begin{figure}
  \caption{[number of stored floats $N_{p}$ vs. metric value, one curve per 
approximation method, one panel per combination of metric (horizontal) and 
$N_{p}$ (vertical)]
  \label{fig:stacked}}
\end{figure}





\section{Conclusions \& Future Directions}
\label{sec:conclusions}

We have presented a method for determining the most appropriate compression 
formats and number of stored parameters for large catalogs of photo-$z$ PDFs 
produced by surveys with limited storage capacity.   In the case of 
well-behaved photo-$z$ PDFs, we observe [RESULTS].  In the case of photo-$z$ 
PDFs from lower quality data, we observe [RESULTS].  Given the constraint that 
LSST will be able to store 200 floating point numbers to quantify the redshift 
of each galaxy and intends to include several photo-$z$ PDF codes, we recommend 
the [RECOMMENDATION] parametrization.



This work addresses the tradeoff between the accuracy of stored photo-$z$ PDFs 
and the footprint of the data needed to encode the photo-$z$ PDFs.  It does not 
address the computational resources necessary to perform the storage operation 
nor to unpack the stored information into the form necessary for science 
computations.  There may be other issues with the loss of information when 
extracting compressed photo-$z$ PDFs for use in science, with impacts that may 
differ for each science case.

Further applications of \qp functionality for manipulations of photo-$z$ PDFs 
is demonstrated in the LSST-DESC PZ DC1 paper (in prep.).

\subsection*{Acknowledgments}


We thank Melissa Graham and Sam Schmidt for providing the mock datasets.  This 
work was incubated at the 2016 LSST-DESC Hack Week.

\input{acknowledgments}

\input{contributions}


\bibliography{lsstdesc,main}

\end{document}
