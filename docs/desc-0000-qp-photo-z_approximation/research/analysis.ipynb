{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Analysis Pipeline\n",
    "\n",
    "_Alex Malz (NYU) & Phil Marshall (SLAC)_\n",
    "\n",
    "In this notebook we use the \"survey mode\" machinery to demonstrate how one should choose the optimal parametrization for photo-$z$ PDF storage given the nature of the data, the storage constraints, and the fidelity necessary for a science use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#comment out for NERSC\n",
    "%load_ext autoreload\n",
    "\n",
    "#comment out for NERSC\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "    \n",
    "import hickle\n",
    "import numpy as np\n",
    "import random\n",
    "import cProfile\n",
    "import pstats\n",
    "import StringIO\n",
    "import sys\n",
    "import os\n",
    "import timeit\n",
    "import bisect\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#comment out for NERSC\n",
    "%matplotlib inline\n",
    "\n",
    "import qp\n",
    "from qp.utils import calculate_kl_divergence as make_kld\n",
    "\n",
    "# np.random.seed(seed=42)\n",
    "# random.seed(a=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We want to compare parametrizations for large catalogs, so we'll need to be more efficient.  The `qp.Ensemble` object is a wrapper for `qp.PDF` objects enabling conversions to be performed and metrics to be calculated in parallel.  We'll experiment on a subsample of 100 galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_dataset(dataset_key, skip_rows, skip_cols):\n",
    "    start = timeit.default_timer()\n",
    "    with open(dataset_info[dataset_key]['filename'], 'rb') as data_file:\n",
    "        lines = (line.split(None) for line in data_file)\n",
    "        for r in range(skip_rows):\n",
    "            lines.next()\n",
    "        pdfs = np.array([[float(line[k]) for k in range(skip_cols, len(line))] for line in lines])\n",
    "    print('read in data file in '+str(timeit.default_timer()-start))\n",
    "    return(pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_instantiation(dataset_key, n_gals_use, pdfs, bonus=None):\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    n_gals_tot = len(pdfs)\n",
    "    full_gal_range = range(n_gals_tot)\n",
    "    subset = np.random.choice(full_gal_range, n_gals_use, replace=False)#range(n_gals_use)\n",
    "    pdfs_use = pdfs[subset]\n",
    "    \n",
    "    modality = []\n",
    "    dpdfs = pdfs_use[:,1:] - pdfs_use[:,:-1]\n",
    "    iqrs = []\n",
    "    for i in range(n_gals_use):\n",
    "        modality.append(len(np.where(np.diff(np.signbit(dpdfs[i])))[0]))\n",
    "        cdf = np.cumsum(qp.utils.normalize_integral((dataset_info[dataset_key]['z_grid'], pdfs_use[i]), vb=False)[1])\n",
    "        iqr_lo = dataset_info[dataset_key]['z_grid'][bisect.bisect_left(cdf, 0.25)]\n",
    "        iqr_hi = dataset_info[dataset_key]['z_grid'][bisect.bisect_left(cdf, 0.75)]\n",
    "        iqrs.append(iqr_hi - iqr_lo)\n",
    "    modality = np.array(modality)\n",
    "        \n",
    "    dataset_info[dataset_key]['N_GMM'] = int(np.median(modality))\n",
    "    print('n_gmm for '+dataset_info[dataset_key]['name']+' = '+str(dataset_info[dataset_key]['N_GMM']))\n",
    "      \n",
    "    # using the same grid for output as the native format, but doesn't need to be so\n",
    "    dataset_info[dataset_key]['in_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "    dataset_info[dataset_key]['metric_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "    \n",
    "    print('preprocessed data in '+str(timeit.default_timer()-start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, dataset_key+str(n_gals_use)+'pzs_'+bonus)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['randos'] = randos\n",
    "        info['z_grid'] = dataset_info[dataset_key]['in_z_grid']\n",
    "        info['pdfs'] = pdfs_use\n",
    "        info['modes'] = modality\n",
    "        info['iqrs'] = iqrs\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(pdfs_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_examples(n_gals_use, dataset_key, bonus=None):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, dataset_key+str(n_gals_use)+'pzs_'+bonus)\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        randos = info['randos']\n",
    "        z_grid = info['z_grid']\n",
    "        pdfs = info['pdfs']\n",
    "    \n",
    "    plt.figure()\n",
    "    for i in range(n_plot):\n",
    "        data = (z_grid, pdfs[randos[i]])\n",
    "        data = qp.utils.normalize_integral(qp.utils.normalize_gridded(data))\n",
    "        pz_max.append(np.max(data))\n",
    "        plt.plot(data[0], data[1], label=dataset_info[dataset_key]['name']+' #'+str(randos[i]))\n",
    "    plt.xlabel(r'$z$', fontsize=14)\n",
    "    plt.ylabel(r'$p(z)$', fontsize=14)\n",
    "    plt.xlim(min(z_grid), max(z_grid))\n",
    "    plt.ylim(0., max(pz_max))\n",
    "    plt.title(dataset_info[dataset_key]['name']+' data', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.savefig(loc+'.png', dpi=250)\n",
    "    plt.close()\n",
    "    \n",
    "    if 'modes' in info.keys():\n",
    "        modes = info['modes']\n",
    "        modes_max.append(np.max(modes))\n",
    "        plt.figure()\n",
    "        ax = plt.hist(modes, color='k', alpha=1./n_plot, histtype='stepfilled', bins=range(max(modes_max)+1))\n",
    "        plt.xlabel('modes')\n",
    "        plt.ylabel('frequency')\n",
    "        plt.title(dataset_info[dataset_key]['name']+' modality distribution', fontsize=16)\n",
    "        plt.savefig(loc+'modality.png', dpi=250)\n",
    "        plt.close()\n",
    "        \n",
    "    if 'iqrs' in info.keys():\n",
    "        iqrs = info['iqrs']\n",
    "        iqr_min.append(min(iqrs))\n",
    "        iqr_max.append(max(iqrs))\n",
    "        plot_bins = np.linspace(min(iqr_min), max(iqr_max), 20)\n",
    "        plt.figure()\n",
    "        ax = plt.hist(iqrs, bins=plot_bins, color='k', alpha=1./n_plot, histtype='stepfilled')\n",
    "        plt.xlabel('IQR')\n",
    "        plt.ylabel('frequency')\n",
    "        plt.title(dataset_info[dataset_key]['name']+' IQR distribution', fontsize=16)\n",
    "        plt.savefig(loc+'iqrs.png', dpi=250)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by reading in our catalog of gridded PDFs, sampling them, fitting GMMs to the samples, and establishing a new `qp.Ensemble` object where each meber `qp.PDF` object has `qp.PDF.truth`$\\neq$`None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_from_grid(dataset_key, in_pdfs, z_grid, N_comps, high_res=1000, bonus=None):\n",
    "    \n",
    "    #read in the data, happens to be gridded\n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    N_pdfs = len(in_pdfs)\n",
    "    \n",
    "#     plot_examples(N_pdfs, z_grid, pdfs)\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    print('making the initial ensemble of '+str(N_pdfs)+' PDFs')\n",
    "    E0 = qp.Ensemble(N_pdfs, gridded=(z_grid, in_pdfs), limits=dataset_info[dataset_key]['z_lim'], vb=True)\n",
    "    print('made the initial ensemble of '+str(N_pdfs)+' PDFs in '+str(timeit.default_timer() - start))    \n",
    "    \n",
    "    #fit GMMs to gridded pdfs based on samples (faster than fitting to gridded)\n",
    "    start = timeit.default_timer()\n",
    "    print('sampling for the GMM fit')\n",
    "    samparr = E0.sample(high_res, vb=False)\n",
    "    print('took '+str(high_res)+' samples in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    print('making a new ensemble from samples')\n",
    "    Ei = qp.Ensemble(N_pdfs, samples=samparr, limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made a new ensemble from samples in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    print('fitting the GMM to samples')\n",
    "    GMMs = Ei.mix_mod_fit(comps=N_comps, vb=False)\n",
    "    print('fit the GMM to samples in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    #set the GMMS as the truth\n",
    "    start = timeit.default_timer()\n",
    "    print('making the final ensemble')\n",
    "    Ef = qp.Ensemble(N_pdfs, truth=GMMs, limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made the final ensemble in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(N_pdfs))\n",
    "    loc = os.path.join(path, dataset_key+str(n_gals_use)+'pzs_'+bonus)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['randos'] = randos\n",
    "        info['z_grid'] = z_grid\n",
    "        info['pdfs'] = Ef.evaluate(z_grid, using='truth', norm=True, vb=False)[1]\n",
    "        hickle.dump(info, filename)\n",
    "        \n",
    "    start = timeit.default_timer()\n",
    "    print('calculating '+str(n_moments_use)+' moments of original PDFs')\n",
    "    in_moments, vals = [], []\n",
    "    for n in range(n_moments_use):\n",
    "        in_moments.append(Ef.moment(n, using='truth', limits=zlim, \n",
    "                                    dx=delta_z, vb=False))\n",
    "        vals.append(n)\n",
    "    moments = np.array(in_moments)\n",
    "    print('calculated '+str(n_moments_use)+' moments of original PDFs in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(N_pdfs))\n",
    "    loc = os.path.join(path, 'pz_moments_'+bonus+dataset_key+str(N_pdfs))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['truth'] = moments\n",
    "        info['orders'] = vals\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(Ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the KLD between each approximation and the truth for every member of the ensemble.  We make the `qp.Ensemble.kld` into a `qp.PDF` object of its own to compare the moments of the KLD distributions for different parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_individual(E, z_grid, N_floats, dataset_key, N_moments=4, i=None):\n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    z_range = zlim[-1] - zlim[0]\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    \n",
    "    Eq, Eh, Es = E, E, E\n",
    "    inits = {}\n",
    "    for f in formats:\n",
    "        inits[f] = {}\n",
    "        for ff in formats:\n",
    "            inits[f][ff] = None\n",
    "            \n",
    "    qstart = timeit.default_timer()\n",
    "    print('performing quantization')\n",
    "    inits['quantiles']['quantiles'] = Eq.quantize(N=N_floats, vb=True)\n",
    "    print('finished quantization at '+str(timeit.default_timer() - qstart))\n",
    "    hstart = timeit.default_timer()\n",
    "    print('performing histogramization')\n",
    "    inits['histogram']['histogram'] = Eh.histogramize(N=N_floats, binrange=zlim, vb=False)\n",
    "    print('finished histogramization at '+str(timeit.default_timer() - hstart))\n",
    "    sstart = timeit.default_timer()\n",
    "    print('performing sampling')\n",
    "    inits['samples']['samples'] = Es.sample(samps=N_floats, vb=False)\n",
    "    print('finished sampling at '+str(timeit.default_timer() - sstart))\n",
    "        \n",
    "    print('making the approximate ensembles')\n",
    "    Eo = {}\n",
    "    for f in formats:\n",
    "        start = timeit.default_timer()\n",
    "        Eo[f] = qp.Ensemble(E.n_pdfs, truth=E.truth, \n",
    "                            quantiles=inits[f]['quantiles'], \n",
    "                            histogram=inits[f]['histogram'],\n",
    "                            samples=inits[f]['samples'], \n",
    "                            limits=dataset_info[dataset_key]['z_lim'])\n",
    "        bonus = str(N_floats)+f+str(i)\n",
    "        loc = os.path.join(path, dataset_key+str(n_gals_use)+'pzs_'+bonus)\n",
    "        with open(loc+'.hkl', 'w') as filename:\n",
    "            info = {}\n",
    "            info['randos'] = randos\n",
    "            info['z_grid'] = z_grid\n",
    "            info['pdfs'] = Eo[f].evaluate(z_grid, using=f, norm=True, vb=False)[1]\n",
    "            hickle.dump(info, filename)\n",
    "        print('made '+f+' ensemble in '+str(timeit.default_timer()-start))\n",
    "    print('made the approximate ensembles')\n",
    "    \n",
    "    print('calculating the individual metrics')\n",
    "    metric_start = timeit.default_timer()\n",
    "    inloc = os.path.join(path, 'pz_moments_post-fit'+str(i)+dataset_key+str(n_gals_use))\n",
    "    with open(inloc+'.hkl', 'r') as infilename:\n",
    "        pz_moments = hickle.load(infilename)\n",
    "    klds, metrics, kld_moments = {}, {}, {}\n",
    "    \n",
    "    for key in Eo.keys():\n",
    "        key_start = timeit.default_timer()\n",
    "        print('starting '+key)\n",
    "        klds[key] = Eo[key].kld(using=key, limits=zlim, dx=delta_z)\n",
    "        samp_metric = qp.PDF(samples=klds[key])\n",
    "        gmm_metric = samp_metric.mix_mod_fit(n_components=dataset_info[dataset_key]['N_GMM'], \n",
    "                                             using='samples', vb=False)\n",
    "        metrics[key] = qp.PDF(truth=gmm_metric)\n",
    "        \n",
    "        \n",
    "        pz_moments[key], kld_moments[key] = [], []\n",
    "        for n in range(N_moments):\n",
    "            kld_moments[key].append(qp.utils.calculate_moment(metrics[key], n,\n",
    "                                                          using='truth', \n",
    "                                                          limits=zlim, \n",
    "                                                          dx=delta_z, \n",
    "                                                          vb=False))\n",
    "            pz_moments[key].append(Eo[key].moment(n, using=key, limits=zlim, \n",
    "                                                  dx=delta_z, vb=False))\n",
    "        print('finished with '+key+' in '+str(timeit.default_timer() - key_start))\n",
    "    print('calculated the individual metrics in '+str(timeit.default_timer() - metric_start))\n",
    "\n",
    "    loc = os.path.join(path, str(N_floats)+'kld_hist'+str(n_gals_use)+dataset_key+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['z_grid'] = z_grid\n",
    "        info['N_floats'] = N_floats\n",
    "        info['pz_klds'] = klds\n",
    "        hickle.dump(info, filename)\n",
    "\n",
    "    outloc = os.path.join(path, str(N_floats)+'pz_moments'+str(n_gals_use)+dataset_key+str(i))\n",
    "    with open(outloc+'.hkl', 'w') as outfilename:\n",
    "        hickle.dump(pz_moments, outfilename)\n",
    "    \n",
    "    return(Eo, klds, kld_moments, pz_moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_individual_kld(n_gals_use, dataset_key, N_floats, i):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    a = 1./len(formats)\n",
    "    loc = os.path.join(path, str(N_floats)+'kld_hist'+str(n_gals_use)+dataset_key+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        z_grid = info['z_grid']\n",
    "        N_floats = info['N_floats']\n",
    "        pz_klds = info['pz_klds']\n",
    "    \n",
    "    plt.figure()\n",
    "    plot_bins = np.linspace(-3., 3., 20)\n",
    "    for key in pz_klds.keys():\n",
    "        logdata = qp.utils.safelog(pz_klds[key])\n",
    "        kld_hist = plt.hist(logdata, color=colors[key], alpha=a, histtype='stepfilled', edgecolor='k',\n",
    "             label=key, normed=True, bins=plot_bins, linestyle=stepstyles[key], ls=stepstyles[key], lw=3)\n",
    "        hist_max.append(max(kld_hist[0]))\n",
    "        dist_min.append(min(logdata))\n",
    "        dist_max.append(max(logdata))\n",
    "    plt.legend()\n",
    "    plt.ylabel('frequency', fontsize=14)\n",
    "    plt.xlabel(r'$\\log[KLD]$', fontsize=14)\n",
    "#     plt.xlim(min(dist_min), max(dist_max))\n",
    "#     plt.ylim(0., max(hist_max))\n",
    "    plt.title(dataset_info[dataset_key]['name']+r' data $p(KLD)$ with $N_{f}='+str(N_floats)+r'$', fontsize=16)\n",
    "    plt.savefig(loc+'.png', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_individual_moment(n_gals_use, dataset_key, N_floats, i):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    a = 1./len(formats)    \n",
    "    loc = os.path.join(path, str(N_floats)+'pz_moments'+str(n_gals_use)+dataset_key+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        moments = hickle.load(filename)\n",
    "    delta_moments = {}\n",
    "        \n",
    "    plt.figure(figsize=(5, 5 * (n_moments_use-1)))\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax = plt.subplot(n_moments_use, 1, n)\n",
    "        ends = (min(moments['truth'][n]), max(moments['truth'][n]))\n",
    "        for key in formats:\n",
    "            ends = (min(ends[0], min(moments[key][n])), max(ends[-1], max(moments[key][n])))\n",
    "        plot_bins = np.linspace(ends[0], ends[-1], 20)\n",
    "        ax.hist([-100], color='k', alpha=a, histtype='stepfilled', edgecolor='k', label='truth', \n",
    "                    linestyle='-', ls='-')\n",
    "        ax.hist(moments['truth'][n], bins=plot_bins, color='k', alpha=a, histtype='stepfilled', normed=True)\n",
    "        ax.hist(moments['truth'][n], bins=plot_bins, color='k', histtype='step', normed=True, linestyle='-', alpha=a)\n",
    "        for key in formats:\n",
    "            ax.hist([-100], color=colors[key], alpha=a, histtype='stepfilled', edgecolor='k', label=key, \n",
    "                    linestyle=stepstyles[key], ls=stepstyles[key])\n",
    "            ax.hist(moments[key][n], bins=plot_bins, color=colors[key], alpha=a, histtype='stepfilled', normed=True)\n",
    "            ax.hist(moments[key][n], bins=plot_bins, color='k', histtype='step', normed=True, linestyle=stepstyles[key], alpha=a)\n",
    "        ax.legend()\n",
    "        ax.set_ylabel('frequency', fontsize=14)\n",
    "        ax.set_xlabel(moment_names[n], fontsize=14)\n",
    "        ax.set_xlim(min(plot_bins), max(plot_bins))\n",
    "    plt.suptitle(dataset_info[dataset_key]['name']+r' data moments with $N_{f}='+str(N_floats)+r'$', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(loc+'.png', dpi=250)\n",
    "    plt.close()\n",
    "        \n",
    "    plt.figure(figsize=(5, 5 * (n_moments_use-1)))\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax = plt.subplot(n_moments_use, 1, n)\n",
    "        ends = (100., -100.)\n",
    "        for key in formats:\n",
    "            delta_moments[key] = moments[key] - moments['truth']\n",
    "            ends = (min(ends[0], min(delta_moments[key][n])), max(ends[-1], max(delta_moments[key][n])))\n",
    "        plot_bins = np.linspace(ends[0], ends[-1], 20)\n",
    "        for key in formats:\n",
    "            ax.hist([-100], color=colors[key], alpha=a, histtype='stepfilled', edgecolor='k', label=key, \n",
    "                    linestyle=stepstyles[key], ls=stepstyles[key])\n",
    "            ax.hist(delta_moments[key][n], bins=plot_bins, color=colors[key], alpha=a, histtype='stepfilled', normed=True)\n",
    "            ax.hist(delta_moments[key][n], bins=plot_bins, color='k', histtype='step', normed=True, linestyle=stepstyles[key], alpha=a)\n",
    "        ax.legend()\n",
    "        ax.set_ylabel('frequency', fontsize=14)\n",
    "        ax.set_xlabel(r'$\\Delta$ '+moment_names[n], fontsize=14)\n",
    "        ax.set_xlim(min(plot_bins), max(plot_bins))\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(dataset_info[dataset_key]['name']+r' data moment differences with $N_{f}='+str(N_floats)+r'$', fontsize=16)\n",
    "    plt.savefig(loc+'_delta.png', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate metrics on the stacked estimator $\\hat{n}(z)$ that is the average of all members of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyze_stacked(E0, E, z_grid, n_floats_use, dataset_key, i=None):\n",
    "    \n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    z_range = zlim[-1] - zlim[0]\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    \n",
    "    print('stacking the ensembles')\n",
    "    stack_start = timeit.default_timer()\n",
    "    stacked_pdfs, stacks = {}, {}\n",
    "    for key in formats:\n",
    "        start = timeit.default_timer()\n",
    "        stacked_pdfs[key] = qp.PDF(gridded=E[key].stack(z_grid, using=key, \n",
    "                                                        vb=False)[key])\n",
    "        stacks[key] = stacked_pdfs[key].evaluate(z_grid, using='gridded', norm=True, vb=False)[1]\n",
    "        print('stacked '+key+ 'in '+str(timeit.default_timer()-start))\n",
    "    stacked_pdfs['truth'] = qp.PDF(gridded=E0.stack(z_grid, using='truth', \n",
    "                                                    vb=False)['truth'])\n",
    "    \n",
    "    stacks['truth'] = stacked_pdfs['truth'].evaluate(z_grid, using='gridded', norm=True, vb=False)[1]\n",
    "    print('stacked the ensembles in '+str(timeit.default_timer() - stack_start))\n",
    "    \n",
    "    print('calculating the metrics')\n",
    "    metric_start = timeit.default_timer()\n",
    "    klds, moments = {}, {}\n",
    "    moments['truth'] = []\n",
    "    for n in range(n_moments_use):\n",
    "        moments['truth'].append(qp.utils.calculate_moment(stacked_pdfs['truth'], n, \n",
    "                                                          limits=zlim, \n",
    "                                                          dx=delta_z, \n",
    "                                                          vb=False))\n",
    "    for key in formats:\n",
    "        klds[key] = qp.utils.calculate_kl_divergence(stacked_pdfs['truth'],\n",
    "                                                     stacked_pdfs[key], \n",
    "                                                     limits=zlim, dx=delta_z)\n",
    "        moments[key] = []\n",
    "        for n in range(n_moments_use):\n",
    "            moments[key].append(qp.utils.calculate_moment(stacked_pdfs[key], n, \n",
    "                                                          limits=zlim, \n",
    "                                                          dx=delta_z, \n",
    "                                                          vb=False))\n",
    "    print('calculated the metrics in '+str(timeit.default_timer() - metric_start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(E0.n_pdfs))\n",
    "    loc = os.path.join(path, str(n_floats_use)+'nz_comp'+str(n_gals_use)+dataset_key+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['z_grid'] = z_grid\n",
    "        info['stacks'] = stacks\n",
    "        info['klds'] = klds\n",
    "        info['moments'] = moments\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(stacked_pdfs, klds, moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_estimators(n_gals_use, dataset_key, n_floats_use, i=None):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, str(n_floats_use)+'nz_comp'+str(n_gals_use)+dataset_key+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        z_grid = info['z_grid']\n",
    "        stacks = info['stacks']\n",
    "        klds = info['klds']\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(z_grid, stacks['truth'], color='black', lw=4, alpha=0.3, label='truth')\n",
    "    nz_max.append(max(stacks['truth']))\n",
    "    for key in formats:\n",
    "        nz_max.append(max(stacks[key]))\n",
    "        plt.plot(z_grid, stacks[key], label=key+r' KLD='+str(klds[key]), color=colors[key], linestyle=styles[key])\n",
    "    plt.xlabel(r'$z$', fontsize=14)\n",
    "    plt.ylabel(r'$\\hat{n}(z)$', fontsize=14)\n",
    "    plt.xlim(min(z_grid), max(z_grid))\n",
    "#     plt.ylim(0., max(nz_max))\n",
    "    plt.legend()\n",
    "    plt.title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ with $N_{f}='+str(n_floats_use)+r'$', fontsize=16)\n",
    "    plt.savefig(loc+'.png', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the data so we can remake the plots later without running everything again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "We'd like to do this for many values of $N_{f}$ as well as larger catalog subsamples, repeating the analysis many times to establish error bars on the KLD as a function of format, $N_{f}$, and dataset.  The things we want to plot across multiple datasets/number of parametes are:\n",
    "\n",
    "1. KLD of stacked estimator, i.e. `N_f` vs. `nz_output[dataset][format][instantiation][KLD_val_for_N_f]`\n",
    "2. moments of KLD of individual PDFs, i.e. `n_moment, N_f` vs. `pz_output[dataset][format][n_moment][instantiation][moment_val_for_N_f]`\n",
    "\n",
    "So, we ned to make sure these are saved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the moments of the KLD distribution for each format as $N_{f}$ changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_moments(dataset_key, n_gals_use, N_f, stat, stat_name):\n",
    "\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_key)\n",
    "    \n",
    "    if os.path.exists(loc+'.hkl'):\n",
    "        with open(loc+'.hkl', 'r') as stat_file:\n",
    "        #read in content of list/dict\n",
    "            stats = hickle.load(stat_file)\n",
    "    else:\n",
    "        stats = {}\n",
    "        stats['N_f'] = []\n",
    "        for f in stat.keys():\n",
    "            stats[f] = []\n",
    "            for m in range(n_moments_use):\n",
    "                stats[f].append([])\n",
    "\n",
    "    if N_f not in stats['N_f']:\n",
    "        stats['N_f'].append(N_f)\n",
    "        for f in stat.keys():\n",
    "            for m in range(n_moments_use):\n",
    "                stats[f][m].append([])\n",
    "        \n",
    "    where_N_f = stats['N_f'].index(N_f)\n",
    "        \n",
    "    for f in stat.keys():\n",
    "        for m in range(n_moments_use):\n",
    "            stats[f][m][where_N_f].append(stat[f][m])\n",
    "\n",
    "    with open(loc+'.hkl', 'w') as stat_file:\n",
    "        hickle.dump(stats, stat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_pz_metrics(dataset_key, n_gals_use):\n",
    "\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pz_kld_moments'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as pz_file:\n",
    "        pz_stats = hickle.load(pz_file)\n",
    "#     if len(instantiations) == 10:\n",
    "#         for f in formats:\n",
    "#             for n in range(n_moments_use):\n",
    "#                 if not np.shape(pz_stats[f][n]) == (4, 10):\n",
    "#                     for s in range(len(pz_stats[f][n])):\n",
    "#                         pz_stats[f][n][s] = np.array(np.array(pz_stats[f][n][s])[:10]).flatten()\n",
    "        \n",
    "    flat_floats = np.array(pz_stats['N_f']).flatten()\n",
    "    in_x = np.log(flat_floats)\n",
    "\n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)\n",
    "\n",
    "    shapes = moment_shapes\n",
    "    marksize = 50\n",
    "    a = 1./len(formats)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax_n.plot([-1], [0], color=colors[key], label=key, linestyle=styles[key], linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        n_factor = 0.05 * (n - 2)\n",
    "        ax.scatter([-1], [0], color='k', marker=shapes[n], s=marksize, label='moment '+str(n))\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = np.swapaxes(np.array(pz_stats[f][n]), 0, 1)#go from n_floats*instantiations to instantiations*n_floats\n",
    "            for i in data_arr:\n",
    "                ax_n.scatter(np.exp(in_x+n_factor), i, marker=shapes[n], s=marksize, color=colors[f], alpha=a)\n",
    "                moment_max[n-1].append(max(i))\n",
    "        ax_n.set_ylabel(moment_names[n], fontsize=14)\n",
    "        ax_n.set_ylim(0., max(moment_max[n-1]))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\log[KLD]$ moments', fontsize=16)\n",
    "    ax.legend(loc='lower left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_all.png', dpi=250)\n",
    "    plt.close()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "\n",
    "    for key in formats:\n",
    "        ax.plot([-1], [0], color=colors[key], label=key, linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax.scatter([-1], [0], color='k', alpha=1., marker=shapes[n], s=marksize, label=moment_names[n])\n",
    "        n_factor = 0.05 * (n - 2)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = np.log(np.swapaxes(np.array(pz_stats[f][n]), 0, 1))#go from n_floats*instantiations to instantiations*n_floats\n",
    "            mean = np.mean(data_arr, axis=0).flatten()\n",
    "            std = np.std(data_arr, axis=0).flatten()\n",
    "            y_plus = mean + std\n",
    "            y_minus = mean - std\n",
    "            y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "            ax_n.scatter(np.exp(in_x+n_factor), mean, marker=shapes[n], s=marksize, alpha=2. * a, color=colors[f])\n",
    "            ax_n.vlines(np.exp(in_x+n_factor), y_minus, y_plus, linewidth=3., alpha=a, color=colors[f])\n",
    "            mean_max[n] = max(mean_max[n], np.max(y_plus))\n",
    "            mean_min[n] = min(mean_min[n], np.min(y_minus))\n",
    "        ax_n.set_ylabel(moment_names[n], fontsize=14)\n",
    "        ax_n.set_ylim((mean_min[n]-1., mean_max[n]+1.))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\log[KLD]$ moments', fontsize=16)\n",
    "    ax.legend(loc='lower left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_clean.png', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#similar plot with moments of pz moment distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the KLD on $\\hat{n}(z)$ for all formats as $N_{f}$ changes.  We want to repeat this for many subsamples of the catalog to establush error bars on the KLD values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_nz_metrics(dataset_key, n_gals_use, N_f, nz_klds, stat_name):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_key)\n",
    "    if os.path.exists(loc+'.hkl'):\n",
    "        with open(loc+'.hkl', 'r') as nz_file:\n",
    "        #read in content of list/dict\n",
    "            nz_stats = hickle.load(nz_file)\n",
    "    else:\n",
    "        nz_stats = {}\n",
    "        nz_stats['N_f'] = []\n",
    "        for f in formats:\n",
    "            nz_stats[f] = []\n",
    "    \n",
    "    if N_f not in nz_stats['N_f']:\n",
    "        nz_stats['N_f'].append(N_f)\n",
    "        for f in formats:\n",
    "            nz_stats[f].append([])\n",
    "        \n",
    "    where_N_f = nz_stats['N_f'].index(N_f) \n",
    "    \n",
    "    for f in formats:\n",
    "        nz_stats[f][where_N_f].append(nz_klds[f])\n",
    "\n",
    "    with open(loc+'.hkl', 'w') as nz_file:\n",
    "        hickle.dump(nz_stats, nz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_nz_klds(dataset_key, n_gals_use):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_klds'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as nz_file:\n",
    "        nz_stats = hickle.load(nz_file)\n",
    "    if len(instantiations) == 10:\n",
    "        for f in formats:\n",
    "            if not np.shape(nz_stats[f]) == (4, 10):\n",
    "                for s in range(len(floats)):\n",
    "                    nz_stats[f][s] = np.array(np.array(nz_stats[f][s])[:10]).flatten()\n",
    "\n",
    "    flat_floats = np.array(nz_stats['N_f']).flatten()\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for f in formats:\n",
    "        data_arr = np.swapaxes(np.array(nz_stats[f]), 0, 1)#turn N_f * instantiations into instantiations * N_f\n",
    "        n_i = len(data_arr)\n",
    "        a = 1./len(formats)#1./n_i\n",
    "        plt.plot([10. * max(flat_floats), 10. * max(flat_floats)], [1., 10.], color=colors[f], alpha=a, label=f, linestyle=styles[f])\n",
    "        for i in data_arr:\n",
    "            plt.plot(flat_floats, i, color=colors[f], alpha=a, linestyle=styles[f])\n",
    "            kld_min.append(min(i))\n",
    "            kld_max.append(max(i))\n",
    "    plt.semilogy()\n",
    "    plt.semilogx()\n",
    "    plt.xticks(flat_floats, [str(ff) for ff in flat_floats])\n",
    "    plt.ylim(min(kld_min) / 10., 10. *  max(kld_max))\n",
    "    plt.xlim(min(flat_floats) / 3., max(flat_floats) * 3.)\n",
    "    plt.xlabel(r'number of parameters', fontsize=14)\n",
    "    plt.ylabel(r'KLD', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(r'$\\hat{n}(z)$ KLD on '+str(n_gals_use)+' from '+dataset_info[dataset_key]['name']+' mock catalog', fontsize=16)\n",
    "    plt.savefig(loc+'.png', dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    a = 1./len(formats)\n",
    "    for f in formats:\n",
    "        data_arr = np.swapaxes(np.array(nz_stats[f]), 0, 1)#turn N_f * instantiations into instantiations * N_f\n",
    "        plt.plot([10. * max(flat_floats), 10. * max(flat_floats)], [1., 10.], color=colors[f], label=f, linestyle=styles[f])\n",
    "        kld_min.append(np.min(data_arr))\n",
    "        kld_max.append(np.max(data_arr))\n",
    "        mean = np.mean(data_arr, axis=0)\n",
    "        std = np.std(data_arr, axis=0)\n",
    "        x_cor = np.array([flat_floats[:-1], flat_floats[:-1], flat_floats[1:], flat_floats[1:]])\n",
    "        y_plus = mean + std\n",
    "        y_minus = mean - std\n",
    "        y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "        plt.plot(flat_floats, mean, color=colors[f], linestyle=styles[f])\n",
    "        plt.fill(x_cor, y_cor, color=colors[f], alpha=a, linewidth=0.)\n",
    "    plt.semilogy()\n",
    "    plt.semilogx()\n",
    "    plt.xticks(flat_floats, [str(ff) for ff in flat_floats])\n",
    "    plt.ylim(min(kld_min) / 10., 10. *  max(kld_max))\n",
    "    plt.xlim(min(flat_floats), max(flat_floats))\n",
    "    plt.xlabel(r'number of parameters', fontsize=14)\n",
    "    plt.ylabel(r'KLD', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ KLD', fontsize=16)\n",
    "    plt.savefig(loc+'_clean.png', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_nz_moments(dataset_key, n_gals_use):\n",
    "\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_moments'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as nz_file:\n",
    "        nz_stats = hickle.load(nz_file)\n",
    "    flat_floats = np.array(nz_stats['N_f']).flatten()\n",
    "    in_x = np.log(flat_floats)\n",
    "    a = 1./len(formats)\n",
    "    shapes = moment_shapes\n",
    "    marksize = 50\n",
    "    \n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)\n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax_n.plot([-1], [0], color=colors[key], label=key, linestyle=styles[key], linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        n_factor = 0.05 * (n - 2)\n",
    "        ax.scatter([-1], [0], color='k', marker=shapes[n], s=marksize, label='moment '+str(n))\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = data_arr = np.log(nz_stats[f][n])\n",
    "            for i in data_arr:\n",
    "                ax_n.scatter(np.exp(in_x+n_factor), i, marker=shapes[n], s=marksize, color=colors[f], alpha=a)\n",
    "                moment_max[n-1].append(max(i))\n",
    "        ax_n.scatter(np.exp(in_x+n_factor), np.log(nz_stats['truth'][n]), marker=shapes[n], s=marksize, alpha=a, color='k')\n",
    "        ax_n.set_ylabel(moment_names[n], fontsize=14)\n",
    "        ax_n.set_ylim((mean_min[n]-1., mean_max[n]+1.))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ moments', fontsize=16)\n",
    "    ax.legend(loc='lower left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_all.png', dpi=250)\n",
    "    plt.close()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax.plot([-1], [0], color=colors[key], label=key, linewidth=1)\n",
    "    ax.plot([-1], [0], color='k', label='original', linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax.scatter([-1], [0], color='k', alpha=0.5, marker=shapes[n], s=marksize, label=moment_names[n])\n",
    "        n_factor = 0.05 * (n - 2)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = np.log(nz_stats[f][n])#np.log(np.swapaxes(np.array(nz_stats[f]), 0, 1)[:][:][n])#go from n_floats*instantiations to instantiations*n_floats\n",
    "            mean = np.mean(data_arr, axis=0).flatten()\n",
    "            std = np.std(data_arr, axis=0).flatten()\n",
    "            y_plus = mean + std\n",
    "            y_minus = mean - std\n",
    "            y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "            ax_n.scatter(np.exp(in_x+n_factor), mean, marker=shapes[n], s=marksize, alpha=2. * a, color=colors[f])\n",
    "            ax_n.vlines(np.exp(in_x+n_factor), y_minus, y_plus, linewidth=3., alpha=a, color=colors[f])\n",
    "            mean_max[n] = max(mean_max[n], np.max(y_plus))\n",
    "            mean_min[n] = min(mean_min[n], np.min(y_minus))\n",
    "        ax_n.scatter(np.exp(in_x+n_factor), np.log(nz_stats['truth'][n]), marker=shapes[n], s=marksize, alpha=a, color='k')\n",
    "        ax_n.set_ylabel(moment_names[n], fontsize=14)\n",
    "        ax_n.set_ylim((mean_min[n]-1., mean_max[n]+1.))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ moments', fontsize=16)\n",
    "    ax.legend(loc='lower left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_clean.png', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Okay, now all I have to do is have this loop over both datasets, number of galaxies, number of floats, and instantiations!\n",
    "\n",
    "Note: It takes about 5 minutes per \\# floats considered for 100 galaxies, and about 40 minutes per \\# floats for 1000 galaxies.  (So, yes, it scales more or less as expected!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_info = {}\n",
    "delta = 0.01\n",
    "\n",
    "dataset_keys = ['mg', 'ss']\n",
    "\n",
    "for dataset_key in dataset_keys:\n",
    "    dataset_info[dataset_key] = {}\n",
    "    if dataset_key == 'mg':\n",
    "        datafilename = 'bpz_euclid_test_10_3.probs'\n",
    "        z_low = 0.01\n",
    "        z_high = 3.51\n",
    "#         nc_needed = 3\n",
    "        plotname = 'brighter'\n",
    "        skip_rows = 1\n",
    "        skip_cols = 1\n",
    "    elif dataset_key == 'ss':\n",
    "        datafilename = 'test_magscat_trainingfile_probs.out'\n",
    "        z_low = 0.005\n",
    "        z_high = 2.11\n",
    "#         nc_needed = 5\n",
    "        plotname = 'fainter'\n",
    "        skip_rows = 1\n",
    "        skip_cols = 1\n",
    "    dataset_info[dataset_key]['filename'] = datafilename  \n",
    "    \n",
    "    dataset_info[dataset_key]['z_lim'] = (z_low, z_high)\n",
    "    z_grid = np.arange(z_low, z_high, delta, dtype='float')#np.arange(z_low, z_high + delta, delta, dtype='float')\n",
    "    z_range = z_high - z_low\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    dataset_info[dataset_key]['z_grid'] = z_grid\n",
    "    dataset_info[dataset_key]['delta_z'] = delta_z\n",
    "\n",
    "#     dataset_info[dataset_key]['N_GMM'] = nc_needed\n",
    "    dataset_info[dataset_key]['name'] = plotname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_res = 300\n",
    "n_plot = 5\n",
    "n_moments_use = 4\n",
    "moment_names = ['integral', 'mean', 'variance', 'kurtosis']\n",
    "moment_shapes = ['o', '*', '+', 'x']\n",
    "\n",
    "#make this a more clever structure, i.e. a dict\n",
    "formats = ['quantiles', 'histogram', 'samples']\n",
    "colors = {'quantiles': 'blueviolet', 'histogram': 'darkorange', 'samples': 'forestgreen'}\n",
    "styles = {'quantiles': '--', 'histogram': ':', 'samples': '-.'}\n",
    "stepstyles = {'quantiles': 'dashed', 'histogram': 'dotted', 'samples': 'dashdot'}\n",
    "\n",
    "iqr_min = [3.5]\n",
    "iqr_max = [delta]\n",
    "modes_max = [0]\n",
    "pz_max = [1.]\n",
    "nz_max = [1.]\n",
    "hist_max = [1.]\n",
    "dist_min = [0.]\n",
    "dist_max = [0.]\n",
    "moment_max = [[]] * (n_moments_use)\n",
    "mean_max = -10.*np.ones(n_moments_use)\n",
    "mean_min = 10.*np.ones(n_moments_use)\n",
    "kld_min = [1.]\n",
    "kld_max = [1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change all for NERSC\n",
    "\n",
    "floats = [3]#[3, 10, 30, 100]\n",
    "sizes = [10]#[100]#[100, 1000, 10000]\n",
    "names = dataset_info.keys()\n",
    "instantiations = range(0, 1)#range(0, 10)\n",
    "\n",
    "all_randos = [[np.random.choice(size, n_plot, replace=False) for size in sizes] for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"pipeline\" is a bunch of nested `for` loops because `qp.Ensemble` makes heavy use of multiprocessing.  Doing multiprocessing within multiprocessing may or may not cause problems, but I am certain that it makes debugging a nightmare.\n",
    "\n",
    "Okay, without further ado, let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the \"pipeline\"\n",
    "\n",
    "for n in range(len(names)):\n",
    "    name = names[n]\n",
    "    \n",
    "    dataset_start = timeit.default_timer()\n",
    "    print('started '+name)\n",
    "    \n",
    "    pdfs = setup_dataset(name, skip_rows, skip_cols)\n",
    "    \n",
    "    for s in range(len(sizes)):\n",
    "        size=sizes[s]\n",
    "        \n",
    "        size_start = timeit.default_timer()\n",
    "        print('started '+str(size)+name)\n",
    "        \n",
    "        path = os.path.join(name, str(size))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        n_gals_use = size\n",
    "        \n",
    "        randos = all_randos[n][s]\n",
    "        \n",
    "        for i in instantiations:\n",
    "        \n",
    "            original = 'original'+str(i)\n",
    "            pdfs_use = make_instantiation(name, size, pdfs, bonus=original)\n",
    "            plot = plot_examples(size, name, bonus=original)\n",
    "        \n",
    "            z_grid = dataset_info[name]['in_z_grid']\n",
    "            N_comps = dataset_info[name]['N_GMM']\n",
    "        \n",
    "            postfit = 'post-fit'+str(i)\n",
    "            catalog = setup_from_grid(name, pdfs_use, z_grid, N_comps, high_res=high_res, bonus=postfit)\n",
    "            plot = plot_examples(size, name, bonus=postfit)\n",
    "        \n",
    "            for n_floats_use in floats:\n",
    "            \n",
    "                float_start = timeit.default_timer()\n",
    "                print('started '+str(size)+name+str(n_floats_use)+'#'+str(i))\n",
    "        \n",
    "                (ensembles, pz_klds, metric_moments, pz_moments) = analyze_individual(catalog, \n",
    "                                                          z_grid,#dataset_info[name]['metric_z_grid'], \n",
    "                                                          n_floats_use, name, n_moments_use, i=i)\n",
    "                for f in formats:\n",
    "                    fname = str(n_floats_use)+f+str(i)\n",
    "                    plot = plot_examples(size, name, bonus=fname)\n",
    "                plot = plot_individual_kld(size, name, n_floats_use, i=i)\n",
    "                plot = plot_individual_moment(size, name, n_floats_use, i=i)\n",
    "                save_moments(name, size, n_floats_use, metric_moments, 'pz_kld_moments')\n",
    "                save_moments(name, size, n_floats_use, pz_moments, 'pz_moments')\n",
    "            \n",
    "                (stack_evals, nz_klds, nz_moments) = analyze_stacked(catalog, ensembles, z_grid,#dataset_info[name]['metric_z_grid'], \n",
    "                                                     n_floats_use, name, i=i)\n",
    "                plot = plot_estimators(size, name, n_floats_use, i=i)\n",
    "                save_nz_metrics(name, size, n_floats_use, nz_klds, 'nz_klds')\n",
    "                save_moments(name, size, n_floats_use, nz_moments, 'nz_moments')\n",
    "            \n",
    "                print('finished '+str(size)+name+str(n_floats_use)+' in '+str(timeit.default_timer() - float_start))\n",
    "        \n",
    "        plot = plot_pz_metrics(name, size)\n",
    "        \n",
    "        plot = plot_nz_klds(name, size)\n",
    "        plot = plot_nz_moments(name, size)\n",
    "        \n",
    "        print('finished '+str(size)+name+' in '+str(timeit.default_timer() - size_start))\n",
    "        \n",
    "    print('finished '+name+' in '+str(timeit.default_timer() - dataset_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remake the plots to share axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#comment out for NERSC\n",
    "\n",
    "for name in names:\n",
    "    for size in sizes:\n",
    "        path = os.path.join(name, str(size))\n",
    "        for i in instantiations:\n",
    "            \n",
    "            plot = plot_examples(size, name, bonus='original'+str(i))\n",
    "            plot = plot_examples(size, name, bonus='post-fit'+str(i))\n",
    "            \n",
    "            for n_floats_use in floats:\n",
    "            \n",
    "                for f in formats:\n",
    "                    fname = str(n_floats_use)+f+str(i)\n",
    "                    plot = plot_examples(size, name, bonus=fname)\n",
    "                plot = plot_individual_kld(size, name, n_floats_use, i)\n",
    "                plot = plot_individual_moment(size, name, n_floats_use, i)\n",
    "                plot = plot_estimators(size, name, n_floats_use, i)\n",
    "            \n",
    "        plot = plot_pz_metrics(name, size)\n",
    "        \n",
    "        plot = plot_nz_klds(name, size)\n",
    "        plot = plot_nz_moments(name, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
