{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Analysis Pipeline\n",
    "\n",
    "_Alex Malz (NYU) & Phil Marshall (SLAC)_\n",
    "\n",
    "In this notebook we use the \"survey mode\" machinery to demonstrate how one should choose the optimal parametrization for photo-$z$ PDF storage given the nature of the data, the storage constraints, and the fidelity necessary for a science use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#comment out for NERSC\n",
    "%load_ext autoreload\n",
    "\n",
    "#comment out for NERSC\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "    \n",
    "import pickle\n",
    "import hickle\n",
    "import numpy as np\n",
    "import random\n",
    "import cProfile\n",
    "import pstats\n",
    "import StringIO\n",
    "import sys\n",
    "import os\n",
    "import timeit\n",
    "import bisect\n",
    "import re\n",
    "\n",
    "import qp\n",
    "from qp.utils import calculate_kl_divergence as make_kld\n",
    "\n",
    "# np.random.seed(seed=42)\n",
    "# random.seed(a=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['mathtext.rm'] = 'serif'\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = 'Times New Roman'\n",
    "mpl.rcParams['axes.titlesize'] = 16\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['savefig.dpi'] = 250\n",
    "mpl.rcParams['savefig.format'] = 'pdf'\n",
    "mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "#comment out for NERSC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We want to compare parametrizations for large catalogs, so we'll need to be more efficient.  The `qp.Ensemble` object is a wrapper for `qp.PDF` objects enabling conversions to be performed and metrics to be calculated in parallel.  We'll experiment on a subsample of 100 galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_dataset(dataset_key, skip_rows, skip_cols):\n",
    "    start = timeit.default_timer()\n",
    "    with open(dataset_info[dataset_key]['filename'], 'rb') as data_file:\n",
    "        lines = (line.split(None) for line in data_file)\n",
    "        for r in range(skip_rows):\n",
    "            lines.next()\n",
    "        pdfs = np.array([[float(line[k]) for k in range(skip_cols, len(line))] for line in lines])\n",
    "    print('read in data file in '+str(timeit.default_timer()-start))\n",
    "    return(pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_instantiation(dataset_key, n_gals_use, pdfs, bonus=None):\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    n_gals_tot = len(pdfs)\n",
    "    full_gal_range = range(n_gals_tot)\n",
    "    subset = np.random.choice(full_gal_range, n_gals_use, replace=False)#range(n_gals_use)\n",
    "#     subset = indices\n",
    "    print('randos for debugging: '+str(subset))\n",
    "    pdfs_use = pdfs[subset]\n",
    "    \n",
    "    modality = []\n",
    "    dpdfs = pdfs_use[:,1:] - pdfs_use[:,:-1]\n",
    "    iqrs = []\n",
    "    for i in range(n_gals_use):\n",
    "        modality.append(len(np.where(np.diff(np.signbit(dpdfs[i])))[0]))\n",
    "        cdf = np.cumsum(qp.utils.normalize_integral((dataset_info[dataset_key]['z_grid'], pdfs_use[i]), vb=False)[1])\n",
    "        iqr_lo = dataset_info[dataset_key]['z_grid'][bisect.bisect_left(cdf, 0.25)]\n",
    "        iqr_hi = dataset_info[dataset_key]['z_grid'][bisect.bisect_left(cdf, 0.75)]\n",
    "        iqrs.append(iqr_hi - iqr_lo)\n",
    "    modality = np.array(modality)\n",
    "        \n",
    "    dataset_info[dataset_key]['N_GMM'] = int(np.median(modality))+1\n",
    "#     print('n_gmm for '+dataset_info[dataset_key]['name']+' = '+str(dataset_info[dataset_key]['N_GMM']))\n",
    "      \n",
    "    # using the same grid for output as the native format, but doesn't need to be so\n",
    "    dataset_info[dataset_key]['in_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "    dataset_info[dataset_key]['metric_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "    \n",
    "    print('preprocessed data in '+str(timeit.default_timer()-start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['randos'] = randos\n",
    "        info['z_grid'] = dataset_info[dataset_key]['in_z_grid']\n",
    "        info['pdfs'] = pdfs_use\n",
    "        info['modes'] = modality\n",
    "        info['iqrs'] = iqrs\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(pdfs_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_examples(n_gals_use, dataset_key, bonus=None, norm=False):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        randos = info['randos']\n",
    "        z_grid = info['z_grid']\n",
    "        pdfs = info['pdfs']\n",
    "    \n",
    "    plt.figure()\n",
    "    for i in range(n_plot):\n",
    "        data = (z_grid, pdfs[randos[i]])\n",
    "        data = qp.utils.normalize_integral(qp.utils.normalize_gridded(data))\n",
    "        pz_max.append(np.max(data))\n",
    "        plt.plot(data[0], data[1], label=dataset_info[dataset_key]['name']+' \\#'+str(randos[i]), color=color_cycle[i])\n",
    "    plt.xlabel(r'$z$', fontsize=14)\n",
    "    plt.ylabel(r'$p(z)$', fontsize=14)\n",
    "    plt.xlim(min(z_grid), max(z_grid))\n",
    "    plt.title(dataset_info[dataset_key]['name']+' data examples', fontsize=16)\n",
    "    if norm:\n",
    "        plt.ylim(0., max(pz_max))\n",
    "        plt.savefig(loc+'norm.pdf', dpi=250)\n",
    "    else:\n",
    "        plt.savefig(loc+'.pdf', dpi=250)\n",
    "    plt.close()\n",
    "    \n",
    "    if 'modes' in info.keys():\n",
    "        modes = info['modes']\n",
    "        modes_max.append(np.max(modes))\n",
    "        plt.figure()\n",
    "        ax = plt.hist(modes, color='k', alpha=1./n_plot, histtype='stepfilled', bins=range(max(modes_max)+1))\n",
    "        plt.xlabel('modes')\n",
    "        plt.ylabel('frequency')\n",
    "        plt.title(dataset_info[dataset_key]['name']+' data modality distribution (median='+str(dataset_info[dataset_key]['N_GMM'])+')', fontsize=16)\n",
    "        plt.savefig(loc+'modality.pdf', dpi=250)\n",
    "        plt.close()\n",
    "        \n",
    "    if 'iqrs' in info.keys():\n",
    "        iqrs = info['iqrs']\n",
    "        iqr_min.append(min(iqrs))\n",
    "        iqr_max.append(max(iqrs))\n",
    "        plot_bins = np.linspace(min(iqr_min), max(iqr_max), 20)\n",
    "        plt.figure()\n",
    "        ax = plt.hist(iqrs, bins=plot_bins, color='k', alpha=1./n_plot, histtype='stepfilled')\n",
    "        plt.xlabel('IQR')\n",
    "        plt.ylabel('frequency')\n",
    "        plt.title(dataset_info[dataset_key]['name']+' data IQR distribution', fontsize=16)\n",
    "        plt.savefig(loc+'iqrs.pdf', dpi=250)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to incrementally save the quantities that are costly to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_one_stat(dataset_name, n_gals_use, N_f, i, stat, stat_name):\n",
    "    path = os.path.join(dataset_name, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_name+str(N_f)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        hickle.dump(stat, filename)\n",
    "        \n",
    "def load_one_stat(dataset_name, n_gals_use, N_f, i, stat_name):\n",
    "    path = os.path.join(dataset_name, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_name+str(N_f)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        stat = hickle.load(filename)\n",
    "#     print(stat)\n",
    "    return stat\n",
    "\n",
    "def save_moments_wrapper(dataset_name, n_gals_use, N_f, i, stat_name):\n",
    "    stat = load_one_stat(dataset_name, n_gals_use, N_f, i, stat_name)\n",
    "    save_moments(dataset_name, n_gals_use, N_f, stat, stat_name)\n",
    "        \n",
    "def save_metrics_wrapper(dataset_name, n_gals_use, N_f, i, stat_name):\n",
    "    stat = load_one_stat(dataset_name, n_gals_use, N_f, i, stat_name)\n",
    "    save_nz_metrics(dataset_name, n_gals_use, N_f, stat, stat_name)\n",
    "    \n",
    "def clear_stats(dataset_name, n_gals_use, stat_name):\n",
    "    path = os.path.join(dataset_name, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_name+'.hkl')\n",
    "    if os.path.isfile(loc):\n",
    "        os.remove(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by reading in our catalog of gridded PDFs, sampling them, fitting GMMs to the samples, and establishing a new `qp.Ensemble` object where each meber `qp.PDF` object has `qp.PDF.truth`$\\neq$`None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_from_grid(dataset_key, in_pdfs, z_grid, N_comps, high_res=1000, bonus=None):\n",
    "    \n",
    "    #read in the data, happens to be gridded\n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    N_pdfs = len(in_pdfs)\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "#     print('making the initial ensemble of '+str(N_pdfs)+' PDFs')\n",
    "    E0 = qp.Ensemble(N_pdfs, gridded=(z_grid, in_pdfs), limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made the initial ensemble of '+str(N_pdfs)+' PDFs in '+str(timeit.default_timer() - start))    \n",
    "    \n",
    "    #fit GMMs to gridded pdfs based on samples (faster than fitting to gridded)\n",
    "    start = timeit.default_timer()\n",
    "#     print('sampling for the GMM fit')\n",
    "    samparr = E0.sample(high_res, vb=False)\n",
    "    print('took '+str(high_res)+' samples in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "#     print('making a new ensemble from samples')\n",
    "    Ei = qp.Ensemble(N_pdfs, samples=samparr, limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made a new ensemble from samples in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "#     print('fitting the GMM to samples')\n",
    "    GMMs = Ei.mix_mod_fit(comps=N_comps, vb=False)\n",
    "    print('fit the GMM to samples in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    #set the GMMS as the truth\n",
    "    start = timeit.default_timer()\n",
    "#     print('making the final ensemble')\n",
    "    Ef = qp.Ensemble(N_pdfs, truth=GMMs, limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made the final ensemble in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(N_pdfs))\n",
    "    loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['randos'] = randos\n",
    "        info['z_grid'] = z_grid\n",
    "        info['pdfs'] = Ef.evaluate(z_grid, using='truth', norm=True, vb=False)[1]\n",
    "        hickle.dump(info, filename)\n",
    "        \n",
    "    start = timeit.default_timer()\n",
    "#     print('calculating '+str(n_moments_use)+' moments of original PDFs')\n",
    "    in_moments, vals = [], []\n",
    "    for n in range(n_moments_use):\n",
    "        in_moments.append(Ef.moment(n, using='truth', limits=zlim, \n",
    "                                    dx=delta_z, vb=False))\n",
    "        vals.append(n)\n",
    "    moments = np.array(in_moments)\n",
    "    print('calculated '+str(n_moments_use)+' moments of original PDFs in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(N_pdfs))\n",
    "    loc = os.path.join(path, 'pz_moments'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['truth'] = moments\n",
    "        info['orders'] = vals\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(Ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the KLD between each approximation and the truth for every member of the ensemble.  We make the `qp.Ensemble.kld` into a `qp.PDF` object of its own to compare the moments of the KLD distributions for different parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyze_individual(E, z_grid, N_floats, dataset_key, N_moments=4, i=None, bonus=None):\n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    z_range = zlim[-1] - zlim[0]\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    \n",
    "    Eq, Eh, Es = E, E, E\n",
    "    inits = {}\n",
    "    for f in formats:\n",
    "        inits[f] = {}\n",
    "        for ff in formats:\n",
    "            inits[f][ff] = None\n",
    "            \n",
    "    qstart = timeit.default_timer()\n",
    "    inits['quantiles']['quantiles'] = Eq.quantize(N=N_floats, vb=False)\n",
    "    print('finished quantization in '+str(timeit.default_timer() - qstart))\n",
    "    hstart = timeit.default_timer()\n",
    "    inits['histogram']['histogram'] = Eh.histogramize(N=N_floats, binrange=zlim, vb=False)\n",
    "    print('finished histogramization in '+str(timeit.default_timer() - hstart))\n",
    "    sstart = timeit.default_timer()\n",
    "    inits['samples']['samples'] = Es.sample(samps=N_floats, vb=False)\n",
    "    print('finished sampling in '+str(timeit.default_timer() - sstart))\n",
    "        \n",
    "    Eo = {}\n",
    "    \n",
    "    metric_start = timeit.default_timer()\n",
    "    inloc = os.path.join(path, 'pz_moments'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(inloc+'.hkl', 'r') as infilename:\n",
    "        pz_moments = hickle.load(infilename)\n",
    "    \n",
    "    klds, metrics, kld_moments, pz_moment_deltas = {}, {}, {}, {}\n",
    "    \n",
    "    for f in formats:\n",
    "        fstart = timeit.default_timer()\n",
    "        Eo[f] = qp.Ensemble(E.n_pdfs, truth=E.truth, \n",
    "                            quantiles=inits[f]['quantiles'], \n",
    "                            histogram=inits[f]['histogram'],\n",
    "                            samples=inits[f]['samples'], \n",
    "                            limits=dataset_info[dataset_key]['z_lim'])\n",
    "        \n",
    "        fbonus = str(N_floats)+f+str(i)\n",
    "        loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+fbonus)\n",
    "        with open(loc+'.hkl', 'w') as filename:\n",
    "            info = {}\n",
    "            info['randos'] = randos\n",
    "            info['z_grid'] = z_grid\n",
    "            info['pdfs'] = Eo[f].evaluate(z_grid, using=f, norm=True, vb=False)[1]\n",
    "            hickle.dump(info, filename)\n",
    "        print('made '+f+' ensemble in '+str(timeit.default_timer()-fstart))\n",
    "\n",
    "        key = f\n",
    "        \n",
    "        fstart = timeit.default_timer()\n",
    "        klds[key] = Eo[key].kld(using=key, limits=zlim, dx=delta_z, vb=False)\n",
    "        print('calculated the '+key+' individual klds in '+str(timeit.default_timer() - fstart))\n",
    "        \n",
    "        fstart = timeit.default_timer()\n",
    "        kld_moments[key] = []\n",
    "        samp_metric = qp.PDF(samples=klds[key])\n",
    "        gmm_metric = samp_metric.mix_mod_fit(n_components=dataset_info[dataset_key]['N_GMM'], \n",
    "                                             using='samples', vb=False)\n",
    "        metrics[key] = qp.PDF(truth=gmm_metric)\n",
    "        for n in range(N_moments):\n",
    "            kld_moments[key].append(qp.utils.calculate_moment(metrics[key], n,\n",
    "                                                          using='truth', \n",
    "                                                          limits=zlim, \n",
    "                                                          dx=delta_z, \n",
    "                                                          vb=False))\n",
    "        save_one_stat(name, size, n_floats_use, i, kld_moments, 'pz_kld_moments')\n",
    "        print('calculated the '+key+' kld moments in '+str(timeit.default_timer() - fstart))\n",
    "        \n",
    "        pz_moment_deltas[key], pz_moments[key] = [], []\n",
    "        for n in range(N_moments):\n",
    "            start = timeit.default_timer()\n",
    "            new_moment = Eo[key].moment(n, using=key, limits=zlim, \n",
    "                                                  dx=delta_z, vb=False)\n",
    "            pz_moments[key].append(new_moment)\n",
    "            #NOTE: delta_moment is crazy for clean data!\n",
    "            delta_moment = (new_moment - pz_moments['truth'][n]) / pz_moments['truth'][n]\n",
    "            pz_moment_deltas[key].append(delta_moment)\n",
    "            print('calculated the '+key+' individual moment '+str(n)+' in '+str(timeit.default_timer() - start))\n",
    "        save_one_stat(name, size, n_floats_use, i, pz_moments, 'pz_moments')\n",
    "        save_one_stat(name, size, n_floats_use, i, pz_moment_deltas, 'pz_moment_deltas')\n",
    "        \n",
    "    loc = os.path.join(path, 'kld_hist'+str(n_gals_use)+dataset_key+str(N_floats)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['z_grid'] = z_grid\n",
    "        info['N_floats'] = N_floats\n",
    "        info['pz_klds'] = klds\n",
    "        hickle.dump(info, filename)\n",
    "\n",
    "    outloc = os.path.join(path, 'pz_moments'+str(n_gals_use)+dataset_key+str(N_floats)+'_'+str(i))\n",
    "    with open(outloc+'.hkl', 'w') as outfilename:\n",
    "        hickle.dump(pz_moments, outfilename)\n",
    "    \n",
    "#     save_moments(name, size, n_floats_use, kld_moments, 'pz_kld_moments')\n",
    "#     save_moments(name, size, n_floats_use, pz_moments, 'pz_moments')\n",
    "#     save_moments(name, size, n_floats_use, pz_moment_deltas, 'pz_moment_deltas')\n",
    "    \n",
    "    return(Eo)#, klds, kld_moments, pz_moments, pz_moment_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_all_examples(name, size, N_floats, init, bonus={}):\n",
    "    path = os.path.join(name, str(size))\n",
    "    fig, ax = plt.subplots()\n",
    "#     fig_check, ax_check = plt.subplots()\n",
    "    lines = []\n",
    "    loc = os.path.join(path, 'pzs'+str(size)+name+'_postfit'+str(init))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        ref_pdfs = info['pdfs']  \n",
    "#     klds = {}\n",
    "    for bonus_key in bonus.keys():\n",
    "        loc = os.path.join(path, 'pzs'+str(size)+name+bonus_key)\n",
    "        with open(loc+'.hkl', 'r') as filename:\n",
    "            info = hickle.load(filename)\n",
    "            randos = info['randos']\n",
    "            z_grid = info['z_grid']\n",
    "            pdfs = info['pdfs']\n",
    "        ls = bonus[bonus_key][0]\n",
    "        a = bonus[bonus_key][1]\n",
    "        lab = re.sub(r'[\\_]', '', bonus_key)\n",
    "        line, = ax.plot([-1., 0.], [0., 0.], linestyle=ls, alpha=a, color='k', label=lab[:-1])\n",
    "        lines.append(line)\n",
    "        leg = ax.legend(loc='upper right', handles=lines)\n",
    "#         klds[bonus_key] = []\n",
    "        for i in range(n_plot):\n",
    "            data = (z_grid, pdfs[randos[i]])\n",
    "            data = qp.utils.normalize_integral(qp.utils.normalize_gridded(data))\n",
    "            ax.plot(data[0], data[1], linestyle=ls, alpha=a, color=color_cycle[i])\n",
    "            #     ax.legend(loc='upper right')\n",
    "#         for i in range(size):\n",
    "#             data = (z_grid, pdfs[i])\n",
    "#             kld = qp.utils.quick_kl_divergence(ref_pdfs[i], pdfs[i], dx=0.01)\n",
    "#             klds[bonus_key].append(kld)\n",
    "#     plot_bins = np.linspace(-3., 3., 20)\n",
    "#     for bonus_key in bonus.keys()[1:-1]:\n",
    "#         ax_check.hist(np.log(np.array(klds[bonus_key])), alpha=a, \n",
    "#                       histtype='stepfilled', edgecolor='k', \n",
    "#                       label=bonus_key, normed=True, bins=plot_bins, lw=2)\n",
    "    ax.set_xlabel(r'$z$', fontsize=14)\n",
    "    ax.set_ylabel(r'$p(z)$', fontsize=14)\n",
    "    ax.set_xlim(min(z_grid), max(z_grid))\n",
    "    ax.set_title(dataset_info[name]['name']+r' examples with $N_{f}=$'+str(N_floats), fontsize=16)\n",
    "    saveloc = os.path.join(path, 'pzs'+str(size)+name+str(N_floats)+'all'+str(init))\n",
    "    fig.savefig(saveloc+'.pdf', dpi=250)\n",
    "#     ax_check.legend()\n",
    "#     ax_check.set_ylabel('frequency', fontsize=14)\n",
    "#     ax_check.set_xlabel(r'$\\mathrm{KLD}$', fontsize=14)\n",
    "#     ax_check.set_title(name+r' data $p(\\mathrm{KLD})$ with $N_{f}='+str(N_floats)+r'$', fontsize=16)\n",
    "#     fig_check.savefig(saveloc+'kld_check.pdf', dpi=250)\n",
    "    plt.close()\n",
    "#     with open(saveloc+'.p', 'w') as kldfile:\n",
    "#         pickle.dump(klds, kldfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_individual_kld(n_gals_use, dataset_key, N_floats, i):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    a = 1./len(formats)\n",
    "    loc = os.path.join(path, 'kld_hist'+str(n_gals_use)+dataset_key+str(N_floats)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        z_grid = info['z_grid']\n",
    "        N_floats = info['N_floats']\n",
    "        pz_klds = info['pz_klds']\n",
    "    \n",
    "    plt.figure()\n",
    "    plot_bins = np.linspace(-10., 5., 30)\n",
    "    for key in pz_klds.keys():\n",
    "        logdata = qp.utils.safelog(pz_klds[key])\n",
    "        dist_min.append(min(logdata))\n",
    "        dist_max.append(max(logdata))\n",
    "#         plot_bins = np.linspace(-10., 5., 20)\n",
    "        kld_hist = plt.hist(logdata, color=colors[key], alpha=a, histtype='stepfilled', edgecolor='k',\n",
    "             label=key, normed=True, bins=plot_bins, linestyle=stepstyles[key], ls=stepstyles[key], lw=2)\n",
    "#         kld_hist = plt.hist(pz_klds[key], color=colors[key], alpha=a, histtype='stepfilled', edgecolor='k',\n",
    "#              label=key, normed=True, bins=plot_bins, linestyle=stepstyles[key], ls=stepstyles[key], lw=2)\n",
    "        hist_max.append(max(kld_hist[0]))\n",
    "    plt.legend()\n",
    "    plt.ylabel('frequency', fontsize=14)\n",
    "#     plt.xlabel(r'$\\log[\\mathrm{KLD}]$', fontsize=14)\n",
    "    plt.xlabel(r'$\\mathrm{KLD}$', fontsize=14)\n",
    "#     plt.xlim(min(dist_min), max(dist_max))\n",
    "#     plt.ylim(0., max(hist_max))\n",
    "    plt.title(dataset_info[dataset_key]['name']+r' data $p(\\mathrm{KLD})$ with $N_{f}='+str(N_floats)+r'$', fontsize=16)\n",
    "    plt.savefig(loc+'.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate metrics on the stacked estimator $\\hat{n}(z)$ that is the average of all members of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyze_stacked(E0, E, z_grid, n_floats_use, dataset_key, i=None):\n",
    "    \n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    z_range = zlim[-1] - zlim[0]\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    \n",
    "    n_gals_use = E0.n_pdfs\n",
    "    \n",
    "#     print('stacking the ensembles')\n",
    "#     stack_start = timeit.default_timer()\n",
    "    stacked_pdfs, stacks = {}, {}\n",
    "    for key in formats:\n",
    "        start = timeit.default_timer()\n",
    "        stacked_pdfs[key] = qp.PDF(gridded=E[key].stack(z_grid, using=key, \n",
    "                                                        vb=False)[key])\n",
    "        stacks[key] = stacked_pdfs[key].evaluate(z_grid, using='gridded', norm=True, vb=False)[1]\n",
    "        print('stacked '+key+ ' in '+str(timeit.default_timer()-start))\n",
    "    \n",
    "    stack_start = timeit.default_timer()\n",
    "    stacked_pdfs['truth'] = qp.PDF(gridded=E0.stack(z_grid, using='truth', \n",
    "                                                    vb=False)['truth'])\n",
    "    \n",
    "    stacks['truth'] = stacked_pdfs['truth'].evaluate(z_grid, using='gridded', norm=True, vb=False)[1]\n",
    "    print('stacked truth in '+str(timeit.default_timer() - stack_start))\n",
    "    \n",
    "    klds = {}\n",
    "    for key in formats:\n",
    "        kld_start = timeit.default_timer()\n",
    "        klds[key] = qp.utils.calculate_kl_divergence(stacked_pdfs['truth'],\n",
    "                                                     stacked_pdfs[key], \n",
    "                                                     limits=zlim, dx=delta_z)\n",
    "        print('calculated the '+key+' stacked kld in '+str(timeit.default_timer() - kld_start))\n",
    "    save_one_stat(dataset_key, n_gals_use, n_floats_use, i, klds, 'nz_klds')\n",
    "#     save_nz_metrics(name, size, n_floats_use, klds, 'nz_klds')\n",
    "        \n",
    "    moments = {}\n",
    "    for key in formats_plus:\n",
    "        moment_start = timeit.default_timer()\n",
    "        moments[key] = []\n",
    "        for n in range(n_moments_use):\n",
    "            moments[key].append(qp.utils.calculate_moment(stacked_pdfs[key], n, \n",
    "                                                          limits=zlim, \n",
    "                                                          dx=delta_z, \n",
    "                                                          vb=False))\n",
    "        print('calculated the '+key+' stacked moments in '+str(timeit.default_timer() - moment_start))\n",
    "    save_one_stat(dataset_key, n_gals_use, n_floats_use, i, moments, 'nz_moments')\n",
    "#     save_moments(name, size, n_floats_use, moments, 'nz_moments') \n",
    "    \n",
    "    path = os.path.join(dataset_key, str(E0.n_pdfs))\n",
    "    loc = os.path.join(path, 'nz_comp'+str(n_gals_use)+dataset_key+str(n_floats_use)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['z_grid'] = z_grid\n",
    "        info['stacks'] = stacks\n",
    "        info['klds'] = klds\n",
    "        info['moments'] = moments\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(stacked_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_estimators(n_gals_use, dataset_key, n_floats_use, i=None):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_comp'+str(n_gals_use)+dataset_key+str(n_floats_use)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        z_grid = info['z_grid']\n",
    "        stacks = info['stacks']\n",
    "        klds = info['klds']\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(z_grid, stacks['truth'], color='black', lw=3, alpha=0.3, label='original')\n",
    "    nz_max.append(max(stacks['truth']))\n",
    "    for key in formats:\n",
    "        nz_max.append(max(stacks[key]))\n",
    "        plt.plot(z_grid, stacks[key], label=key+r' KLD='+str(klds[key])[:8], color=colors[key], linestyle=styles[key])\n",
    "    plt.xlabel(r'$z$', fontsize=14)\n",
    "    plt.ylabel(r'$\\hat{n}(z)$', fontsize=14)\n",
    "    plt.xlim(min(z_grid), max(z_grid))\n",
    "#     plt.ylim(0., max(nz_max))\n",
    "    plt.legend()\n",
    "    plt.title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ with $N_{f}='+str(n_floats_use)+r'$', fontsize=16)\n",
    "    plt.savefig(loc+'.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the data so we can remake the plots later without running everything again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "We'd like to do this for many values of $N_{f}$ as well as larger catalog subsamples, repeating the analysis many times to establish error bars on the KLD as a function of format, $N_{f}$, and dataset.  The things we want to plot across multiple datasets/number of parametes are:\n",
    "\n",
    "1. KLD of stacked estimator, i.e. `N_f` vs. `nz_output[dataset][format][instantiation][KLD_val_for_N_f]`\n",
    "2. moments of KLD of individual PDFs, i.e. `n_moment, N_f` vs. `pz_output[dataset][format][n_moment][instantiation][moment_val_for_N_f]`\n",
    "\n",
    "So, we ned to make sure these are saved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the moments of the KLD distribution for each format as $N_{f}$ changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_moments(dataset_name, n_gals_use, N_f, stat, stat_name):\n",
    "\n",
    "    path = os.path.join(dataset_name, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_name)\n",
    "    \n",
    "    if os.path.exists(loc+'.hkl'):\n",
    "        with open(loc+'.hkl', 'r') as stat_file:\n",
    "        #read in content of list/dict\n",
    "            stats = hickle.load(stat_file)\n",
    "    else:\n",
    "        stats = {}\n",
    "        stats['N_f'] = []\n",
    "        for f in stat.keys():\n",
    "            stats[f] = []\n",
    "            for m in range(n_moments_use):\n",
    "                stats[f].append([])\n",
    "\n",
    "    if N_f not in stats['N_f']:\n",
    "        stats['N_f'].append(N_f)\n",
    "        for f in stat.keys():\n",
    "            for m in range(n_moments_use):\n",
    "                stats[f][m].append([])\n",
    "        \n",
    "    where_N_f = stats['N_f'].index(N_f)\n",
    "        \n",
    "    for f in stat.keys():\n",
    "        for m in range(n_moments_use):\n",
    "            stats[f][m][where_N_f].append(stat[f][m])\n",
    "\n",
    "    with open(loc+'.hkl', 'w') as stat_file:\n",
    "        hickle.dump(stats, stat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_pz_metrics(dataset_key, n_gals_use):\n",
    "\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pz_kld_moments'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as pz_file:\n",
    "        pz_stats = hickle.load(pz_file)\n",
    "  \n",
    "    flat_floats = np.array(pz_stats['N_f']).flatten()\n",
    "    in_x = np.log(flat_floats)\n",
    "\n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)\n",
    "\n",
    "    shapes = moment_shapes\n",
    "    marksize = 10\n",
    "    a = 1./len(formats)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax.plot([-1], [0], color=colors[key], label=key, linewidth=1, linestyle=styles[key], alpha=a)\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax.scatter([-1], [0], color='k', alpha=a, marker=shapes[n], facecolors='none', s=2*marksize, label=moment_names[n])\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "#             print('pz metrics data shape '+str(pz_stats[f][n]))\n",
    "            data_arr = np.log(np.swapaxes(np.array(pz_stats[f][n]), 0, 1))#go from n_floats*instantiations to instantiations*n_floats\n",
    "            mean = np.mean(data_arr, axis=0).flatten()\n",
    "            std = np.std(data_arr, axis=0).flatten()\n",
    "            y_plus = mean + std\n",
    "            y_minus = mean - std\n",
    "#             y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "            ax_n.plot(np.exp(in_x+n_factor), mean, marker=shapes[n], mfc='none', markersize=marksize, linestyle=styles[f], alpha=a, color=colors[f])\n",
    "            ax_n.vlines(np.exp(in_x+n_factor), y_minus, y_plus, linewidth=3., alpha=a, color=colors[f])\n",
    "            pz_mean_max[n] = max(pz_mean_max[n], np.max(y_plus))\n",
    "            pz_mean_min[n] = min(pz_mean_min[n], np.min(y_minus))\n",
    "        ax_n.set_ylabel(r'$\\log[\\mathrm{'+moment_names[n]+r'}]$', rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim((pz_mean_min[n]-1., pz_mean_max[n]+1.))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\log[\\mathrm{KLD}]$ log-moments', fontsize=16)\n",
    "    ax.legend(loc='lower right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_clean.pdf', dpi=250)\n",
    "    plt.close()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax_n.plot([-1], [0], color=colors[key], label=key, linestyle=styles[key], alpha=a, linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        ax.scatter([-1], [0], color='k', alpha=a, marker=shapes[n], facecolors='none', s=2*marksize, label=moment_names[n])\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "#             print('pz metrics data shape '+str(pz_stats[f][n]))\n",
    "            data_arr = np.log(np.swapaxes(np.array(pz_stats[f][n]), 0, 1))#go from n_floats*instantiations to instantiations*n_floats\n",
    "            for i in data_arr:\n",
    "                ax_n.plot(np.exp(in_x+n_factor), i, linestyle=styles[f], marker=shapes[n], mfc='none', markersize=marksize, color=colors[f], alpha=a)\n",
    "#                 pz_moment_max[n-1].append(max(i))\n",
    "        ax_n.set_ylabel(r'$\\log[\\mathrm{'+moment_names[n]+r'}]$', rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim(pz_mean_min[n]-1., pz_mean_max[n]+1.)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\log[\\mathrm{KLD}]$ log-moments', fontsize=16)\n",
    "    ax.legend(loc='lower right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_all.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pz_delta_moments(name, size):\n",
    "    n_gals_use = size\n",
    "    \n",
    "    # should look like nz_moments\n",
    "    path = os.path.join(name, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pz_moment_deltas'+str(n_gals_use)+name)\n",
    "    with open(loc+'.hkl', 'r') as pz_file:\n",
    "        pz_stats = hickle.load(pz_file)\n",
    "    flat_floats = np.array(pz_stats['N_f']).flatten()\n",
    "    in_x = np.log(flat_floats)\n",
    "    a = 1./len(formats)\n",
    "    shapes = moment_shapes\n",
    "    marksize = 10\n",
    "    \n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)   \n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], alpha=a, linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax.scatter([-10], [0], color='k', alpha=a, marker=shapes[n], facecolors='none', s=2*marksize, label=moment_names[n])\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            old_shape = np.shape(np.array(pz_stats[f][n]))\n",
    "            new_shape = (old_shape[0], np.prod(old_shape[1:]))\n",
    "            data_arr = np.array(pz_stats[f][n]).reshape(new_shape)#go from n_floats*instantiations to instantiations*n_floats\n",
    "#             data_arr = np.median(data_arr, axis=2) * 100.\n",
    "            mean = np.mean(data_arr, axis=-1) * 100.\n",
    "            std = np.std(data_arr, axis=-1) * 100.\n",
    "            y_plus = mean + std\n",
    "            y_minus = mean - std\n",
    "#             y_cor = np.array([y_minus, y_plus, y_plus, y_minus])\n",
    "            ax_n.plot(np.exp(in_x+n_factor), mean, linestyle=styles[key], marker=shapes[n], mfc='none', markersize=marksize, alpha=a, color=colors[f])\n",
    "            ax_n.vlines(np.exp(in_x+n_factor), y_minus, y_plus, linewidth=3., alpha=a, color=colors[f])\n",
    "            n_delta_max[n] = max(n_delta_max[n], np.max(y_plus))\n",
    "            n_delta_min[n] = min(n_delta_min[n], np.min(y_minus))\n",
    "        ax_n.set_ylabel(r'percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        extremum = np.max(np.abs([n_delta_min[n],n_delta_max[n]]))+1.\n",
    "        ax_n.set_ylim(-1.*extremum, extremum)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[name]['name']+r' data $\\hat{p}(z)$ moment errors', fontsize=16)\n",
    "    ax.legend(loc='lower right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_clean.pdf', dpi=250)\n",
    "    plt.close()\n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax_n.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], alpha=a, linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        ax.scatter([-10], [0], color='k', alpha=a, marker=shapes[n], facecolors='none', s=2*marksize, label=moment_names[n])\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = np.swapaxes(np.array(pz_stats[f][n]), 0, 1)\n",
    "            data_arr = np.median(data_arr, axis=2) * 100.\n",
    "            for i in data_arr:\n",
    "                ax_n.plot(np.exp(in_x+n_factor), i, linestyle=styles[f], marker=shapes[n], mfc='none', markersize=marksize, color=colors[f], alpha=a)\n",
    "        ax_n.set_ylabel(r'median percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim(-1.*extremum, extremum)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[name]['name']+r' data $\\hat{n}(z)$ moments', fontsize=16)\n",
    "    ax.legend(loc='lower right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_all.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the KLD on $\\hat{n}(z)$ for all formats as $N_{f}$ changes.  We want to repeat this for many subsamples of the catalog to establush error bars on the KLD values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_nz_metrics(dataset_name, n_gals_use, N_f, nz_klds, stat_name):\n",
    "    \n",
    "    path = os.path.join(dataset_name, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_name)\n",
    "    if os.path.exists(loc+'.hkl'):\n",
    "        with open(loc+'.hkl', 'r') as nz_file:\n",
    "        #read in content of list/dict\n",
    "            nz_stats = hickle.load(nz_file)\n",
    "    else:\n",
    "        nz_stats = {}\n",
    "        nz_stats['N_f'] = []\n",
    "        for f in formats:\n",
    "            nz_stats[f] = []\n",
    "    \n",
    "    if N_f not in nz_stats['N_f']:\n",
    "        nz_stats['N_f'].append(N_f)\n",
    "        for f in formats:\n",
    "            nz_stats[f].append([])\n",
    "        \n",
    "    where_N_f = nz_stats['N_f'].index(N_f) \n",
    "    \n",
    "    for f in formats:\n",
    "        nz_stats[f][where_N_f].append(nz_klds[f])\n",
    "\n",
    "    with open(loc+'.hkl', 'w') as nz_file:\n",
    "        hickle.dump(nz_stats, nz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_nz_klds(dataset_key, n_gals_use):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_klds'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as nz_file:\n",
    "        nz_stats = hickle.load(nz_file)\n",
    "#     if len(instantiations) == 10:\n",
    "#         for f in formats:\n",
    "#             if not np.shape(nz_stats[f]) == (4, 10):\n",
    "#                 for s in range(len(floats)):\n",
    "#                     nz_stats[f][s] = np.array(np.array(nz_stats[f][s])[:10]).flatten()\n",
    "\n",
    "    flat_floats = np.array(nz_stats['N_f']).flatten()\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for f in formats:\n",
    "#         print('nz klds data shape '+str(nz_stats[f][n]))\n",
    "        data_arr = np.swapaxes(np.array(nz_stats[f]), 0, 1)#turn N_f * instantiations into instantiations * N_f\n",
    "        n_i = len(data_arr)\n",
    "        a = 1./len(formats)#1./n_i\n",
    "        plt.plot([10. * max(flat_floats), 10. * max(flat_floats)], [1., 10.], color=colors[f], alpha=a, label=f, linestyle=styles[f])\n",
    "        for i in data_arr:\n",
    "            plt.plot(flat_floats, i, color=colors[f], alpha=a, linestyle=styles[f])\n",
    "            kld_min.append(min(i))\n",
    "            kld_max.append(max(i))\n",
    "    plt.semilogy()\n",
    "    plt.semilogx()\n",
    "    plt.xticks(flat_floats, [str(ff) for ff in flat_floats])\n",
    "    plt.ylim(min(kld_min) / 10., 10. *  max(kld_max))\n",
    "    plt.xlim(min(flat_floats) / 3., max(flat_floats) * 3.)\n",
    "    plt.xlabel(r'number of parameters', fontsize=14)\n",
    "    plt.ylabel(r'KLD', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(r'$\\hat{n}(z)$ KLD on '+str(n_gals_use)+' from '+dataset_info[dataset_key]['name']+' mock catalog', fontsize=16)\n",
    "    plt.savefig(loc+'_all.pdf', dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    a = 1./len(formats)\n",
    "    for f in formats:\n",
    "#         print('nz klds data shape '+str(nz_stats[f][n]))\n",
    "        data_arr = np.swapaxes(np.array(nz_stats[f]), 0, 1)#turn N_f * instantiations into instantiations * N_f\n",
    "        plt.plot([10. * max(flat_floats), 10. * max(flat_floats)], [1., 10.], color=colors[f], label=f, linestyle=styles[f])\n",
    "        kld_min.append(np.min(data_arr))\n",
    "        kld_max.append(np.max(data_arr))\n",
    "        mean = np.mean(data_arr, axis=0)\n",
    "        std = np.std(data_arr, axis=0)\n",
    "        x_cor = np.array([flat_floats[:-1], flat_floats[:-1], flat_floats[1:], flat_floats[1:]])\n",
    "        y_plus = mean + std\n",
    "        y_minus = mean - std\n",
    "        y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "        plt.plot(flat_floats, mean, color=colors[f], linestyle=styles[f])\n",
    "        plt.fill(x_cor, y_cor, color=colors[f], alpha=a, linewidth=0.)\n",
    "    plt.semilogy()\n",
    "    plt.semilogx()\n",
    "    plt.xticks(flat_floats, [str(ff) for ff in flat_floats])\n",
    "    plt.ylim(min(kld_min) / 10., 10. *  max(kld_max))\n",
    "    plt.xlim(min(flat_floats), max(flat_floats))\n",
    "    plt.xlabel(r'number of parameters', fontsize=14)\n",
    "    plt.ylabel(r'KLD', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ KLD', fontsize=16)\n",
    "    plt.savefig(loc+'_clean.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_nz_moments(dataset_key, n_gals_use):\n",
    "\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_moments'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as nz_file:\n",
    "        nz_stats = hickle.load(nz_file)\n",
    "    flat_floats = np.array(nz_stats['N_f']).flatten()\n",
    "    in_x = np.log(flat_floats)\n",
    "    a = 1./len(formats)\n",
    "    shapes = moment_shapes\n",
    "    marksize = 10\n",
    "    \n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)   \n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], alpha=a, linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax.scatter([-10], [0], color='k', alpha=a, marker=shapes[n], facecolors='none', s=2*marksize, label=moment_names[n])\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        truth = np.swapaxes(np.array(nz_stats['truth'][n]), 0, 1)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = (np.swapaxes(np.array(nz_stats[f][n]), 0, 1) - truth) / truth * 100.#np.log(np.swapaxes(np.array(nz_stats[f]), 0, 1)[:][:][n])#go from n_floats*instantiations to instantiations*n_floats\n",
    "            mean = np.mean(data_arr, axis=0).flatten()\n",
    "            std = np.std(data_arr, axis=0).flatten()\n",
    "            y_plus = mean + std\n",
    "            y_minus = mean - std\n",
    "#             y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "            ax_n.plot(np.exp(in_x+n_factor), mean, linestyle=styles[key], marker=shapes[n], mfc='none', markersize=marksize, alpha=a, color=colors[f])\n",
    "            ax_n.vlines(np.exp(in_x+n_factor), y_minus, y_plus, linewidth=3., alpha=a, color=colors[f])\n",
    "            nz_mean_max[n] = max(nz_mean_max[n], np.max(y_plus))\n",
    "            nz_mean_min[n] = min(nz_mean_min[n], np.min(y_minus))\n",
    "        ax_n.set_ylabel(r'percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        extremum = np.max(np.abs([nz_mean_min[n], nz_mean_max[n]]))+1.\n",
    "        ax_n.set_ylim(-1. * extremum, extremum)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ moments', fontsize=16)\n",
    "    ax.legend(loc='lower right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_clean.pdf', dpi=250)\n",
    "    plt.close()\n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax_n.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], alpha=a, linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        ax.scatter([-10], [0], color='k', alpha=a, marker=shapes[n], facecolors='none', s=2*marksize, label=moment_names[n])\n",
    "        truth = np.swapaxes(np.array(nz_stats['truth'][n]), 0, 1)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = (np.swapaxes(np.array(nz_stats[f][n]), 0, 1) - truth) / truth * 100.\n",
    "            for i in data_arr:\n",
    "                ax_n.plot(np.exp(in_x+n_factor), i, linestyle=styles[f], marker=shapes[n], mfc='none', markersize=marksize, color=colors[f], alpha=a)\n",
    "        ax_n.set_ylabel(r'percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim(-1. * extremum, extremum)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ moments', fontsize=16)\n",
    "    ax.legend(loc='lower right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_all.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Okay, now all I have to do is have this loop over both datasets, number of galaxies, number of floats, and instantiations!\n",
    "\n",
    "Note: It takes about 5 minutes per \\# floats considered for 100 galaxies, and about 40 minutes per \\# floats for 1000 galaxies.  (So, yes, it scales more or less as expected!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_info = {}\n",
    "delta = 0.01\n",
    "\n",
    "dataset_keys = ['mg', 'ss']\n",
    "\n",
    "for name in dataset_keys:\n",
    "    dataset_info[name] = {}\n",
    "    if name == 'mg':\n",
    "        datafilename = 'bpz_euclid_test_10_3.probs'\n",
    "        z_low = 0.01\n",
    "        z_high = 3.51\n",
    "        nc_needed = 3\n",
    "        plotname = 'brighter'\n",
    "        skip_rows = 1\n",
    "        skip_cols = 1\n",
    "    elif name == 'ss':\n",
    "        datafilename = 'test_magscat_trainingfile_probs.out'\n",
    "        z_low = 0.005\n",
    "        z_high = 2.11\n",
    "        nc_needed = 5\n",
    "        plotname = 'fainter'\n",
    "        skip_rows = 1\n",
    "        skip_cols = 1\n",
    "    dataset_info[name]['filename'] = datafilename  \n",
    "    \n",
    "    dataset_info[name]['z_lim'] = (z_low, z_high)\n",
    "    z_grid = np.arange(z_low, z_high, delta, dtype='float')#np.arange(z_low, z_high + delta, delta, dtype='float')\n",
    "    z_range = z_high - z_low\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    dataset_info[name]['z_grid'] = z_grid\n",
    "    dataset_info[name]['delta_z'] = delta_z\n",
    "\n",
    "    dataset_info[name]['N_GMM'] = nc_needed# will be overwritten later\n",
    "    dataset_info[name]['name'] = plotname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formats = ['quantiles', 'histogram', 'samples']\n",
    "formats_plus = list(formats)\n",
    "formats_plus.append('truth')\n",
    "\n",
    "high_res = 300\n",
    "\n",
    "color_cycle = np.array([(230, 159, 0), (86, 180, 233), (0, 158, 115), (240, 228, 66), (0, 114, 178), (213, 94, 0), (204, 121, 167)])/256.\n",
    "color_cycle_names = ['Orange', 'Sky blue', 'Bluish green', 'Yellow', 'Blue', 'Vermilion', 'Reddish purple']\n",
    "n_plot = len(color_cycle)\n",
    "\n",
    "n_moments_use = 4\n",
    "moment_names = ['integral', 'mean', 'variance', 'kurtosis']\n",
    "moment_shapes = ['o', '*', 'P', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change all for NERSC\n",
    "\n",
    "floats = [3, 10, 30, 100]\n",
    "sizes = [100]#[10, 100, 1000]\n",
    "names = dataset_info.keys()\n",
    "instantiations = range(0, 10)\n",
    "\n",
    "all_randos = [[np.random.choice(size, n_plot, replace=False) for size in sizes] for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"pipeline\" is a bunch of nested `for` loops because `qp.Ensemble` makes heavy use of multiprocessing.  Doing multiprocessing within multiprocessing may or may not cause problems, but I am certain that it makes debugging a nightmare.\n",
    "\n",
    "Okay, without further ado, let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the \"pipeline\"\n",
    "global_start = timeit.default_timer()\n",
    "for n in range(len(names)):\n",
    "    name = names[n]\n",
    "    \n",
    "    dataset_start = timeit.default_timer()\n",
    "    print('started '+name)\n",
    "    \n",
    "    pdfs = setup_dataset(name, skip_rows, skip_cols)\n",
    "    \n",
    "    for s in range(len(sizes)):\n",
    "        size=sizes[s]\n",
    "        \n",
    "        size_start = timeit.default_timer()\n",
    "        print('started '+name+str(size))\n",
    "        \n",
    "        path = os.path.join(name, str(size))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        n_gals_use = size\n",
    "        \n",
    "        randos = all_randos[n][s]\n",
    "        \n",
    "        for i in instantiations:\n",
    "            top_bonusdict = {}\n",
    "            i_start = timeit.default_timer()\n",
    "            print('started '+name+str(size)+' #'+str(i))\n",
    "        \n",
    "            original = '_original'+str(i)\n",
    "            pdfs_use = make_instantiation(name, size, pdfs, bonus=original)\n",
    "#             plot = plot_examples(size, name, bonus=original)\n",
    "            top_bonusdict[original] = ['-', 0.25]\n",
    "        \n",
    "            z_grid = dataset_info[name]['in_z_grid']\n",
    "            N_comps = dataset_info[name]['N_GMM']\n",
    "        \n",
    "            postfit = '_postfit'+str(i)\n",
    "            catalog = setup_from_grid(name, pdfs_use, z_grid, N_comps, high_res=high_res, bonus=postfit)\n",
    "#             plot = plot_examples(size, name, bonus=postfit)\n",
    "            top_bonusdict[postfit] = ['-', 0.5]\n",
    "        \n",
    "            for n_floats_use in floats:\n",
    "#                 bonusdict = top_bonusdict.copy()\n",
    "                float_start = timeit.default_timer()\n",
    "                print('started '+name+str(size)+' #'+str(i)+' with '+str(n_floats_use))\n",
    "        \n",
    "                ensembles = analyze_individual(catalog, z_grid, n_floats_use, name, n_moments_use, i=i, bonus=postfit)\n",
    "                \n",
    "                for f in formats:\n",
    "                    fname = str(n_floats_use)+f+str(i)\n",
    "#                     plot = plot_examples(size, name, bonus=fname)\n",
    "                    bonusdict[fname] = [styles[f], 0.5]\n",
    "#                 plot = plot_all_examples(name, size, n_floats_use, i, bonus=bonusdict)\n",
    "#                 plot = plot_individual_kld(size, name, n_floats_use, i=i)\n",
    "            \n",
    "                stack_evals = analyze_stacked(catalog, ensembles, z_grid, n_floats_use, name, i=i)\n",
    "#                 plot = plot_estimators(size, name, n_floats_use, i=i)\n",
    "            \n",
    "                print('FINISHED '+name+str(size)+' #'+str(i)+' with '+str(n_floats_use)+' in '+str(timeit.default_timer() - float_start))\n",
    "            print('FINISHED '+name+str(size)+' #'+str(i)+' in '+str(timeit.default_timer() - i_start))\n",
    "#         plot = plot_pz_metrics(name, size)\n",
    "#         plot = plot_pz_delta_moments(name, size)      \n",
    "#         plot = plot_nz_klds(name, size)\n",
    "#         plot = plot_nz_moments(name, size)\n",
    "        \n",
    "        print('FINISHED '+name+str(size)+' in '+str(timeit.default_timer() - size_start))\n",
    "        \n",
    "    print('FINISHED '+name+' in '+str(timeit.default_timer() - dataset_start))\n",
    "print('FINISHED everything in '+str(timeit.default_timer() - global_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remake the plots to share axes, enabling combination of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "floats = [3, 10, 30, 100]\n",
    "sizes = [100]#[10, 100, 1000]\n",
    "names = dataset_info.keys()\n",
    "instantiations = range(0, 10)\n",
    "\n",
    "all_randos = [[np.random.choice(size, n_plot, replace=False) for size in sizes] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make this a more clever structure, i.e. a dict\n",
    "colors = {'quantiles': 'darkviolet', 'histogram': 'darkorange', 'samples': 'g'}\n",
    "styles = {'quantiles': '--', 'histogram': ':', 'samples': '-.'}\n",
    "stepstyles = {'quantiles': 'dashed', 'histogram': 'dotted', 'samples': 'dashdot'}\n",
    "\n",
    "colors_plus = colors.copy()\n",
    "colors_plus['truth'] = 'black'\n",
    "styles_plus = styles.copy()\n",
    "styles_plus['truth'] = '-'\n",
    "\n",
    "iqr_min = [3.5]\n",
    "iqr_max = [delta]\n",
    "modes_max = [0]\n",
    "pz_max = [1.]\n",
    "nz_max = [1.]\n",
    "hist_max = [1.]\n",
    "dist_min = [0.]\n",
    "dist_max = [0.]\n",
    "pz_mean_max = -10.*np.ones(n_moments_use)\n",
    "pz_mean_min = 10.*np.ones(n_moments_use)\n",
    "kld_min = [1.]\n",
    "kld_max = [1.]\n",
    "nz_mean_max = -10.*np.ones(n_moments_use)\n",
    "nz_mean_min = 10.*np.ones(n_moments_use)\n",
    "n_delta_max = -10.*np.ones(n_moments_use)\n",
    "n_delta_min = 10.*np.ones(n_moments_use)\n",
    "\n",
    "norm = False#true for shared axes on individual instantiation plots, otherwise false\n",
    "\n",
    "moments_to_save = ['pz_kld_moments', 'pz_moments', 'pz_moment_deltas', 'nz_moments']\n",
    "metrics_to_save = ['nz_klds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# comment out for NERSC\n",
    "# set norm to True and run twice to match axis limits\n",
    "\n",
    "# for name in names:\n",
    "#     for size in sizes:\n",
    "#         for stat_name in moments_to_save + metrics_to_save:\n",
    "#             clear_stats(name, size, stat_name)\n",
    "#         for i in instantiations:\n",
    "#             top_bonusdict = {}\n",
    "#             bo = '_original'+str(i)\n",
    "#             plot = plot_examples(size, name, bonus=bo, norm=norm)\n",
    "#             top_bonusdict[bo] = ['-', 0.25]\n",
    "#             bp = '_postfit'+str(i)\n",
    "#             plot = plot_examples(size, name, bonus=bp, norm=norm)\n",
    "#             top_bonusdict[bp] = ['-', 0.5]\n",
    "#             for n in range(len(floats)):\n",
    "#                 bonusdict = top_bonusdict.copy()\n",
    "#                 n_floats_use = floats[n]\n",
    "#                 for f in formats:\n",
    "#                     fname = str(n_floats_use)+f+str(i)\n",
    "#                     plot = plot_examples(size, name, bonus=fname, norm=norm)\n",
    "#                     bonusdict[fname] = [styles[f], 0.5]\n",
    "#                 plot = plot_all_examples(name, size, n_floats_use, i, bonus=bonusdict)\n",
    "#                 plot = plot_individual_kld(size, name, n_floats_use, i)\n",
    "#                 plot = plot_estimators(size, name, n_floats_use, i)\n",
    "#                 for stat_name in moments_to_save:\n",
    "#                     save_moments_wrapper(name, size, n_floats_use, i, stat_name)\n",
    "#                 for stat_name in metrics_to_save:\n",
    "#                     save_metrics_wrapper(name, size, n_floats_use, i, stat_name)\n",
    "#         plot = plot_pz_metrics(name, size)\n",
    "#         plot = plot_pz_delta_moments(name, size)\n",
    "#         plot = plot_nz_klds(name, size)\n",
    "#         plot = plot_nz_moments(name, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for name in names:\n",
    "#     for size in sizes:\n",
    "#         for i in instantiations:\n",
    "#             path = os.path.join(name, str(size))\n",
    "#             for Nf in floats:\n",
    "#                 place = os.path.join(path, 'pzs'+str(size)+name+str(Nf)+'all'+str(i))\n",
    "#                 with open(place+'.p', 'r') as filename:\n",
    "#                     klds = pickle.load(filename)\n",
    "#                     print(name, size, i, Nf, klds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# indices = '92250   3847  83378  12742  43667  10569  31701  26828  29136  11683'\n",
    "# indices += ' 43998  96531  34802  14008   5083  94955 106754  86870  23547  93601'\n",
    "# indices += '  5869   5157 100074    316  96728  75727   2662  41331  41474  93074'\n",
    "# indices += '   784 105537  39558 108553  46954  41754  47130  54528  34920  58321'\n",
    "# indices += ' 70453 108822  98370  74756  25879  80431  61434  65169  46466   6126'\n",
    "# indices += '  6466 101890 108524  96272  25660  81478  92854  24288  88348   7223'\n",
    "# indices += ' 58928  49020   2141  25304  75384  34641  65491  45164  44332 107756'\n",
    "# indices += ' 91896  75871  87481  24340   7056  80483  49792  20459  70865 109372'\n",
    "# indices += ' 34026  53985  60089   4565  38033   5947  51576   3856  24570   3438'\n",
    "# indices += ' 22431  60534  81397  16680  88137  14027  86049  21710  96081  13413'\n",
    "# indices = '9604  43445  88556  50193   1408  76204 104276  48054 104136  58073'\n",
    "# indices += ' 10084  32784 101990  59630  78907  27352  13652  56942  27011 101717'\n",
    "# indices += ' 105840  73315  41895  21820 105664  18054  94791  29329  99846  56379'\n",
    "# indices += '  13504  45749  32028  45607  56649   2589  24215   9117  97779  27706'\n",
    "# indices += '  75812  14868  59759  41794  87621  99253  83269  23886  83001  67509'\n",
    "# indices += '  37047  28435  72226  64501  57296  26271  13468  50067  26576   5017'\n",
    "# indices += '    827  22780  65501  78088  75632  28483 108573   6032  60818  26916'\n",
    "# indices += '  99955   8065  89647   4756  91047  73095  12845  10803  52331  62513'\n",
    "# indices += '   6845  26550  94541   3467  37175 101384  96101 109303 110300  53161'\n",
    "# indices += '  41110  31736  70330  11116  58618  52321  68545  87421  61994  48439'\n",
    "# # indices = [ 14619,  66891,  67914,  20931,  97633,   7202,  46756, 109704,  93110,  59915]\n",
    "# #   35851,  65657,   3292,   3838,  10862,  50447,   5316,  49503,  39363, 110951\n",
    "# #   12543,  52661,  46216,  53296,  95524,  84574 ,  2607  ,56017 , 64794,   7600\n",
    "# #   94746  59924  73186  21069   2579  34780   4623  93464  44621  29828\n",
    "# #  111140  74609  34411  42554  32981  34904  10264   1667  42037  23986\n",
    "# #   51790  98555  94971  58683  99752  87479  67286  89575  36950  84283\n",
    "# #   89866  64959  53221 102714  48642  37379  95257  11874  70743  15107\n",
    "# #   93651  48304  93829  64956  94703 107021  88900   7849  88808  71397\n",
    "# #   26862  74765  89470   2741  56888  94275  40017  85989  94077  66553\n",
    "# #   74666  90417  12553  21928  14720  53798  30290 109516  37033  95242]\n",
    "# # indices = [ 59935,  44820,  26407,  84617,  98728,  35216,  73968, 105130, 844,  63892]\n",
    "# indices = map(int, indices.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# thing = load_one_stat('ss', 100, 3, 0, 'pz_moment_deltas')\n",
    "# print(np.mean(np.shape(thing['quantiles']), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_moments('ss', 100, 3, thing, 'pz_moment_deltas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# path = os.path.join('ss', str(100))\n",
    "# loc = os.path.join(path, 'pz_moment_deltas'+str(100)+'ss')\n",
    "# with open(loc+'.hkl', 'r') as pz_file:\n",
    "#     pz_stats = hickle.load(pz_file)\n",
    "    \n",
    "# print(np.shape(pz_stats['quantiles'][0]))#N_f * n_m * n_i * n_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modified = np.array(pz_stats['quantiles']).reshape(4, 4, 1000)*100.\n",
    "# print(np.shape(modified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(np.shape(np.array(pz_stats[f][0]).reshape(4, 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# more_modified = modified * 100.\n",
    "# mean = np.mean(more_modified, axis=-1)\n",
    "# print(mean)\n",
    "# std = np.std(more_modified, axis=-1)\n",
    "# print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # print(np.shape(modified))\n",
    "# # plt.hist(modified[0][3])\n",
    "# weird_x = np.log(np.array(floats))\n",
    "\n",
    "# moment_num = 3\n",
    "# for s in range(3):\n",
    "#     f = formats[s]\n",
    "#     const = 0.1\n",
    "#     f_factor = const * (s - 1)\n",
    "#     new_data = np.array(pz_stats[f][moment_num]).reshape(4, 1000)*100.\n",
    "#     plt.plot(np.exp(weird_x+f_factor), np.median(new_data, axis=-1), linestyle=styles[f], marker=moment_shapes[moment_num], mfc='none', markersize=5, alpha=0.5, color=colors[f])\n",
    "#     violin = plt.violinplot(list(new_data), np.exp(weird_x+f_factor), showextrema=False, showmeans=False, showmedians=False, widths=np.exp(weird_x+const/2.)-np.exp(weird_x))\n",
    "# #     for partname in ['cmedians']:\n",
    "# #         vp = violin[partname]\n",
    "# #         vp.set_edgecolor(colors[f])\n",
    "# #         vp.set_linewidth(3)\n",
    "# # Make the violin body blue with a red border:\n",
    "#     for vp in violin['bodies']:\n",
    "#         vp.set_facecolor(colors[f])\n",
    "# #         vp.set_edgecolor('k')\n",
    "# #         vp.set_linewidth(0)\n",
    "#         vp.set_alpha(0.5)\n",
    "# plt.semilogx()\n",
    "# plt.ylim(-50., 50.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(np.shape(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.boxplot(list(new_data), floats, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(np.shape(pz_stats['quantiles'][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(violin.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# help(plt.boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
