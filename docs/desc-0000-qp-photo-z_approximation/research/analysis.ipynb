{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Analysis Pipeline\n",
    "\n",
    "_Alex Malz (NYU) & Phil Marshall (SLAC)_\n",
    "\n",
    "In this notebook we use the \"survey mode\" machinery to demonstrate how one should choose the optimal parametrization for photo-$z$ PDF storage given the nature of the data, the storage constraints, and the fidelity necessary for a science use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#comment out for NERSC\n",
    "%load_ext autoreload\n",
    "\n",
    "#comment out for NERSC\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "    \n",
    "import hickle\n",
    "import numpy as np\n",
    "import random\n",
    "import cProfile\n",
    "import pstats\n",
    "import StringIO\n",
    "import sys\n",
    "import os\n",
    "import timeit\n",
    "import bisect\n",
    "import re\n",
    "\n",
    "import qp\n",
    "from qp.utils import calculate_kl_divergence as make_kld\n",
    "\n",
    "# np.random.seed(seed=42)\n",
    "# random.seed(a=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['mathtext.rm'] = 'serif'\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = 'Times New Roman'\n",
    "mpl.rcParams['axes.titlesize'] = 16\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['savefig.dpi'] = 250\n",
    "mpl.rcParams['savefig.format'] = 'pdf'\n",
    "mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "#comment out for NERSC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We want to compare parametrizations for large catalogs, so we'll need to be more efficient.  The `qp.Ensemble` object is a wrapper for `qp.PDF` objects enabling conversions to be performed and metrics to be calculated in parallel.  We'll experiment on a subsample of 100 galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_dataset(dataset_key, skip_rows, skip_cols):\n",
    "    start = timeit.default_timer()\n",
    "    with open(dataset_info[dataset_key]['filename'], 'rb') as data_file:\n",
    "        lines = (line.split(None) for line in data_file)\n",
    "        for r in range(skip_rows):\n",
    "            lines.next()\n",
    "        pdfs = np.array([[float(line[k]) for k in range(skip_cols, len(line))] for line in lines])\n",
    "    print('read in data file in '+str(timeit.default_timer()-start))\n",
    "    return(pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_instantiation(dataset_key, n_gals_use, pdfs, bonus=None):\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    n_gals_tot = len(pdfs)\n",
    "    full_gal_range = range(n_gals_tot)\n",
    "    subset = np.random.choice(full_gal_range, n_gals_use, replace=False)#range(n_gals_use)\n",
    "    subset = [50904, 24239, 76385, 88784, 70208, 95397, 74433, 86406,  5199,  4373]\n",
    "    print('randos for debugging: '+str(subset))\n",
    "    pdfs_use = pdfs[subset]\n",
    "    \n",
    "    modality = []\n",
    "    dpdfs = pdfs_use[:,1:] - pdfs_use[:,:-1]\n",
    "    iqrs = []\n",
    "    for i in range(n_gals_use):\n",
    "        modality.append(len(np.where(np.diff(np.signbit(dpdfs[i])))[0]))\n",
    "        cdf = np.cumsum(qp.utils.normalize_integral((dataset_info[dataset_key]['z_grid'], pdfs_use[i]), vb=False)[1])\n",
    "        iqr_lo = dataset_info[dataset_key]['z_grid'][bisect.bisect_left(cdf, 0.25)]\n",
    "        iqr_hi = dataset_info[dataset_key]['z_grid'][bisect.bisect_left(cdf, 0.75)]\n",
    "        iqrs.append(iqr_hi - iqr_lo)\n",
    "    modality = np.array(modality)\n",
    "        \n",
    "    dataset_info[dataset_key]['N_GMM'] = int(np.median(modality))+1\n",
    "#     print('n_gmm for '+dataset_info[dataset_key]['name']+' = '+str(dataset_info[dataset_key]['N_GMM']))\n",
    "      \n",
    "    # using the same grid for output as the native format, but doesn't need to be so\n",
    "    dataset_info[dataset_key]['in_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "    dataset_info[dataset_key]['metric_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "    \n",
    "    print('preprocessed data in '+str(timeit.default_timer()-start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['randos'] = randos\n",
    "        info['z_grid'] = dataset_info[dataset_key]['in_z_grid']\n",
    "        info['pdfs'] = pdfs_use\n",
    "        info['modes'] = modality\n",
    "        info['iqrs'] = iqrs\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(pdfs_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_examples(n_gals_use, dataset_key, bonus=None):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        randos = info['randos']\n",
    "        z_grid = info['z_grid']\n",
    "        pdfs = info['pdfs']\n",
    "    \n",
    "    plt.figure()\n",
    "    for i in range(n_plot):\n",
    "        data = (z_grid, pdfs[randos[i]])\n",
    "        data = qp.utils.normalize_integral(qp.utils.normalize_gridded(data))\n",
    "        pz_max.append(np.max(data))\n",
    "        plt.plot(data[0], data[1], label=dataset_info[dataset_key]['name']+' \\#'+str(randos[i]), color=color_cycle[i])\n",
    "    plt.xlabel(r'$z$', fontsize=14)\n",
    "    plt.ylabel(r'$p(z)$', fontsize=14)\n",
    "    plt.xlim(min(z_grid), max(z_grid))\n",
    "    plt.ylim(0., max(pz_max))\n",
    "    plt.title(dataset_info[dataset_key]['name']+' data examples', fontsize=16)\n",
    "    plt.savefig(loc+'.pdf', dpi=250)\n",
    "    plt.close()\n",
    "    \n",
    "    if 'modes' in info.keys():\n",
    "        modes = info['modes']\n",
    "        modes_max.append(np.max(modes))\n",
    "        plt.figure()\n",
    "        ax = plt.hist(modes, color='k', alpha=1./n_plot, histtype='stepfilled', bins=range(max(modes_max)+1))\n",
    "        plt.xlabel('modes')\n",
    "        plt.ylabel('frequency')\n",
    "        plt.title(dataset_info[dataset_key]['name']+' data modality distribution (median='+str(dataset_info[dataset_key]['N_GMM'])+')', fontsize=16)\n",
    "        plt.savefig(loc+'modality.pdf', dpi=250)\n",
    "        plt.close()\n",
    "        \n",
    "    if 'iqrs' in info.keys():\n",
    "        iqrs = info['iqrs']\n",
    "        iqr_min.append(min(iqrs))\n",
    "        iqr_max.append(max(iqrs))\n",
    "        plot_bins = np.linspace(min(iqr_min), max(iqr_max), 20)\n",
    "        plt.figure()\n",
    "        ax = plt.hist(iqrs, bins=plot_bins, color='k', alpha=1./n_plot, histtype='stepfilled')\n",
    "        plt.xlabel('IQR')\n",
    "        plt.ylabel('frequency')\n",
    "        plt.title(dataset_info[dataset_key]['name']+' data IQR distribution', fontsize=16)\n",
    "        plt.savefig(loc+'iqrs.pdf', dpi=250)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to incrementally save the quantities that are costly to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_one_stat(dataset_name, n_gals_use, N_f, i, stat, stat_name):\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_key+str(N_f)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        hickle.dump(stat, filename)\n",
    "        \n",
    "def load_one_stat(dataset_name, n_gals_use, N_f, i, stat_name):\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_key+str(N_f)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        stat = hickle.load(filename)\n",
    "    return stat\n",
    "\n",
    "def save_moments_wrapper(dataset_name, n_gals_use, N_f, i, stat_name):\n",
    "    stat = load_one_stat(dataset_name, n_gals_use, N_f, i, stat_name)\n",
    "    save_moments(dataset_name, n_gals_use, N_f, stat, stat_name)\n",
    "        \n",
    "def save_metrics_wrapper(dataset_name, n_gals_use, N_f, i, stat_name):\n",
    "    stat = load_one_stat(dataset_name, n_gals_use, N_f, i, stat_name)\n",
    "    save_nz_metrics(datset_name, n_gals_use, N_f, stat, stat_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by reading in our catalog of gridded PDFs, sampling them, fitting GMMs to the samples, and establishing a new `qp.Ensemble` object where each meber `qp.PDF` object has `qp.PDF.truth`$\\neq$`None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_from_grid(dataset_key, in_pdfs, z_grid, N_comps, high_res=1000, bonus=None):\n",
    "    \n",
    "    #read in the data, happens to be gridded\n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    N_pdfs = len(in_pdfs)\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "#     print('making the initial ensemble of '+str(N_pdfs)+' PDFs')\n",
    "    E0 = qp.Ensemble(N_pdfs, gridded=(z_grid, in_pdfs), limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made the initial ensemble of '+str(N_pdfs)+' PDFs in '+str(timeit.default_timer() - start))    \n",
    "    \n",
    "    #fit GMMs to gridded pdfs based on samples (faster than fitting to gridded)\n",
    "    start = timeit.default_timer()\n",
    "#     print('sampling for the GMM fit')\n",
    "    samparr = E0.sample(high_res, vb=False)\n",
    "    print('took '+str(high_res)+' samples in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "#     print('making a new ensemble from samples')\n",
    "    Ei = qp.Ensemble(N_pdfs, samples=samparr, limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made a new ensemble from samples in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "#     print('fitting the GMM to samples')\n",
    "    GMMs = Ei.mix_mod_fit(comps=N_comps, vb=False)\n",
    "    print('fit the GMM to samples in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    #set the GMMS as the truth\n",
    "    start = timeit.default_timer()\n",
    "#     print('making the final ensemble')\n",
    "    Ef = qp.Ensemble(N_pdfs, truth=GMMs, limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made the final ensemble in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(N_pdfs))\n",
    "    loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['randos'] = randos\n",
    "        info['z_grid'] = z_grid\n",
    "        info['pdfs'] = Ef.evaluate(z_grid, using='truth', norm=True, vb=False)[1]\n",
    "        hickle.dump(info, filename)\n",
    "        \n",
    "    start = timeit.default_timer()\n",
    "#     print('calculating '+str(n_moments_use)+' moments of original PDFs')\n",
    "    in_moments, vals = [], []\n",
    "    for n in range(n_moments_use):\n",
    "        in_moments.append(Ef.moment(n, using='truth', limits=zlim, \n",
    "                                    dx=delta_z, vb=False))\n",
    "        vals.append(n)\n",
    "    moments = np.array(in_moments)\n",
    "    print('calculated '+str(n_moments_use)+' moments of original PDFs in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(N_pdfs))\n",
    "    loc = os.path.join(path, 'pz_moments'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['truth'] = moments\n",
    "        info['orders'] = vals\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(Ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the KLD between each approximation and the truth for every member of the ensemble.  We make the `qp.Ensemble.kld` into a `qp.PDF` object of its own to compare the moments of the KLD distributions for different parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyze_individual(E, z_grid, N_floats, dataset_key, N_moments=4, i=None, bonus=None):\n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    z_range = zlim[-1] - zlim[0]\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    \n",
    "    Eq, Eh, Es = E, E, E\n",
    "    inits = {}\n",
    "    for f in formats:\n",
    "        inits[f] = {}\n",
    "        for ff in formats:\n",
    "            inits[f][ff] = None\n",
    "            \n",
    "    qstart = timeit.default_timer()\n",
    "    inits['quantiles']['quantiles'] = Eq.quantize(N=N_floats, vb=False)\n",
    "    print('finished quantization in '+str(timeit.default_timer() - qstart))\n",
    "    hstart = timeit.default_timer()\n",
    "    inits['histogram']['histogram'] = Eh.histogramize(N=N_floats, binrange=zlim, vb=False)\n",
    "    print('finished histogramization in '+str(timeit.default_timer() - hstart))\n",
    "    sstart = timeit.default_timer()\n",
    "    inits['samples']['samples'] = Es.sample(samps=N_floats, vb=False)\n",
    "    print('finished sampling in '+str(timeit.default_timer() - sstart))\n",
    "        \n",
    "    Eo = {}\n",
    "    \n",
    "    metric_start = timeit.default_timer()\n",
    "    inloc = os.path.join(path, 'pz_moments'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(inloc+'.hkl', 'r') as infilename:\n",
    "        pz_moments = hickle.load(infilename)\n",
    "    \n",
    "    klds, metrics, kld_moments, pz_moment_deltas = {}, {}, {}, {}\n",
    "    \n",
    "    for f in formats:\n",
    "        fstart = timeit.default_timer()\n",
    "        Eo[f] = qp.Ensemble(E.n_pdfs, truth=E.truth, \n",
    "                            quantiles=inits[f]['quantiles'], \n",
    "                            histogram=inits[f]['histogram'],\n",
    "                            samples=inits[f]['samples'], \n",
    "                            limits=dataset_info[dataset_key]['z_lim'])\n",
    "        \n",
    "        fbonus = str(N_floats)+f+str(i)\n",
    "        loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+fbonus)\n",
    "        with open(loc+'.hkl', 'w') as filename:\n",
    "            info = {}\n",
    "            info['randos'] = randos\n",
    "            info['z_grid'] = z_grid\n",
    "            info['pdfs'] = Eo[f].evaluate(z_grid, using=f, norm=True, vb=False)[1]\n",
    "            hickle.dump(info, filename)\n",
    "        print('made '+f+' ensemble in '+str(timeit.default_timer()-fstart))\n",
    "\n",
    "        key = f\n",
    "        \n",
    "        fstart = timeit.default_timer()\n",
    "        klds[key] = Eo[key].kld(using=key, limits=zlim, dx=delta_z, vb=False)\n",
    "        print('calculated the '+key+' individual klds in '+str(timeit.default_timer() - fstart))\n",
    "        \n",
    "        fstart = timeit.default_timer()\n",
    "        kld_moments[key] = []\n",
    "        samp_metric = qp.PDF(samples=klds[key])\n",
    "        gmm_metric = samp_metric.mix_mod_fit(n_components=dataset_info[dataset_key]['N_GMM'], \n",
    "                                             using='samples', vb=False)\n",
    "        metrics[key] = qp.PDF(truth=gmm_metric)\n",
    "        for n in range(N_moments):\n",
    "            kld_moments[key].append(qp.utils.calculate_moment(metrics[key], n,\n",
    "                                                          using='truth', \n",
    "                                                          limits=zlim, \n",
    "                                                          dx=delta_z, \n",
    "                                                          vb=False))\n",
    "        save_one_stat(name, size, n_floats_use, i, kld_moments, 'pz_kld_moments')\n",
    "        print('calculated the '+key+' kld moments in '+str(timeit.default_timer() - fstart))\n",
    "        \n",
    "        pz_moment_deltas[key], pz_moments[key] = [], []\n",
    "        for n in range(N_moments):\n",
    "            start = timeit.default_timer()\n",
    "            new_moment = Eo[key].moment(n, using=key, limits=zlim, \n",
    "                                                  dx=delta_z, vb=False)\n",
    "            pz_moments[key].append(new_moment)\n",
    "            delta_moment = (new_moment - pz_moments['truth'][n]) / pz_moments['truth'][n]\n",
    "            pz_moment_deltas[key].append(delta_moment)\n",
    "            print('calculated the '+key+' individual moment '+str(n)+' in '+str(timeit.default_timer() - start))\n",
    "        save_one_stat(name, size, n_floats_use, i, pz_moments, 'pz_moments')\n",
    "        save_one_stat(name, size, n_floats_use, i, pz_moment_deltas, 'pz_moment_deltas')\n",
    "        \n",
    "    loc = os.path.join(path, 'kld_hist'+str(n_gals_use)+dataset_key+str(N_floats)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['z_grid'] = z_grid\n",
    "        info['N_floats'] = N_floats\n",
    "        info['pz_klds'] = klds\n",
    "        hickle.dump(info, filename)\n",
    "\n",
    "    outloc = os.path.join(path, 'pz_moments'+str(n_gals_use)+dataset_key+str(N_floats)+'_'+str(i))\n",
    "    with open(outloc+'.hkl', 'w') as outfilename:\n",
    "        hickle.dump(pz_moments, outfilename)\n",
    "    \n",
    "#     save_moments(name, size, n_floats_use, kld_moments, 'pz_kld_moments')\n",
    "#     save_moments(name, size, n_floats_use, pz_moments, 'pz_moments')\n",
    "#     save_moments(name, size, n_floats_use, pz_moment_deltas, 'pz_moment_deltas')\n",
    "    \n",
    "    return(Eo)#, klds, kld_moments, pz_moments, pz_moment_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_all_examples(name, size, N_floats, init, bonus={}):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    lines = []\n",
    "    for bonus_key in bonus.keys():\n",
    "        path = os.path.join(name, str(size))\n",
    "        loc = os.path.join(path, 'pzs'+str(size)+name+bonus_key)\n",
    "        with open(loc+'.hkl', 'r') as filename:\n",
    "            info = hickle.load(filename)\n",
    "            randos = info['randos']\n",
    "            z_grid = info['z_grid']\n",
    "            pdfs = info['pdfs']\n",
    "        ls = bonus[bonus_key][0]\n",
    "        a = bonus[bonus_key][1]\n",
    "        lab = re.sub(r'[\\_]', '', bonus_key)\n",
    "        line, = ax.plot([-1., 0.], [0., 0.], linestyle=ls, alpha=a, color='k', label=lab)\n",
    "        lines.append(line)\n",
    "        leg = ax.legend(loc='upper right', handles=lines)\n",
    "        for i in range(n_plot):\n",
    "            data = (z_grid, pdfs[randos[i]])\n",
    "            data = qp.utils.normalize_integral(qp.utils.normalize_gridded(data))\n",
    "            ax.plot(data[0], data[1], linestyle=ls, alpha=a, color=color_cycle[i])\n",
    "#     ax.legend(loc='upper right')\n",
    "    ax.set_xlabel(r'$z$', fontsize=14)\n",
    "    ax.set_ylabel(r'$p(z)$', fontsize=14)\n",
    "    ax.set_xlim(min(z_grid), max(z_grid))\n",
    "    ax.set_title(dataset_info[name]['name']+r' examples with $N_{f}=$'+str(N_floats), fontsize=16)\n",
    "    saveloc = os.path.join(path, 'pzs'+str(size)+name+str(N_floats)+'_'+str(init))\n",
    "    fig.savefig(saveloc+'.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_individual_kld(n_gals_use, dataset_key, N_floats, i):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    a = 1./len(formats)\n",
    "    loc = os.path.join(path, 'kld_hist'+str(n_gals_use)+dataset_key+str(N_floats)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        z_grid = info['z_grid']\n",
    "        N_floats = info['N_floats']\n",
    "        pz_klds = info['pz_klds']\n",
    "    \n",
    "    plt.figure()\n",
    "    plot_bins = np.linspace(-3., 3., 20)\n",
    "    for key in pz_klds.keys():\n",
    "        logdata = qp.utils.safelog(pz_klds[key])\n",
    "        kld_hist = plt.hist(logdata, color=colors[key], alpha=a, histtype='stepfilled', edgecolor='k',\n",
    "             label=key, normed=True, bins=plot_bins, linestyle=stepstyles[key], ls=stepstyles[key], lw=2)\n",
    "        hist_max.append(max(kld_hist[0]))\n",
    "        dist_min.append(min(logdata))\n",
    "        dist_max.append(max(logdata))\n",
    "    plt.legend()\n",
    "    plt.ylabel('frequency', fontsize=14)\n",
    "    plt.xlabel(r'$\\log[KLD]$', fontsize=14)\n",
    "#     plt.xlim(min(dist_min), max(dist_max))\n",
    "#     plt.ylim(0., max(hist_max))\n",
    "    plt.title(dataset_info[dataset_key]['name']+r' data $p(KLD)$ with $N_{f}='+str(N_floats)+r'$', fontsize=16)\n",
    "    plt.savefig(loc+'.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate metrics on the stacked estimator $\\hat{n}(z)$ that is the average of all members of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyze_stacked(E0, E, z_grid, n_floats_use, dataset_key, i=None):\n",
    "    \n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    z_range = zlim[-1] - zlim[0]\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    \n",
    "#     print('stacking the ensembles')\n",
    "#     stack_start = timeit.default_timer()\n",
    "    stacked_pdfs, stacks = {}, {}\n",
    "    for key in formats:\n",
    "        start = timeit.default_timer()\n",
    "        stacked_pdfs[key] = qp.PDF(gridded=E[key].stack(z_grid, using=key, \n",
    "                                                        vb=False)[key])\n",
    "        stacks[key] = stacked_pdfs[key].evaluate(z_grid, using='gridded', norm=True, vb=False)[1]\n",
    "        print('stacked '+key+ ' in '+str(timeit.default_timer()-start))\n",
    "    \n",
    "    stack_start = timeit.default_timer()\n",
    "    stacked_pdfs['truth'] = qp.PDF(gridded=E0.stack(z_grid, using='truth', \n",
    "                                                    vb=False)['truth'])\n",
    "    \n",
    "    stacks['truth'] = stacked_pdfs['truth'].evaluate(z_grid, using='gridded', norm=True, vb=False)[1]\n",
    "    print('stacked truth in '+str(timeit.default_timer() - stack_start))\n",
    "    \n",
    "    klds = {}\n",
    "    for key in formats:\n",
    "        kld_start = timeit.default_timer()\n",
    "        klds[key] = qp.utils.calculate_kl_divergence(stacked_pdfs['truth'],\n",
    "                                                     stacked_pdfs[key], \n",
    "                                                     limits=zlim, dx=delta_z)\n",
    "        print('calculated the '+key+' stacked kld in '+str(timeit.default_timer() - kld_start))\n",
    "    save_one_stat(name, size, n_floats_use, i, klds, 'nz_klds')\n",
    "#     save_nz_metrics(name, size, n_floats_use, klds, 'nz_klds')\n",
    "        \n",
    "    moments = {}\n",
    "    for key in formats_plus:\n",
    "        moment_start = timeit.default_timer()\n",
    "        moments[key] = []\n",
    "        for n in range(n_moments_use):\n",
    "            moments[key].append(qp.utils.calculate_moment(stacked_pdfs[key], n, \n",
    "                                                          limits=zlim, \n",
    "                                                          dx=delta_z, \n",
    "                                                          vb=False))\n",
    "        print('calculated the '+key+' stacked moments in '+str(timeit.default_timer() - moment_start))\n",
    "    save_one_stat(name, size, n_floats_use, i, moments, 'nz_moments')\n",
    "#     save_moments(name, size, n_floats_use, moments, 'nz_moments') \n",
    "    \n",
    "    path = os.path.join(dataset_key, str(E0.n_pdfs))\n",
    "    loc = os.path.join(path, 'nz_comp'+str(n_gals_use)+dataset_key+str(n_floats_use)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['z_grid'] = z_grid\n",
    "        info['stacks'] = stacks\n",
    "        info['klds'] = klds\n",
    "        info['moments'] = moments\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(stacked_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_estimators(n_gals_use, dataset_key, n_floats_use, i=None):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_comp'+str(n_gals_use)+dataset_key+str(n_floats_use)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        z_grid = info['z_grid']\n",
    "        stacks = info['stacks']\n",
    "        klds = info['klds']\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(z_grid, stacks['truth'], color='black', lw=3, alpha=0.3, label='original')\n",
    "    nz_max.append(max(stacks['truth']))\n",
    "    for key in formats:\n",
    "        nz_max.append(max(stacks[key]))\n",
    "        plt.plot(z_grid, stacks[key], label=key+r' KLD='+str(klds[key])[:8], color=colors[key], linestyle=styles[key])\n",
    "    plt.xlabel(r'$z$', fontsize=14)\n",
    "    plt.ylabel(r'$\\hat{n}(z)$', fontsize=14)\n",
    "    plt.xlim(min(z_grid), max(z_grid))\n",
    "#     plt.ylim(0., max(nz_max))\n",
    "    plt.legend()\n",
    "    plt.title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ with $N_{f}='+str(n_floats_use)+r'$', fontsize=16)\n",
    "    plt.savefig(loc+'.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the data so we can remake the plots later without running everything again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "We'd like to do this for many values of $N_{f}$ as well as larger catalog subsamples, repeating the analysis many times to establish error bars on the KLD as a function of format, $N_{f}$, and dataset.  The things we want to plot across multiple datasets/number of parametes are:\n",
    "\n",
    "1. KLD of stacked estimator, i.e. `N_f` vs. `nz_output[dataset][format][instantiation][KLD_val_for_N_f]`\n",
    "2. moments of KLD of individual PDFs, i.e. `n_moment, N_f` vs. `pz_output[dataset][format][n_moment][instantiation][moment_val_for_N_f]`\n",
    "\n",
    "So, we ned to make sure these are saved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the moments of the KLD distribution for each format as $N_{f}$ changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_moments(dataset_key, n_gals_use, N_f, stat, stat_name):\n",
    "\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_key)\n",
    "    \n",
    "    if os.path.exists(loc+'.hkl'):\n",
    "        with open(loc+'.hkl', 'r') as stat_file:\n",
    "        #read in content of list/dict\n",
    "            stats = hickle.load(stat_file)\n",
    "    else:\n",
    "        stats = {}\n",
    "        stats['N_f'] = []\n",
    "        for f in stat.keys():\n",
    "            stats[f] = []\n",
    "            for m in range(n_moments_use):\n",
    "                stats[f].append([])\n",
    "\n",
    "    if N_f not in stats['N_f']:\n",
    "        stats['N_f'].append(N_f)\n",
    "        for f in stat.keys():\n",
    "            for m in range(n_moments_use):\n",
    "                stats[f][m].append([])\n",
    "        \n",
    "    where_N_f = stats['N_f'].index(N_f)\n",
    "        \n",
    "    for f in stat.keys():\n",
    "        for m in range(n_moments_use):\n",
    "            stats[f][m][where_N_f].append(stat[f][m])\n",
    "\n",
    "    with open(loc+'.hkl', 'w') as stat_file:\n",
    "        hickle.dump(stats, stat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_pz_metrics(dataset_key, n_gals_use):\n",
    "\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pz_kld_moments'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as pz_file:\n",
    "        pz_stats = hickle.load(pz_file)\n",
    "  \n",
    "    flat_floats = np.array(pz_stats['N_f']).flatten()\n",
    "    in_x = np.log(flat_floats)\n",
    "\n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)\n",
    "\n",
    "    shapes = moment_shapes\n",
    "    marksize = 10\n",
    "    a = 1./len(formats)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax.plot([-1], [0], color=colors[key], label=key, linewidth=1, linestyle=styles[key])\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax.scatter([-1], [0], color='k', alpha=a, marker=shapes[n], s=2*marksize, label=moment_names[n])\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "#             print('pz metrics data shape '+str(pz_stats[f][n]))\n",
    "            data_arr = np.log(np.swapaxes(np.array(pz_stats[f][n]), 0, 1))#go from n_floats*instantiations to instantiations*n_floats\n",
    "            mean = np.mean(data_arr, axis=0).flatten()\n",
    "            std = np.std(data_arr, axis=0).flatten()\n",
    "            y_plus = mean + std\n",
    "            y_minus = mean - std\n",
    "            y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "            ax_n.plot(np.exp(in_x+n_factor), mean, marker=shapes[n], markersize=marksize, linestyle=styles[f], alpha=a, color=colors[f])\n",
    "            ax_n.vlines(np.exp(in_x+n_factor), y_minus, y_plus, linewidth=3., alpha=a, color=colors[f])\n",
    "            pz_mean_max[n] = max(pz_mean_max[n], np.max(y_plus))\n",
    "            pz_mean_min[n] = min(pz_mean_min[n], np.min(y_minus))\n",
    "        ax_n.set_ylabel(r'$\\log[\\mathrm{'+moment_names[n]+r'}]$', rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim((pz_mean_min[n]-1., pz_mean_max[n]+1.))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\log[KLD]$ log-moments', fontsize=16)\n",
    "    ax.legend(loc='lower left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_clean.pdf', dpi=250)\n",
    "    plt.close()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax_n.plot([-1], [0], color=colors[key], label=key, linestyle=styles[key], linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        ax.scatter([-1], [0], color='k', alpha=a, marker=shapes[n], s=2*marksize, label=moment_names[n])\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "#             print('pz metrics data shape '+str(pz_stats[f][n]))\n",
    "            data_arr = np.log(np.swapaxes(np.array(pz_stats[f][n]), 0, 1))#go from n_floats*instantiations to instantiations*n_floats\n",
    "            for i in data_arr:\n",
    "                ax_n.plot(np.exp(in_x+n_factor), i, linestyle=styles[f], marker=shapes[n], markersize=marksize, color=colors[f], alpha=a)\n",
    "#                 pz_moment_max[n-1].append(max(i))\n",
    "        ax_n.set_ylabel(r'$\\log[\\mathrm{'+moment_names[n]+r'}]$', rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim(pz_mean_min[n]-1., pz_mean_max[n]+1.)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\log[KLD]$ log-moments', fontsize=16)\n",
    "    ax.legend(loc='lower left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_all.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pz_delta_moments(name, size):\n",
    "    n_gals_use = size\n",
    "    \n",
    "    # should look like nz_moments\n",
    "    path = os.path.join(name, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pz_moment_deltas'+str(n_gals_use)+name)\n",
    "    with open(loc+'.hkl', 'r') as pz_file:\n",
    "        pz_stats = hickle.load(pz_file)\n",
    "    flat_floats = np.array(pz_stats['N_f']).flatten()\n",
    "    in_x = np.log(flat_floats)\n",
    "    a = 1./len(formats)\n",
    "    shapes = moment_shapes\n",
    "    marksize = 10\n",
    "    \n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)   \n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax.scatter([-10], [0], color='k', alpha=a, marker=shapes[n], s=2*marksize, label=moment_names[n])\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = np.swapaxes(np.array(pz_stats[f][n]), 0, 1)#go from n_floats*instantiations to instantiations*n_floats\n",
    "            data_arr = np.median(data_arr, axis=2) * 100.\n",
    "            mean = np.mean(data_arr, axis=0).flatten()\n",
    "            std = np.std(data_arr, axis=0).flatten()\n",
    "            y_plus = mean + std\n",
    "            y_minus = mean - std\n",
    "            y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "            ax_n.plot(np.exp(in_x+n_factor), mean, linestyle=styles[key], marker=shapes[n], markersize=marksize, alpha=a, color=colors[f])\n",
    "            ax_n.vlines(np.exp(in_x+n_factor), y_minus, y_plus, linewidth=3., alpha=a, color=colors[f])\n",
    "            n_delta_max[n] = max(n_delta_max[n], np.max(y_plus))\n",
    "            n_delta_min[n] = min(n_delta_min[n], np.min(y_minus))\n",
    "        ax_n.set_ylabel(r'median percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim((min(n_delta_min)-1., max(n_delta_max)+1.))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[name]['name']+r' data $\\hat{p}(z)$ moment errors', fontsize=16)\n",
    "    ax.legend(loc='upper right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_clean.pdf', dpi=250)\n",
    "    plt.close()\n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax_n.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        ax.scatter([-10], [0], color='k', alpha=a, marker=shapes[n], s=2*marksize, label=moment_names[n])\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = np.swapaxes(np.array(pz_stats[f][n]), 0, 1)\n",
    "            data_arr = np.median(data_arr, axis=2) * 100.\n",
    "            for i in data_arr:\n",
    "                ax_n.plot(np.exp(in_x+n_factor), i, linestyle=styles[f], marker=shapes[n], markersize=marksize, color=colors[f], alpha=a)\n",
    "        ax_n.set_ylabel(r'median percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim((min(n_delta_min)-1., min(n_delta_max)+1.))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[name]['name']+r' data $\\hat{n}(z)$ moments', fontsize=16)\n",
    "    ax.legend(loc='upper right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_all.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the KLD on $\\hat{n}(z)$ for all formats as $N_{f}$ changes.  We want to repeat this for many subsamples of the catalog to establush error bars on the KLD values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_nz_metrics(dataset_key, n_gals_use, N_f, nz_klds, stat_name):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_key)\n",
    "    if os.path.exists(loc+'.hkl'):\n",
    "        with open(loc+'.hkl', 'r') as nz_file:\n",
    "        #read in content of list/dict\n",
    "            nz_stats = hickle.load(nz_file)\n",
    "    else:\n",
    "        nz_stats = {}\n",
    "        nz_stats['N_f'] = []\n",
    "        for f in formats:\n",
    "            nz_stats[f] = []\n",
    "    \n",
    "    if N_f not in nz_stats['N_f']:\n",
    "        nz_stats['N_f'].append(N_f)\n",
    "        for f in formats:\n",
    "            nz_stats[f].append([])\n",
    "        \n",
    "    where_N_f = nz_stats['N_f'].index(N_f) \n",
    "    \n",
    "    for f in formats:\n",
    "        nz_stats[f][where_N_f].append(nz_klds[f])\n",
    "\n",
    "    with open(loc+'.hkl', 'w') as nz_file:\n",
    "        hickle.dump(nz_stats, nz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_nz_klds(dataset_key, n_gals_use):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_klds'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as nz_file:\n",
    "        nz_stats = hickle.load(nz_file)\n",
    "    if len(instantiations) == 10:\n",
    "        for f in formats:\n",
    "            if not np.shape(nz_stats[f]) == (4, 10):\n",
    "                for s in range(len(floats)):\n",
    "                    nz_stats[f][s] = np.array(np.array(nz_stats[f][s])[:10]).flatten()\n",
    "\n",
    "    flat_floats = np.array(nz_stats['N_f']).flatten()\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for f in formats:\n",
    "#         print('nz klds data shape '+str(nz_stats[f][n]))\n",
    "        data_arr = np.swapaxes(np.array(nz_stats[f]), 0, 1)#turn N_f * instantiations into instantiations * N_f\n",
    "        n_i = len(data_arr)\n",
    "        a = 1./len(formats)#1./n_i\n",
    "        plt.plot([10. * max(flat_floats), 10. * max(flat_floats)], [1., 10.], color=colors[f], alpha=a, label=f, linestyle=styles[f])\n",
    "        for i in data_arr:\n",
    "            plt.plot(flat_floats, i, color=colors[f], alpha=a, linestyle=styles[f])\n",
    "            kld_min.append(min(i))\n",
    "            kld_max.append(max(i))\n",
    "    plt.semilogy()\n",
    "    plt.semilogx()\n",
    "    plt.xticks(flat_floats, [str(ff) for ff in flat_floats])\n",
    "    plt.ylim(min(kld_min) / 10., 10. *  max(kld_max))\n",
    "    plt.xlim(min(flat_floats) / 3., max(flat_floats) * 3.)\n",
    "    plt.xlabel(r'number of parameters', fontsize=14)\n",
    "    plt.ylabel(r'KLD', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(r'$\\hat{n}(z)$ KLD on '+str(n_gals_use)+' from '+dataset_info[dataset_key]['name']+' mock catalog', fontsize=16)\n",
    "    plt.savefig(loc+'_all.pdf', dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    a = 1./len(formats)\n",
    "    for f in formats:\n",
    "#         print('nz klds data shape '+str(nz_stats[f][n]))\n",
    "        data_arr = np.swapaxes(np.array(nz_stats[f]), 0, 1)#turn N_f * instantiations into instantiations * N_f\n",
    "        plt.plot([10. * max(flat_floats), 10. * max(flat_floats)], [1., 10.], color=colors[f], label=f, linestyle=styles[f])\n",
    "        kld_min.append(np.min(data_arr))\n",
    "        kld_max.append(np.max(data_arr))\n",
    "        mean = np.mean(data_arr, axis=0)\n",
    "        std = np.std(data_arr, axis=0)\n",
    "        x_cor = np.array([flat_floats[:-1], flat_floats[:-1], flat_floats[1:], flat_floats[1:]])\n",
    "        y_plus = mean + std\n",
    "        y_minus = mean - std\n",
    "        y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "        plt.plot(flat_floats, mean, color=colors[f], linestyle=styles[f])\n",
    "        plt.fill(x_cor, y_cor, color=colors[f], alpha=a, linewidth=0.)\n",
    "    plt.semilogy()\n",
    "    plt.semilogx()\n",
    "    plt.xticks(flat_floats, [str(ff) for ff in flat_floats])\n",
    "    plt.ylim(min(kld_min) / 10., 10. *  max(kld_max))\n",
    "    plt.xlim(min(flat_floats), max(flat_floats))\n",
    "    plt.xlabel(r'number of parameters', fontsize=14)\n",
    "    plt.ylabel(r'KLD', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ KLD', fontsize=16)\n",
    "    plt.savefig(loc+'_clean.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_nz_moments(dataset_key, n_gals_use):\n",
    "\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_moments'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as nz_file:\n",
    "        nz_stats = hickle.load(nz_file)\n",
    "    flat_floats = np.array(nz_stats['N_f']).flatten()\n",
    "    in_x = np.log(flat_floats)\n",
    "    a = 1./len(formats)\n",
    "    shapes = moment_shapes\n",
    "    marksize = 10\n",
    "    \n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)   \n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax.scatter([-10], [0], color='k', alpha=a, marker=shapes[n], s=2*marksize, label=moment_names[n])\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        truth = np.swapaxes(np.array(nz_stats['truth'][n]), 0, 1)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = (np.swapaxes(np.array(nz_stats[f][n]), 0, 1) - truth) / truth * 100.#np.log(np.swapaxes(np.array(nz_stats[f]), 0, 1)[:][:][n])#go from n_floats*instantiations to instantiations*n_floats\n",
    "            mean = np.mean(data_arr, axis=0).flatten()\n",
    "            std = np.std(data_arr, axis=0).flatten()\n",
    "            y_plus = mean + std\n",
    "            y_minus = mean - std\n",
    "            y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "            ax_n.plot(np.exp(in_x+n_factor), mean, linestyle=styles[key], marker=shapes[n], markersize=marksize, alpha=a, color=colors[f])\n",
    "            ax_n.vlines(np.exp(in_x+n_factor), y_minus, y_plus, linewidth=3., alpha=a, color=colors[f])\n",
    "            nz_mean_max[n] = max(nz_mean_max[n], np.max(y_plus))\n",
    "            nz_mean_min[n] = min(nz_mean_min[n], np.min(y_minus))\n",
    "        ax_n.set_ylabel(r'percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim((min(nz_mean_min)-1., max(nz_mean_max)+1.))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ moments', fontsize=16)\n",
    "    ax.legend(loc='upper right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_clean.pdf', dpi=250)\n",
    "    plt.close()\n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax_n.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], linewidth=1)\n",
    "    for n in range(1, n_moments_use):\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        ax.scatter([-10], [0], color='k', alpha=a, marker=shapes[n], s=2*marksize, label=moment_names[n])\n",
    "        truth = np.swapaxes(np.array(nz_stats['truth'][n]), 0, 1)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = (np.swapaxes(np.array(nz_stats[f][n]), 0, 1) - truth) / truth * 100.\n",
    "            for i in data_arr:\n",
    "                ax_n.plot(np.exp(in_x+n_factor), i, linestyle=styles[f], marker=shapes[n], markersize=marksize, color=colors[f], alpha=a)\n",
    "        ax_n.set_ylabel(r'percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim((min(nz_mean_min)-1., max(nz_mean_max)+1.))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ moments', fontsize=16)\n",
    "    ax.legend(loc='upper right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_all.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Okay, now all I have to do is have this loop over both datasets, number of galaxies, number of floats, and instantiations!\n",
    "\n",
    "Note: It takes about 5 minutes per \\# floats considered for 100 galaxies, and about 40 minutes per \\# floats for 1000 galaxies.  (So, yes, it scales more or less as expected!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_info = {}\n",
    "delta = 0.01\n",
    "\n",
    "dataset_keys = ['mg', 'ss']\n",
    "\n",
    "for dataset_key in dataset_keys:\n",
    "    dataset_info[dataset_key] = {}\n",
    "    if dataset_key == 'mg':\n",
    "        datafilename = 'bpz_euclid_test_10_3.probs'\n",
    "        z_low = 0.01\n",
    "        z_high = 3.51\n",
    "        nc_needed = 3\n",
    "        plotname = 'brighter'\n",
    "        skip_rows = 1\n",
    "        skip_cols = 1\n",
    "    elif dataset_key == 'ss':\n",
    "        datafilename = 'test_magscat_trainingfile_probs.out'\n",
    "        z_low = 0.005\n",
    "        z_high = 2.11\n",
    "        nc_needed = 5\n",
    "        plotname = 'fainter'\n",
    "        skip_rows = 1\n",
    "        skip_cols = 1\n",
    "    dataset_info[dataset_key]['filename'] = datafilename  \n",
    "    \n",
    "    dataset_info[dataset_key]['z_lim'] = (z_low, z_high)\n",
    "    z_grid = np.arange(z_low, z_high, delta, dtype='float')#np.arange(z_low, z_high + delta, delta, dtype='float')\n",
    "    z_range = z_high - z_low\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    dataset_info[dataset_key]['z_grid'] = z_grid\n",
    "    dataset_info[dataset_key]['delta_z'] = delta_z\n",
    "\n",
    "    dataset_info[dataset_key]['N_GMM'] = nc_needed# will be overwritten later\n",
    "    dataset_info[dataset_key]['name'] = plotname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_res = 300\n",
    "color_cycle = np.array([(230, 159, 0), (86, 180, 233), (0, 158, 115), (240, 228, 66), (0, 114, 178), (213, 94, 0), (204, 121, 167)])/256.\n",
    "n_plot = len(color_cycle)\n",
    "n_moments_use = 4\n",
    "moment_names = ['integral', 'mean', 'variance', 'kurtosis']\n",
    "moment_shapes = ['o', '*', 'P', 'X']\n",
    "\n",
    "#make this a more clever structure, i.e. a dict\n",
    "formats = ['quantiles', 'histogram', 'samples']\n",
    "colors = {'quantiles': 'blueviolet', 'histogram': 'darkorange', 'samples': 'forestgreen'}\n",
    "styles = {'quantiles': '--', 'histogram': ':', 'samples': '-.'}\n",
    "stepstyles = {'quantiles': 'dashed', 'histogram': 'dotted', 'samples': 'dashdot'}\n",
    "\n",
    "formats_plus = ['quantiles', 'histogram', 'samples', 'truth']\n",
    "colors_plus = {'quantiles': 'blueviolet', 'histogram': 'darkorange', 'samples': 'forestgreen', 'truth':'black'}\n",
    "styles_plus = {'quantiles': '--', 'histogram': ':', 'samples': '-.', 'truth': '-'}\n",
    "\n",
    "iqr_min = [3.5]\n",
    "iqr_max = [delta]\n",
    "modes_max = [0]\n",
    "pz_max = [1.]\n",
    "nz_max = [1.]\n",
    "hist_max = [1.]\n",
    "dist_min = [0.]\n",
    "dist_max = [0.]\n",
    "pz_mean_max = -10.*np.ones(n_moments_use)\n",
    "pz_mean_min = 10.*np.ones(n_moments_use)\n",
    "kld_min = [1.]\n",
    "kld_max = [1.]\n",
    "nz_mean_max = -10.*np.ones(n_moments_use)\n",
    "nz_mean_min = 10.*np.ones(n_moments_use)\n",
    "n_delta_max = -10.*np.ones(n_moments_use)\n",
    "n_delta_min = 10.*np.ones(n_moments_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change all for NERSC\n",
    "\n",
    "floats = [3, 10, 30, 100]\n",
    "sizes = [10]#[10, 100, 1000]\n",
    "names = ['mg']#dataset_info.keys()\n",
    "instantiations = range(2, 3)#0)\n",
    "\n",
    "all_randos = [[np.random.choice(size, n_plot, replace=False) for size in sizes] for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"pipeline\" is a bunch of nested `for` loops because `qp.Ensemble` makes heavy use of multiprocessing.  Doing multiprocessing within multiprocessing may or may not cause problems, but I am certain that it makes debugging a nightmare.\n",
    "\n",
    "Okay, without further ado, let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the \"pipeline\"\n",
    "global_start = timeit.default_timer()\n",
    "for n in range(len(names)):\n",
    "    name = names[n]\n",
    "    \n",
    "    dataset_start = timeit.default_timer()\n",
    "    print('started '+name)\n",
    "    \n",
    "    pdfs = setup_dataset(name, skip_rows, skip_cols)\n",
    "    \n",
    "    for s in range(len(sizes)):\n",
    "        size=sizes[s]\n",
    "        \n",
    "        size_start = timeit.default_timer()\n",
    "        print('started '+name+str(size))\n",
    "        \n",
    "        path = os.path.join(name, str(size))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        n_gals_use = size\n",
    "        \n",
    "        randos = all_randos[n][s]\n",
    "        \n",
    "        for i in instantiations:\n",
    "#             top_bonusdict = {}\n",
    "            i_start = timeit.default_timer()\n",
    "            print('started '+name+str(size)+' #'+str(i))\n",
    "        \n",
    "            original = '_original'+str(i)\n",
    "            pdfs_use = make_instantiation(name, size, pdfs, bonus=original)\n",
    "#             plot = plot_examples(size, name, bonus=original)\n",
    "#             top_bonusdict[original] = ['-', 0.25]\n",
    "        \n",
    "            z_grid = dataset_info[name]['in_z_grid']\n",
    "            N_comps = dataset_info[name]['N_GMM']\n",
    "        \n",
    "            postfit = '_postfit'+str(i)\n",
    "            catalog = setup_from_grid(name, pdfs_use, z_grid, N_comps, high_res=high_res, bonus=postfit)\n",
    "#             plot = plot_examples(size, name, bonus=postfit)\n",
    "#             top_bonusdict[postfit] = ['-', 0.5]\n",
    "        \n",
    "            for n_floats_use in floats:\n",
    "#                 bonusdict = top_bonusdict.copy()\n",
    "                float_start = timeit.default_timer()\n",
    "                print('started '+name+str(size)+' #'+str(i)+' with '+str(n_floats_use))\n",
    "        \n",
    "                ensembles = analyze_individual(catalog, z_grid, n_floats_use, name, n_moments_use, i=i, bonus=postfit)\n",
    "                \n",
    "#                 for f in formats:\n",
    "#                     fname = str(n_floats_use)+f+str(i)\n",
    "#                     plot = plot_examples(size, name, bonus=fname)\n",
    "#                     bonusdict[fname] = [styles[f], 0.5]\n",
    "#                 plot = plot_all_examples(name, size, n_floats_use, i, bonus=bonusdict)\n",
    "#                 plot = plot_individual_kld(size, name, n_floats_use, i=i)\n",
    "            \n",
    "                stack_evals = analyze_stacked(catalog, ensembles, z_grid, n_floats_use, name, i=i)\n",
    "#                 plot = plot_estimators(size, name, n_floats_use, i=i)\n",
    "            \n",
    "                print('FINISHED '+name+str(size)+' #'+str(i)+' with '+str(n_floats_use)+' in '+str(timeit.default_timer() - float_start))\n",
    "            print('FINISHED '+name+str(size)+' #'+str(i)+' in '+str(timeit.default_timer() - i_start))\n",
    "#         plot = plot_pz_metrics(name, size)\n",
    "#         plot = plot_pz_delta_moments(name, size)      \n",
    "#         plot = plot_nz_klds(name, size)\n",
    "#         plot = plot_nz_moments(name, size)\n",
    "        \n",
    "        print('FINISHED '+name+str(size)+' in '+str(timeit.default_timer() - size_start))\n",
    "        \n",
    "    print('FINISHED '+name+' in '+str(timeit.default_timer() - dataset_start))\n",
    "print('FINISHED everything in '+str(timeit.default_timer() - global_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remake the plots to share axes, enabling combination of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# floats = [3, 10, 30, 100]\n",
    "# sizes = [10]#[10, 100, 1000]\n",
    "# names = ['mg']#dataset_info.keys()\n",
    "# instantiations = range(0, 1)#0)\n",
    "\n",
    "# all_randos = [[np.random.choice(size, n_plot, replace=False) for size in sizes] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# comment out for NERSC\n",
    "# run twice to match axis limits\n",
    "\n",
    "for name in names:\n",
    "    for size in sizes:\n",
    "        for i in instantiations:\n",
    "            top_bonusdict = {}\n",
    "            bo = '_original'+str(i)\n",
    "#             plot = plot_examples(size, name, bonus=bo)\n",
    "            top_bonusdict[bo] = ['-', 0.25]\n",
    "            bp = '_postfit'+str(i)\n",
    "#             plot = plot_examples(size, name, bonus=bp)\n",
    "            top_bonusdict[bp] = ['-', 0.5]\n",
    "            for n in range(len(floats)):\n",
    "                bonusdict = top_bonusdict.copy()\n",
    "                n_floats_use = floats[n]\n",
    "                for f in formats:\n",
    "                    fname = str(n_floats_use)+f+str(i)\n",
    "#                     plot = plot_examples(size, name, bonus=fname)\n",
    "                    bonusdict[fname] = [styles[f], 0.5]\n",
    "                plot = plot_all_examples(name, size, n_floats_use, i, bonus=bonusdict)\n",
    "                plot = plot_individual_kld(size, name, n_floats_use, i)\n",
    "                save_moments_wrapper(name, size, n_floats_use, i, 'pz_kld_moments')\n",
    "                save_moments_wrapper(name, size, n_floats_use, i, 'pz_moments')\n",
    "                save_moments_wrapper(name, size, n_floats_use, i, 'pz_moment_deltas')\n",
    "                plot = plot_estimators(size, name, n_floats_use, i)\n",
    "                save_metrics_wrapper(name, size, n_floats_use, i, 'nz_klds')\n",
    "                save_moments_wrapper(name, size, n_floats_use, i, 'nz_moments')\n",
    "        plot = plot_pz_metrics(name, size)\n",
    "        plot = plot_pz_delta_moments(name, size)\n",
    "        plot = plot_nz_klds(name, size)\n",
    "        plot = plot_nz_moments(name, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# indices = [ 59935,  44820,  26407,  84617,  98728,  35216,  73968, 105130, 844,  63892]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
