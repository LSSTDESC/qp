{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Analysis Pipeline\n",
    "\n",
    "_Alex Malz (NYU) & Phil Marshall (SLAC)_\n",
    "\n",
    "In this notebook we use the \"survey mode\" machinery to demonstrate how one should choose the optimal parametrization for photo-$z$ PDF storage given the nature of the data, the storage constraints, and the fidelity necessary for a science use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "    \n",
    "import hickle\n",
    "import numpy as np\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "import random\n",
    "import cProfile\n",
    "import pstats\n",
    "import StringIO\n",
    "import timeit\n",
    "import psutil\n",
    "import sys\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import qp\n",
    "from qp.utils import calculate_kl_divergence as make_kld\n",
    "\n",
    "# np.random.seed(seed=42)\n",
    "# random.seed(a=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_info = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two datasets available:\n",
    "\n",
    "* $10^{5}$ LSST-like mock data provided by Sam Schmidt (UC Davis, LSST)\n",
    "* $10^{4}$ Euclid-like mock data provided by Melissa Graham (UW, LSST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose one of these:\n",
    "# dataset_key = 'Euclid'# Melissa Graham's data\n",
    "# dataset_key = 'LSST'# Sam Schmidt's data\n",
    "dataset_keys = ['Optical+IR', 'Optical']\n",
    "\n",
    "for dataset_key in dataset_keys:\n",
    "    dataset_info[dataset_key] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both datasets are fit with BPZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset_key in dataset_keys:\n",
    "    if dataset_key == 'Optical+IR':\n",
    "        datafilename = 'bpz_euclid_test_10_2.probs'\n",
    "    elif dataset_key == 'Optical':\n",
    "        datafilename = 'test_magscat_trainingfile_probs.out'\n",
    "    dataset_info[dataset_key]['filename'] = datafilename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data files don't appear to come with information about the native format or metaparameters, but we are told they're evaluations on a regular grid of redshifts with given endpoints and number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delta = 0.01\n",
    "\n",
    "for dataset_key in dataset_keys:\n",
    "    \n",
    "    if dataset_key == 'Optical+IR':\n",
    "        z_low = 0.01\n",
    "        z_high = 3.51\n",
    "    elif dataset_key == 'Optical':\n",
    "        z_low = 0.005\n",
    "        z_high = 2.11\n",
    "    \n",
    "    dataset_info[dataset_key]['z_lim'] = (z_low, z_high)\n",
    "\n",
    "    z_grid = np.arange(z_low, z_high, delta, dtype='float')#np.arange(z_low, z_high + delta, delta, dtype='float')\n",
    "    z_range = z_high - z_low\n",
    "    delta_z = z_range / len(z_grid)\n",
    "\n",
    "    dataset_info[dataset_key]['z_grid'] = z_grid\n",
    "    dataset_info[dataset_key]['delta_z'] = delta_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`qp` cannot currently convert gridded PDFs to histograms or quantiles - we need to make a GMM first, and use this to instantiate a `qp.PDF` object using a `qp.composite` object based on that GMM as `qp.PDF.truth`.  The number of parameters necessary for a qualitatively good fit depends on the characteristics of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dataset_key in dataset_keys:\n",
    "    \n",
    "    if dataset_key == 'Optical+IR':\n",
    "        nc_needed = 3\n",
    "    elif dataset_key == 'Optical':\n",
    "        nc_needed = 5\n",
    "    \n",
    "    dataset_info[dataset_key]['N_GMM'] = nc_needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some useful quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#many_colors = ['red','green','blue','cyan','magenta','yellow']\n",
    "high_res = 300\n",
    "n_plot = 5\n",
    "n_moments_use = 4\n",
    "\n",
    "#make this a more clever structure, i.e. a dict\n",
    "formats = ['quantiles', 'histogram', 'samples']\n",
    "colors = {'quantiles': 'blueviolet', 'histogram': 'darkorange', 'samples': 'forestgreen'}\n",
    "styles = {'quantiles': '--', 'histogram': ':', 'samples': '-.'}\n",
    "stepstyles = {'quantiles': 'dashed', 'histogram': 'dotted', 'samples': 'dashdot'}\n",
    "\n",
    "pz_max = [1.]\n",
    "nz_max = [1.]\n",
    "hist_max = [1.]\n",
    "dist_min = [0.]\n",
    "dist_max = [0.]\n",
    "moment_max = [[]] * (n_moments_use - 1)\n",
    "mean_max = [[]] * (n_moments_use - 1)\n",
    "kld_min = [1.]\n",
    "kld_max = [1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We want to compare parametrizations for large catalogs, so we'll need to be more efficient.  The `qp.Ensemble` object is a wrapper for `qp.PDF` objects enabling conversions to be performed and metrics to be calculated in parallel.  We'll experiment on a subsample of 100 galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_dataset(dataset_key):#, n_gals_use):\n",
    "    \n",
    "    with open(dataset_info[dataset_key]['filename'], 'rb') as data_file:\n",
    "        lines = (line.split(None) for line in data_file)\n",
    "        lines.next()\n",
    "        pdfs = np.array([[float(line[k]) for k in range(1,len(line))] for line in lines])\n",
    "    \n",
    "    # sys.getsizeof(pdfs)\n",
    "\n",
    "#     n_gals_tot = len(pdfs)\n",
    "#     full_gal_range = range(n_gals_tot)\n",
    "#     subset = np.random.choice(full_gal_range, n_gals_use, replace=False)#range(n_gals_use)\n",
    "#     pdfs_use = pdfs[subset]\n",
    "\n",
    "#     # using the same grid for output as the native format, but doesn't need to be so\n",
    "#     dataset_info[dataset_key]['in_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "#     dataset_info[dataset_key]['metric_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "    \n",
    "#     bonus = '_original'\n",
    "#     path = os.path.join(dataset_key, str(n_gals_use))\n",
    "#     loc = os.path.join(path, str(n_gals_use)+'from'+dataset_key+'_pzs'+bonus)\n",
    "#     with open(loc+'.hkl', 'w') as filename:\n",
    "#         info = {}\n",
    "#         info['z_grid'] = dataset_info[dataset_key]['in_z_grid']\n",
    "#         info['pdfs'] = pdfs_use\n",
    "#         hickle.dump(info, filename)\n",
    "    \n",
    "#     return(pdfs_use, bonus)\n",
    "    return(pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_instantiation(dataset_key, n_gals_use, pdfs, bonus=None):\n",
    "    \n",
    "    n_gals_tot = len(pdfs)\n",
    "    full_gal_range = range(n_gals_tot)\n",
    "    subset = np.random.choice(full_gal_range, n_gals_use, replace=False)#range(n_gals_use)\n",
    "    pdfs_use = pdfs[subset]\n",
    "\n",
    "    # using the same grid for output as the native format, but doesn't need to be so\n",
    "    dataset_info[dataset_key]['in_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "    dataset_info[dataset_key]['metric_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pzs'+bonus+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['randos'] = randos\n",
    "        info['z_grid'] = dataset_info[dataset_key]['in_z_grid']\n",
    "        info['pdfs'] = pdfs_use\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(pdfs_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_examples(n_gals_use, dataset_key, bonus=None):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pzs'+bonus+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        randos = info['randos']\n",
    "        z_grid = info['z_grid']\n",
    "        pdfs = info['pdfs']\n",
    "    \n",
    "    plt.figure(1)\n",
    "    for i in range(n_plot):\n",
    "        data = (z_grid, pdfs[randos[i]])\n",
    "        data = qp.utils.normalize_integral(qp.utils.normalize_gridded(data))\n",
    "        pz_max.append(np.max(data))\n",
    "        plt.plot(data[0], data[1], label=dataset_key+'#'+str(randos[i]))\n",
    "    plt.xlabel(r'$z$', fontsize=14)\n",
    "    plt.ylabel(r'$p(z)$', fontsize=14)\n",
    "    plt.xlim(min(z_grid), max(z_grid))\n",
    "    plt.ylim(0., max(pz_max))\n",
    "    plt.title(bonus[1:]+' '+dataset_key+' mock catalog of '+str(n_gals_use), fontsize=16)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(loc+'.png', dpi=250)\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(2)\n",
    "    for i in range(n_plot):\n",
    "        data = (z_grid, pdfs[randos[i]])\n",
    "        data = qp.utils.normalize_integral(qp.utils.normalize_gridded(data))\n",
    "        plt.plot(data[0], data[1], label=dataset_key+'#'+str(randos[i]))\n",
    "    plt.xlabel(r'$z$', fontsize=14)\n",
    "    plt.ylabel(r'$\\log[p(z)]$', fontsize=14)\n",
    "    plt.semilogy()\n",
    "    plt.xlim(min(z_grid), max(z_grid))\n",
    "    plt.ylim(qp.utils.epsilon, max(pz_max))\n",
    "    plt.title(bonus[1:]+' '+dataset_key+' mock catalog of '+str(n_gals_use), fontsize=16)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(loc+'_log.png', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by reading in our catalog of gridded PDFs, sampling them, fitting GMMs to the samples, and establishing a new `qp.Ensemble` object where each meber `qp.PDF` object has `qp.PDF.truth`$\\neq$`None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_from_grid(dataset_key, in_pdfs, z_grid, N_comps, high_res=1000, bonus=None):\n",
    "    \n",
    "    #read in the data, happens to be gridded\n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    N_pdfs = len(in_pdfs)\n",
    "    \n",
    "#     plot_examples(N_pdfs, z_grid, pdfs)\n",
    "    \n",
    "    print('making the initial ensemble of '+str(N_pdfs)+' PDFs')\n",
    "    E0 = qp.Ensemble(N_pdfs, gridded=(z_grid, in_pdfs), limits=dataset_info[dataset_key]['z_lim'], vb=True)\n",
    "    print('made the initial ensemble of '+str(N_pdfs)+' PDFs')\n",
    "    \n",
    "    #fit GMMs to gridded pdfs based on samples (faster than fitting to gridded)\n",
    "    print('sampling for the GMM fit')\n",
    "    samparr = E0.sample(high_res, vb=False)\n",
    "    print('took '+str(high_res)+' samples')\n",
    "    \n",
    "    print('making a new ensemble from samples')\n",
    "    Ei = qp.Ensemble(N_pdfs, samples=samparr, limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made a new ensemble from samples')\n",
    "    \n",
    "    print('fitting the GMM to samples')\n",
    "    GMMs = Ei.mix_mod_fit(comps=N_comps, vb=False)\n",
    "    print('fit the GMM to samples')\n",
    "    \n",
    "    #set the GMMS as the truth\n",
    "    print('making the final ensemble')\n",
    "    Ef = qp.Ensemble(N_pdfs, truth=GMMs, limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made the final ensemble')\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(N_pdfs))\n",
    "    loc = os.path.join(path, 'pzs'+bonus+str(N_pdfs)+dataset_key)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['randos'] = randos\n",
    "        info['z_grid'] = z_grid\n",
    "        info['pdfs'] = Ef.evaluate(z_grid, using='truth', norm=True, vb=False)[1]\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(Ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the KLD between each approximation and the truth for every member of the ensemble.  We make the `qp.Ensemble.kld` into a `qp.PDF` object of its own to compare the moments of the KLD distributions for different parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_individual(E, z_grid, N_floats, dataset_key, N_moments=4, i=None):\n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    z_range = zlim[-1] - zlim[0]\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    \n",
    "    Eq, Eh, Es = E, E, E\n",
    "    inits = {}\n",
    "    for f in formats:\n",
    "        inits[f] = {}\n",
    "        for ff in formats:\n",
    "            inits[f][ff] = None\n",
    "            \n",
    "    qstart = timeit.default_timer()\n",
    "    print('performing quantization')\n",
    "    inits['quantiles']['quantiles'] = Eq.quantize(N=N_floats, vb=True)\n",
    "    print('finished quantization at '+str(timeit.default_timer() - qstart))\n",
    "    hstart = timeit.default_timer()\n",
    "    print('performing histogramization')\n",
    "    inits['histogram']['histogram'] = Eh.histogramize(N=N_floats, binrange=zlim, vb=False)\n",
    "    print('finished histogramization at '+str(timeit.default_timer() - hstart))\n",
    "    sstart = timeit.default_timer()\n",
    "    print('performing sampling')\n",
    "    inits['samples']['samples'] = Es.sample(samps=N_floats, vb=False)\n",
    "    print('finished sampling at '+str(timeit.default_timer() - sstart))\n",
    "        \n",
    "    print('making the approximate ensembles')\n",
    "    Eo = {}\n",
    "    for f in formats:\n",
    "        Eo[f] = qp.Ensemble(E.n_pdfs, truth=E.truth, \n",
    "                            quantiles=inits[f]['quantiles'], \n",
    "                            histogram=inits[f]['histogram'],\n",
    "                            samples=inits[f]['samples'], \n",
    "                            limits=dataset_info[dataset_key]['z_lim'])\n",
    "        bonus = '_'+str(n_floats_use)+f+'_('+str(i)+')'\n",
    "        path = os.path.join(dataset_key, str(n_gals_use))\n",
    "        loc = os.path.join(path, 'pzs'+bonus+str(n_gals_use)+dataset_key)\n",
    "        with open(loc+'.hkl', 'w') as filename:\n",
    "            info = {}\n",
    "            info['randos'] = randos\n",
    "            info['z_grid'] = z_grid\n",
    "            info['pdfs'] = Eo[f].evaluate(z_grid, using=f, norm=True, vb=False)[1]\n",
    "            hickle.dump(info, filename)\n",
    "    print('made the approximate ensembles')\n",
    "    \n",
    "    print('calculating the individual metrics')\n",
    "    metric_start = timeit.default_timer()\n",
    "    klds, metrics, moments = {}, {}, {}\n",
    "    \n",
    "    for key in Eo.keys():\n",
    "        print('starting '+key)\n",
    "        klds[key] = Eo[key].kld(using=key, limits=zlim, dx=delta_z)\n",
    "        samp_metric = qp.PDF(samples=klds[key])\n",
    "        gmm_metric = samp_metric.mix_mod_fit(n_components=dataset_info[dataset_key]['N_GMM'], \n",
    "                                             using='samples', vb=False)\n",
    "        metrics[key] = qp.PDF(truth=gmm_metric)\n",
    "        moments[key] = []\n",
    "        for n in range(N_moments+1):\n",
    "            moments[key].append([qp.utils.calculate_moment(metrics[key], n,\n",
    "                                                          using='truth', \n",
    "                                                          limits=zlim, \n",
    "                                                          dx=delta_z, \n",
    "                                                          vb=False)])\n",
    "        print('finished with '+key)\n",
    "    print('calculated the individual metrics in '+str(timeit.default_timer() - metric_start))\n",
    "\n",
    "    path = os.path.join(dataset_key, str(E.n_pdfs))\n",
    "    loc = os.path.join(path, str(N_floats)+'kld_hist'+str(n_gals_use)+dataset_key+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['z_grid'] = z_grid\n",
    "        info['N_floats'] = N_floats\n",
    "        info['pz_klds'] = klds\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(Eo, klds, moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_individual(n_gals_use, dataset_key, N_floats, i):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, str(N_floats)+'kld_hist'+str(n_gals_use)+dataset_key+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        z_grid = info['z_grid']\n",
    "        N_floats = info['N_floats']\n",
    "        pz_klds = info['pz_klds']\n",
    "    \n",
    "    plt.figure()\n",
    "    plot_bins = np.linspace(-3., 3., 20)\n",
    "    a = 1./len(formats)\n",
    "    for key in pz_klds.keys():\n",
    "        logdata = qp.utils.safelog(pz_klds[key])\n",
    "        kld_hist = plt.hist(logdata, color=colors[key], alpha=a, histtype='stepfilled', edgecolor='k',\n",
    "             label=key, normed=True, bins=plot_bins, linestyle=stepstyles[key], ls=stepstyles[key], lw=3)\n",
    "        hist_max.append(max(kld_hist[0]))\n",
    "        dist_min.append(min(logdata))\n",
    "        dist_max.append(max(logdata))\n",
    "    plt.legend()\n",
    "    plt.ylabel('frequency', fontsize=14)\n",
    "    plt.xlabel(r'$\\log[KLD]$', fontsize=14)\n",
    "    plt.xlim(min(dist_min), max(dist_max))\n",
    "    plt.ylim(0., max(hist_max))\n",
    "    plt.title('KLD distribution of '+str(n_gals_use)+' from '+dataset_key+r' with $N_{f}='+str(N_floats)+r'$', fontsize=16)\n",
    "    plt.savefig(loc+'.png', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate metrics on the stacked estimator $\\hat{n}(z)$ that is the average of all members of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyze_stacked(E0, E, z_grid, n_floats_use, dataset_key, i=None):\n",
    "    \n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    z_range = zlim[-1] - zlim[0]\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    \n",
    "    print('stacking the ensembles')\n",
    "    stack_start = timeit.default_timer()\n",
    "    stacked_pdfs, stacks = {}, {}\n",
    "    for key in formats:\n",
    "        stacked_pdfs[key] = qp.PDF(gridded=E[key].stack(z_grid, using=key, \n",
    "                                                        vb=False)[key])\n",
    "        stacks[key] = stacked_pdfs[key].evaluate(z_grid, using='gridded', norm=True, vb=False)[1]\n",
    "    \n",
    "    stacked_pdfs['truth'] = qp.PDF(gridded=E0.stack(z_grid, using='truth', \n",
    "                                                    vb=False)['truth'])\n",
    "    \n",
    "    stacks['truth'] = stacked_pdfs['truth'].evaluate(z_grid, using='gridded', norm=True, vb=False)[1]\n",
    "    print('stacked the ensembles in '+str(timeit.default_timer() - stack_start))\n",
    "    \n",
    "    print('calculating the metrics')\n",
    "    metric_start = timeit.default_timer()\n",
    "    klds = {}\n",
    "    for key in formats:\n",
    "        klds[key] = qp.utils.calculate_kl_divergence(stacked_pdfs['truth'],\n",
    "                                                     stacked_pdfs[key], \n",
    "                                                     limits=zlim, dx=delta_z)\n",
    "    print('calculated the metrics in '+str(timeit.default_timer() - metric_start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(E0.n_pdfs))\n",
    "    loc = os.path.join(path, str(n_floats_use)+'nz_comp'+str(n_gals_use)+dataset_key+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['z_grid'] = z_grid\n",
    "        info['stacks'] = stacks\n",
    "        info['klds'] = klds\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(stacked_pdfs, klds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_estimators(n_gals_use, dataset_key, n_floats_use, i=None):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, str(n_floats_use)+'nz_comp'+str(n_gals_use)+dataset_key+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        z_grid = info['z_grid']\n",
    "        stacks = info['stacks']\n",
    "        klds = info['klds']\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(z_grid, stacks['truth'], color='black', lw=4, alpha=0.3, label='truth')\n",
    "    nz_max.append(max(stacks['truth']))\n",
    "    for key in formats:\n",
    "        nz_max.append(max(stacks[key]))\n",
    "        plt.plot(z_grid, stacks[key], label=key+r' KLD='+str(klds[key]), color=colors[key], linestyle=styles[key])\n",
    "    plt.xlabel(r'$z$', fontsize=14)\n",
    "    plt.ylabel(r'$\\hat{n}(z)$', fontsize=14)\n",
    "    plt.xlim(min(z_grid), max(z_grid))\n",
    "    plt.ylim(0., max(nz_max))\n",
    "    plt.legend()\n",
    "    plt.title(r'$\\hat{n}(z)$ for '+str(n_gals_use)+r' from '+dataset_key+r' with $N_{f}='+str(n_floats_use)+r'$', fontsize=16)\n",
    "    plt.savefig(loc+'.png', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the data so we can remake the plots later without running everything again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "We'd like to do this for many values of $N_{f}$ as well as larger catalog subsamples, repeating the analysis many times to establish error bars on the KLD as a function of format, $N_{f}$, and dataset.  The things we want to plot across multiple datasets/number of parametes are:\n",
    "\n",
    "1. KLD of stacked estimator, i.e. `N_f` vs. `nz_output[dataset][format][instantiation][KLD_val_for_N_f]`\n",
    "2. moments of KLD of individual PDFs, i.e. `n_moment, N_f` vs. `pz_output[dataset][format][n_moment][instantiation][moment_val_for_N_f]`\n",
    "\n",
    "So, we ned to make sure these are saved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the moments of the KLD distribution for each format as $N_{f}$ changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_pz_metrics(dataset_key, n_gals_use, N_f, metric_moments):\n",
    "\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pz_klds'+str(n_gals_use)+dataset_key)\n",
    "    \n",
    "    if os.path.exists(loc+'.hkl'):\n",
    "        with open(loc+'.hkl', 'r') as pz_file:\n",
    "        #read in content of list/dict\n",
    "            pz_stats = hickle.load(pz_file)\n",
    "    else:\n",
    "        pz_stats = {}\n",
    "        pz_stats['N_f'] = []\n",
    "        for f in formats:#change this name to formats\n",
    "            pz_stats[f] = []\n",
    "            for m in range(n_moments_use + 1):\n",
    "                pz_stats[f].append([])\n",
    "\n",
    "    if N_f not in pz_stats['N_f']:\n",
    "        pz_stats['N_f'].append(N_f)\n",
    "        for f in formats:\n",
    "            for m in range(n_moments_use + 1):\n",
    "                pz_stats[f][m].append([])\n",
    "        \n",
    "    where_N_f = pz_stats['N_f'].index(N_f)\n",
    "        \n",
    "    for f in formats:\n",
    "        for m in range(n_moments_use + 1):\n",
    "            pz_stats[f][m][where_N_f].append(metric_moments[f][m])\n",
    "\n",
    "    with open(loc+'.hkl', 'w') as pz_file:\n",
    "        hickle.dump(pz_stats, pz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_pz_metrics(dataset_key, n_gals_use):\n",
    "# trying really hard to make this colorblind-readable but still failing\n",
    "\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pz_klds'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as pz_file:\n",
    "        pz_stats = hickle.load(pz_file)\n",
    "        \n",
    "    flat_floats = np.array(pz_stats['N_f']).flatten()\n",
    "\n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)\n",
    "\n",
    "    shapes = ['*','+','x']#,'v','^','<','>']\n",
    "    marksize = 50\n",
    "    a = 1./len(formats)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax_n.plot([-1], [0], color=colors[key], label=key, linestyle=styles[key], linewidth=1)\n",
    "\n",
    "    for n in range(1, 4):\n",
    "        ax.scatter([-1], [0], color='k', marker=shapes[n-1], s=marksize, label='moment '+str(n))\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for f in formats:\n",
    "            data_arr = np.swapaxes(np.array(pz_stats[f][n]), 0, 1)#go from n_floats*instantiations to instantiations*n_floats\n",
    "            for i in data_arr:#next try plot with marker and linewidth/linestyle keywords\n",
    "                ax_n.scatter(flat_floats, i, marker=shapes[n-1], s=marksize, color=colors[f], alpha=a)#, \n",
    "                             # linewidth=1, linestyle=styles[f], edgecolor='k')\n",
    "#                 ax_n.scatter(flat_floats, i, marker=shapes[n-1], s=marksize, color='None',\n",
    "#                              linewidth=2, linestyle=styles[f], edgecolor='k', alpha=1.)\n",
    "                moment_max[n-1].append(max(i))\n",
    "        ax_n.set_ylabel('moment '+str(n), fontsize=14)\n",
    "        ax_n.set_ylim(0., max(moment_max[n-1]))\n",
    "    ax.set_xlim(min(flat_floats) - 10**int(np.log10(min(flat_floats))), max(flat_floats) + 10**int(np.log10(max(flat_floats))))\n",
    "    ax.semilogx()\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title('KLD moments on '+str(n_gals_use)+' from '+dataset_key, fontsize=16)\n",
    "    ax.legend(loc='upper left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'.png', dpi=250)\n",
    "    plt.close()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "#     jitters = {}\n",
    "#     factors = {'quantiles':-0.1, 'histogram':0., 'samples':0.1}\n",
    "    for key in formats:\n",
    "        ax_n.plot([-1], [0], color=colors[key], label=key, linewidth=1)\n",
    "#         jitters[key] = factors[key] * np.sqrt(flat_floats)\n",
    "    for n in range(1, 4):\n",
    "        ax.scatter([-1], [0], color='k', marker=shapes[n-1], s=marksize, label='moment '+str(n))\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for f in formats:\n",
    "            data_arr = np.swapaxes(np.array(pz_stats[f][n]), 0, 1)#go from n_floats*instantiations to instantiations*n_floats\n",
    "            mean = np.mean(data_arr, axis=0).flatten()\n",
    "            std = np.std(data_arr, axis=0).flatten()\n",
    "#             x_cor = np.array([flat_floats[:-1], flat_floats[:-1], flat_floats[1:], flat_floats[1:]])\n",
    "#             y_plus = mean + std\n",
    "#             y_minus = mean - std\n",
    "#             y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "            ax_n.scatter(flat_floats, mean, marker=shapes[n-1], s=marksize, alpha=1., color=colors[f])\n",
    "            ax_n.errorbar(flat_floats, mean, yerr=std, color=colors[f], alpha=2*a, capsize=5, elinewidth=1, linewidth=0., visible=True)\n",
    "#             ax_n.fill(x_cor, y_cor, color=colors[f], alpha=a, linewidth=0.)\n",
    "            mean_max[n-1].append(np.max(mean+std))\n",
    "        ax_n.set_ylabel('moment '+str(n), fontsize=14)\n",
    "        ax_n.set_ylim(0., np.max(np.array(mean_max[n-1])))\n",
    "    ax.set_xlim(min(flat_floats)/3., max(flat_floats)*3.)\n",
    "    ax.semilogx()\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title('KLD moments on '+str(n_gals_use)+' from '+dataset_key, fontsize=16)\n",
    "    ax.legend(loc='upper left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_clean.png', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the KLD on $\\hat{n}(z)$ for all formats as $N_{f}$ changes.  We want to repeat this for many subsamples of the catalog to establush error bars on the KLD values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_nz_metrics(dataset_key, n_gals_use, N_f, nz_klds):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_kld'+str(n_gals_use)+dataset_key)\n",
    "    if os.path.exists(loc+'.hkl'):\n",
    "        with open(loc+'.hkl', 'r') as nz_file:\n",
    "        #read in content of list/dict\n",
    "            nz_stats = hickle.load(nz_file)\n",
    "    else:\n",
    "        nz_stats = {}\n",
    "        nz_stats['N_f'] = []\n",
    "        for f in formats:\n",
    "            nz_stats[f] = []\n",
    "    \n",
    "    if N_f not in nz_stats['N_f']:\n",
    "        nz_stats['N_f'].append(N_f)\n",
    "        for f in formats:\n",
    "            nz_stats[f].append([])\n",
    "        \n",
    "    where_N_f = nz_stats['N_f'].index(N_f) \n",
    "    \n",
    "    for f in formats:\n",
    "        nz_stats[f][where_N_f].append(nz_klds[f])\n",
    "\n",
    "    with open(loc+'.hkl', 'w') as nz_file:\n",
    "        hickle.dump(nz_stats, nz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_nz_metrics(dataset_key, n_gals_use):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_kld'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as nz_file:\n",
    "        nz_stats = hickle.load(nz_file)\n",
    "\n",
    "    flat_floats = np.array(nz_stats['N_f']).flatten()\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "\n",
    "    for f in formats:\n",
    "#     mu = np.mean(np.array(nz_stats[dataset_key][f]), axis=0)\n",
    "#     sigma = np.std(np.array(nz_stats[dataset_key][f]), axis=0)\n",
    "        data_arr = np.swapaxes(np.array(nz_stats[f]), 0, 1)#turn N_f * instantiations into instantiations * N_f\n",
    "        n_i = len(data_arr)\n",
    "        a = 1./len(formats)#1./n_i\n",
    "        plt.plot([10. * max(flat_floats), 10. * max(flat_floats)], [1., 10.], color=colors[f], alpha=a, label=f, linestyle=styles[f])\n",
    "        for i in data_arr:\n",
    "            plt.plot(flat_floats, i, color=colors[f], alpha=a, linestyle=styles[f])\n",
    "            kld_min.append(min(i))\n",
    "            kld_max.append(max(i))\n",
    "    plt.semilogy()\n",
    "    plt.semilogx()\n",
    "    plt.ylim(min(kld_min) / 10., 10. *  max(kld_max))\n",
    "    plt.xlim(min(flat_floats) / 3., max(flat_floats) * 3.)\n",
    "    plt.xlabel(r'number of parameters', fontsize=14)\n",
    "    plt.ylabel(r'KLD', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(r'$\\hat{n}(z)$ KLD on '+str(n_gals_use)+' from '+dataset_key, fontsize=16)\n",
    "\n",
    "    plt.savefig(loc+'.png', dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    a = 1./len(formats)\n",
    "    for f in formats:\n",
    "#     mu = np.mean(np.array(nz_stats[dataset_key][f]), axis=0)\n",
    "#     sigma = np.std(np.array(nz_stats[dataset_key][f]), axis=0)\n",
    "        data_arr = np.swapaxes(np.array(nz_stats[f]), 0, 1)#turn N_f * instantiations into instantiations * N_f\n",
    "        plt.plot([10. * max(flat_floats), 10. * max(flat_floats)], [1., 10.], color=colors[f], label=f, linestyle=styles[f])\n",
    "        kld_min.append(np.min(data_arr))\n",
    "        kld_max.append(np.max(data_arr))\n",
    "        mean = np.mean(data_arr, axis=0)\n",
    "        std = np.std(data_arr, axis=0)\n",
    "        x_cor = np.array([flat_floats[:-1], flat_floats[:-1], flat_floats[1:], flat_floats[1:]])\n",
    "        y_plus = mean + std\n",
    "        y_minus = mean - std\n",
    "        y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "        plt.plot(flat_floats, mean, color=colors[f], linestyle=styles[f])\n",
    "        plt.fill(x_cor, y_cor, color=colors[f], alpha=a, linewidth=0.)\n",
    "    plt.semilogy()\n",
    "    plt.semilogx()\n",
    "    plt.ylim(min(kld_min) / 10., 10. *  max(kld_max))\n",
    "    plt.xlim(min(flat_floats) / 3., max(flat_floats) * 3.)\n",
    "    plt.xlabel(r'number of parameters', fontsize=14)\n",
    "    plt.ylabel(r'KLD', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(r'$\\hat{n}(z)$ KLD on '+str(n_gals_use)+' from '+dataset_key, fontsize=16)\n",
    "\n",
    "    plt.savefig(loc+'_clean.png', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Okay, now all I have to do is have this loop over both datasets, number of galaxies, number of floats, and instantiations!\n",
    "\n",
    "Note: It takes about 5 minutes per \\# floats considered for 100 galaxies, and about 40 minutes per \\# floats for 1000 galaxies.  (So, yes, it scales more or less as expected!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "floats = [100]#[3, 10, 30, 100]\n",
    "sizes = [10]#, 1000, 10000]\n",
    "names = ['Optical']#dataset_info.keys()#['Optical', 'Optical+IR']\n",
    "instantiations = range(0, 1)#10)\n",
    "\n",
    "#many_colors = ['red','green','blue','cyan','magenta','yellow']\n",
    "high_res = 300\n",
    "n_plot = 5\n",
    "n_moments_use = 4\n",
    "\n",
    "#make this a more clever structure, i.e. a dict\n",
    "formats = ['quantiles', 'histogram', 'samples']\n",
    "colors = {'quantiles': 'blueviolet', 'histogram': 'darkorange', 'samples': 'forestgreen'}\n",
    "styles = {'quantiles': '--', 'histogram': ':', 'samples': '-.'}\n",
    "stepstyles = {'quantiles': 'dashed', 'histogram': 'dotted', 'samples': 'dashdot'}\n",
    "\n",
    "pz_max = [1.]\n",
    "nz_max = [1.]\n",
    "hist_max = [1.]\n",
    "dist_min = [0.]\n",
    "dist_max = [0.]\n",
    "moment_max = [[]] * (n_moments_use - 1)\n",
    "mean_max = [[]] * (n_moments_use - 1)\n",
    "kld_min = [1.]\n",
    "kld_max = [1.]\n",
    "\n",
    "randos = [np.random.choice(size, (len(names), n_plot), replace=False) for size in sizes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"pipeline\" is a bunch of nested `for` loops because `qp.Ensemble` makes heavy use of multiprocessing.  Doing multiprocessing within multiprocessing may or may not cause problems, but I am certain that it makes debugging a nightmare.\n",
    "\n",
    "Okay, without further ado, let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the \"pipeline\"\n",
    "\n",
    "for n in range(len(names)):\n",
    "    name = names[n]\n",
    "    \n",
    "    dataset_start = timeit.default_timer()\n",
    "    print('started '+name)\n",
    "    \n",
    "    pdfs = setup_dataset(name)\n",
    "    \n",
    "    for s in range(len(sizes)):\n",
    "        size=sizes[s]\n",
    "        \n",
    "        size_start = timeit.default_timer()\n",
    "        print('started '+str(size)+name)\n",
    "        \n",
    "        path = os.path.join(name, str(size))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        n_gals_use = size\n",
    "        \n",
    "        randos = randos[s][n]#np.random.choice(size, n_plot, replace=False)\n",
    "        \n",
    "        for i in instantiations:\n",
    "        \n",
    "            original = '_original_('+str(i)+')'\n",
    "            pdfs_use = make_instantiation(name, size, pdfs, bonus=original)\n",
    "            plot = plot_examples(size, name, bonus=original)\n",
    "        \n",
    "            z_grid = dataset_info[name]['in_z_grid']\n",
    "            N_comps = dataset_info[name]['N_GMM']\n",
    "        \n",
    "            postfit = '_post-fit_('+str(i)+')'\n",
    "            catalog = setup_from_grid(name, pdfs_use, z_grid, N_comps, high_res=high_res, bonus=postfit)\n",
    "            plot = plot_examples(size, name, bonus=postfit)\n",
    "        \n",
    "            for n_floats_use in floats:\n",
    "            \n",
    "                float_start = timeit.default_timer()\n",
    "                print('started '+str(size)+name+str(n_floats_use)+'\\#'+str(i))\n",
    "        \n",
    "                (ensembles, pz_klds, metric_moments) = analyze_individual(catalog, \n",
    "                                                          z_grid,#dataset_info[name]['metric_z_grid'], \n",
    "                                                          n_floats_use, name, n_moments_use, i=i)\n",
    "                for f in formats:\n",
    "                    fname = '_'+str(n_floats_use)+f+'_('+str(i)+')'\n",
    "                    plot = plot_examples(size, name, bonus=fname)\n",
    "                plot = plot_individual(size, name, n_floats_use, i=i)\n",
    "                save_pz_metrics(name, size, n_floats_use, metric_moments)\n",
    "            \n",
    "                (stack_evals, nz_klds) = analyze_stacked(catalog, ensembles, z_grid,#dataset_info[name]['metric_z_grid'], \n",
    "                                                     n_floats_use, name, i=i)\n",
    "                plot = plot_estimators(size, name, n_floats_use, i=i)\n",
    "                save_nz_metrics(name, size, n_floats_use, nz_klds)\n",
    "            \n",
    "                print('finished '+str(size)+name+str(n_floats_use)+' in '+str(timeit.default_timer() - float_start))\n",
    "        \n",
    "        plot = plot_pz_metrics(name, size)\n",
    "        \n",
    "        plot = plot_nz_metrics(name, size)\n",
    "        \n",
    "        print('finished '+str(size)+name+' in '+str(timeit.default_timer() - size_start))\n",
    "        \n",
    "    print('finished '+name+' in '+str(timeit.default_timer() - dataset_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remake the plots to share axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    for size in sizes:\n",
    "        path = os.path.join(name, str(size))\n",
    "        for i in instantiations:\n",
    "            \n",
    "            plot = plot_examples(size, name, bonus='_original_('+str(i)+')')\n",
    "        \n",
    "            plot = plot_examples(size, name, bonus='_post-fit_('+str(i)+')')\n",
    "            \n",
    "            for n_floats_use in floats:\n",
    "            \n",
    "                for f in formats:\n",
    "                    fname = '_'+str(n_floats_use)+f+'_('+str(i)+')'\n",
    "                    plot = plot_examples(size, name, bonus=fname)\n",
    "                plot = plot_individual(size, name, n_floats_use, i)\n",
    "            \n",
    "                plot = plot_estimators(size, name, n_floats_use, i)\n",
    "            \n",
    "        plot = plot_pz_metrics(name, size)\n",
    "        \n",
    "        plot = plot_nz_metrics(name, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
