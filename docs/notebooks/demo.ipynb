{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `qp` Demo\n",
    "\n",
    "_Alex Malz, Phil Marshall * Eric Charles_\n",
    "\n",
    "In this notebook we use the `qp` module to approximate some simple, standard, 1-D PDFs using sets of quantiles, samples, and histograms, and assess their relative accuracy. We also show how such analyses can be extended to use \"composite\" PDFs made up of mixtures of standard distributions.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "To run `qp`, you will need to first install the module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.stats as sps\n",
    "import scipy.interpolate as spi\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import qp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background, the scipy.stats module\n",
    "\n",
    "The scipy.stats module is the standard for manipulating distribtions.  It allows you do define a wide variety of distibutions and uses numpy array broadcasting for efficiency, here are some examples of things you can do with the scipy.stats module, using a Gaussian or normal distribution.\n",
    "\n",
    "Note the distinction passing arguments to `norm` and passing arguments to `pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the PDF at one point for one distribution\n",
    "print(\"PDF at one point for one distribution:\", sps.norm(loc=0, scale=1).pdf(0.5))\n",
    "# evaluate the PDF at multiple points for one distribution\n",
    "print(\"PDF at three points for one distribution:\", sps.norm(loc=0, scale=1).pdf([0.5, 1., 1.5]))\n",
    "# evalute the PDF one point from three distributions\n",
    "print(\"PDF at one point for three distributions:\", sps.norm(loc=[0., 1., 2.], scale=1).pdf(0.5))\n",
    "# evalute the PDF at one different point of each of three distributions\n",
    "print(\"PDF at one differnt point for three distributions:\", sps.norm(loc=[0., 1., 2.], scale=1).pdf([0.5, 1., 1.5]))\n",
    "# evalute the PDF at four differents point of each of three distributions (note the change in shape of the argument)\n",
    "print(\"PDF at four differnt points for three distributions:\\n\",\n",
    "      sps.norm(loc=[0., 1., 2.], scale=1).pdf([[0.5],[1.],[1.5],[2]]))\n",
    "# evalute the PDF at four differents point of each of three distributions (note the change in shape of the argument)\n",
    "print(\"PDF at four differnt points for three distributions: broadcast reversed\\n\",\n",
    "      sps.norm(loc=[[0.], [1.], [2.]], scale=1).pdf([0.5,1.,1.5,2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the scipy.stats classes\n",
    "\n",
    "In the scipy.stats module, all of the distributions are sub-classes of stats.rv_continuous.  You make an object of a particular sub-type, and then 'freeze' it by passing it shape parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the generic distribution class: \", sps._continuous_distns.norm_gen)\n",
    "ng = sps._continuous_distns.norm_gen()\n",
    "print(\"This is an instance of the generic distribution class\", ng)\n",
    "norm_sp = ng(loc=0, scale=1)\n",
    "print(\"This is a frozen distribution, with specific paramters\", norm_sp, norm_sp.kwds)\n",
    "print(\"The frozen object know what distribution it comes from\", norm_sp.dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of distribtuions\n",
    "\n",
    "`scipy.stats` lets you use a evaluate of properties of distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PDF = \", norm_sp.pdf(0.5))\n",
    "print(\"CDF = \", norm_sp.cdf(0.5))\n",
    "print(\"PPF = \", norm_sp.ppf(0.6))\n",
    "print(\"ISF = \", norm_sp.isf(0.6))\n",
    "print(\"SF = \", norm_sp.sf(0.5))\n",
    "print(\"RVS = \", norm_sp.rvs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending `scipy.stats` to implement the distributions that we want to use\n",
    "\n",
    "The next part of this notebook shows how we can extend the functionality of `scipy.stats` to implement distributions that are based on objects like histograms or interpolations or splines or mixture models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qp normal distribution\n",
    "\n",
    "This is a trivial extension, I've just added the number of pdfs as a member of the `scipy.stats.norm_gen` distribution.\n",
    "\n",
    "loc and scale are the means and stdevs of the underlying Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = np.array([[0],[1]])\n",
    "scale = np.array([[1],[1]])\n",
    "norm_dist = qp.stats.norm(loc=loc, scale=scale)\n",
    "xvals = np.linspace(-5, 5, 51)\n",
    "yvals = norm_dist.pdf(xvals)\n",
    "print(\"This object represents %i pdfs\" % norm_dist.npdf)\n",
    "print(\"The input and output shapes are:\", xvals.shape, yvals.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a single distribution you can plot it, the `qp.qp_plot_native` function will find a nice way to represent the data used to construct the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc1 = np.array([[0]])\n",
    "scale1 = np.array([[1]])\n",
    "norm_dist1 = qp.stats.norm(loc=loc1, scale=scale1)\n",
    "fig, axes = qp.plotting.plot_native(norm_dist1, xlim=(-5., 5.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting shapes\n",
    "\n",
    "The shape of the array returned by a call to the pdf function of a distribution depends on the shape of the parameters and evaluate points.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case we return an array were the rows are the evaluation points and the columns the different PDFs\n",
    "vector_pdf = qp.stats.norm(loc=[0., 1., 2], scale=1.)\n",
    "vector_pdf.pdf([[0.], [0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same, except we use `numpy.expand_dims` to shape the input array of evaluation points\n",
    "vector_pdf = qp.stats.norm(loc=[0., 1., 2], scale=1.)\n",
    "vector_pdf.pdf(np.expand_dims(np.array([0., 0.5]), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case we return an array were the rows are pdfs and the columns the evaluation points\n",
    "vector_pdf = qp.stats.norm(loc=[[0.], [1.], [2]], scale=1.)\n",
    "vector_pdf.pdf([0., 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same, except we use `numpy.expand_dims` to shape the input array of pdf parameters\n",
    "vector_pdf = qp.stats.norm(loc=np.expand_dims([0., 1., 2], -1), scale=1.)\n",
    "vector_pdf.pdf([0., 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### qp convention for set of distributions\n",
    "\n",
    "For distributions that take multiple input arrays, we are going to use the convention that the rows are the individual PDFs and the columns are the parameter values, or the evaluation points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other scipy distribtuions\n",
    "\n",
    "`qp` automatically generates classes for all of the scipy distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp.stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(qp.stats.lognorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(qp.stats.lognorm_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qp histogram distribution\n",
    "\n",
    "This represents a set of distributions made by interpolating a set of histograms with shared binning.\n",
    "To construct this you need to give the bin edges (shape=(N)) and the bin values (shape=(npdf, N-1)).\n",
    "\n",
    "Note that the native visual representation is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-5, 5, 11)\n",
    "cdf = norm_dist.cdf(bins)\n",
    "bin_vals = cdf[:,1:] - cdf[:,0:-1]\n",
    "hist_dist = qp.hist(bins=bins, pdfs=bin_vals)\n",
    "yvals = hist_pdf.pdf(xvals)\n",
    "hist_dist1 = qp.hist(bins=bins, pdfs=np.atleast_2d(bin_vals[0]))\n",
    "fig, axes = qp.plotting.plot_native(hist_dist1, xlim=(-5., 5.))\n",
    "leg = fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"For an input vector of shape %s the output shape is %s\" % (xvals.shape, yvals.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to evaluate a vector of input values, where each input value is different for each PDF?  In that case you need the shape of the vector of input value to match the implicit shape of the PDFs, which in this case is (2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals_x = np.array([[-1.], [1.]])\n",
    "yvals_x = hist_dist.pdf(xvals_x)\n",
    "print (\"For an input vector of shape %s the output shape is %s\" % (xvals_x.shape, yvals_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qp interpolated \n",
    "\n",
    "This represents a set of distributions made by interpolating a set of x and y values. To construct this you need to give the x and y values (both of shape=(npdf, N))\n",
    "\n",
    "Note that the native visual representation is pretty similar to the original one for the Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.linspace(-5, 5, 11)\n",
    "yvals = norm_dist.pdf(xvals)\n",
    "interp_dist = qp.interp(xvals=xvals, yvals=yvals)\n",
    "interp_vals = interp_dist.pdf(xvals)\n",
    "print(\"The input and output shapes are:\", xvals.shape, interp_vals.shape)\n",
    "interp_dist1 = qp.interp(xvals=xvals, yvals=np.atleast_2d(yvals[0]))\n",
    "fig, axes = qp.plotting.plot_native(interp_dist1, xlim=(-5., 5.), label=\"interpolated\")\n",
    "leg = fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qp spline distribution\n",
    "\n",
    "This represents a set of distributions made building a set of splines. To construct this you can give the x and y values(both of shape=(npdf, N))\n",
    "\n",
    "Note that the native visual representation is pretty similar to the original one for the Gaussian.\n",
    "\n",
    "Note also that the spline knots are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make a spline you need the spline knots, you can get those from the xval, yval values\n",
    "splx, sply, spln = qp.spline_gen.build_normed_splines(np.ones((2,11))*xvals, yvals)\n",
    "spline_dist_orig = qp.spline(splx=splx, sply=sply, spln=spln)\n",
    "# Or we can do these two steps together using one function\n",
    "spline_dist = qp.spline_from_xy(xvals=np.ones((2,11))*xvals, yvals=yvals)\n",
    "\n",
    "#\n",
    "spline_vals = spline_dist.pdf(xvals)\n",
    "print(\"The input and output shapes are:\", xvals.shape, spline_vals.shape)\n",
    "print(\"Spline knots\", spline_dist.dist.splx, spline_dist.dist.sply, spline_dist.dist.spln)\n",
    "spline_dist1 = qp.spline_from_xy(xvals=np.atleast_2d(xvals), yvals=np.atleast_2d(yvals[0]))\n",
    "print(spline_dist1.dist.splx.shape)\n",
    "fig, axes = qp.plotting.plot_native(spline_dist1, xlim=(-5., 5.), label=\"spline\")\n",
    "leg = fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qp quantile distribution\n",
    "\n",
    "This represents a set of distributions made by interpolating the locations at which various distributions reach a given set of quantiles.\n",
    "To construct this you need to give the quantiles edges (shape=(N)) and the location values (shape=(npdf, N)).\n",
    "\n",
    "Note that the native visual representation is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quants = np.linspace(0.01, 0.99, 7)\n",
    "locs = norm_dist.ppf(quants)\n",
    "quant_dist = qp.quant(quants=quants, locs=locs)\n",
    "quant_vals = quant_dist.pdf(xvals)\n",
    "print(\"The input and output shapes are:\", xvals.shape, quant_vals.shape)\n",
    "quant_dist1 = qp.quant(quants=np.atleast_1d(quants), locs=np.atleast_2d(locs[0]))\n",
    "fig, axes = qp.plotting.plot_native(quant_dist1, xlim=(-5., 5.), label=\"quantiles\")\n",
    "leg = fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernal density estimate \n",
    "\n",
    "This represents a set of distributions made by producing a kernal density estimate from a set of samples.\n",
    "\n",
    "To construct this you need to give the samples edges (shape=(npdf, N)).\n",
    "\n",
    "Note again that the the native visual represenation is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = norm_dist.rvs(size=(2, 1000))\n",
    "xvals_kde = np.linspace(-5., 5., 51)\n",
    "kde_dist = qp.spline_from_samples(xvals=xvals_kde, samples=samples)\n",
    "kde_vals = kde_dist.pdf(xvals_kde)\n",
    "print(\"The input and output shapes are:\", xvals.shape, kde_vals.shape)\n",
    "kde_dist1 = qp.spline_from_samples(xvals=xvals_kde, samples=np.atleast_2d(samples[0]))\n",
    "fig, axes = qp.plotting.plot_native(kde_dist1, xlim=(-5., 5.), label=\"kde\")\n",
    "leg = fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overplotting\n",
    "\n",
    "You can visually compare the represenations by plotting them all on the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = qp.plotting.plot_native(norm_dist1, xlim=(-5., 5.), label=\"norm\")\n",
    "qp.plotting.plot_native(hist_dist1, axes=axes)\n",
    "qp.plotting.plot_native(interp_dist1, axes=axes, label=\"interp\")\n",
    "qp.plotting.plot_native(spline_dist1, axes=axes, label=\"spline\")\n",
    "qp.plotting.plot_native(quant_dist1, axes=axes)\n",
    "qp.plotting.plot_native(kde_dist1, axes=axes)\n",
    "leg = fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `qp.Ensemble` Class\n",
    "\n",
    "This is the basic element of `qp` - an object representing a set of probability density functions. This class is stored in the module `ensemble.py`.  \n",
    "\n",
    "To create a `qp.Ensemble` you need to specify the class used to represent the PDFs, and provide that data for the specific set of PDFs.  Here we will great a 100 Gaussians with means distributed between -1 and 1, and widths distributed between 0.9 and 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = 2* (np.random.uniform(size=(100,1))-0.5)\n",
    "scales = 1 + 0.2*(np.random.uniform(size=(100,1))-0.5)\n",
    "ens_n = qp.Ensemble(qp.stats.norm, data=dict(loc=locs, scale=scales))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the ensemble\n",
    "\n",
    "All of the functions distribution functions, `pdf`, `cdf` etc... work as with underlying classes. \n",
    "\n",
    "Just can use the square brackets operator `[]` to isolate a single member of the ensemble.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_n = ens_n.pdf(xvals)\n",
    "print(\"The shapes are: \", xvals.shape, vals_n.shape)\n",
    "fig, axes = qp.plotting.plot_native(ens_n[15], xlim=(-5.,5.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the ensemble\n",
    "\n",
    "The `qp.qp_convert` function lets you convert ensembles to other representations. To do this you have to provide the original ensemble, the class you want to convert to, and any some keyword arguments to specify details about how to convert to the new class, here are some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-5, 5, 11)\n",
    "quants = np.linspace(0.01, 0.99, 7)\n",
    "print(\"Making hist\")\n",
    "ens_h = ens_n.convert_to(qp.hist_gen, bins=bins)\n",
    "print(\"Making interp\")\n",
    "ens_i = ens_n.convert_to(qp.interp_gen, xvals=bins)\n",
    "print(\"Making spline\")\n",
    "ens_s = ens_n.convert_to(qp.spline_gen, xvals=bins, method=\"xy\")\n",
    "#print(\"Making spline from samples\")\n",
    "#ens_s = ens_n.convert_to(qp.spline_gen, xvals=bins, samples=1000, method=\"samples\")\n",
    "print(\"Making quants\")\n",
    "ens_q = ens_n.convert_to(qp.quant_gen, quants=quants)\n",
    "print(\"Making mixmod\")\n",
    "ens_m = ens_n.convert_to(qp.mixmod_gen, samples=1000, ncomps=3)\n",
    "#print(\"Making flexcode\")\n",
    "#ens_f = ens_n.convert_to(qp.flex_gen, grid=bins, basis_system='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Comparisons: Plotting\n",
    "\n",
    "Let's visualize the PDF object in order to original and the other representaions.  The solid, black line shows the true PDF evaluated between the bounds.  The green rugplot shows the locations of the 1000 samples we took.  The vertical, dotted, blue lines show the percentiles we asked for, and the hotizontal, dotted, red lines show the 10 equally spaced bins we asked for.  Note that the quantiles refer to the probability distribution *between the bounds*, because we are not able to integrate numerically over an infinite range. Interpolations of each parametrization are given as dashed lines in their corresponding colors.  Note that the interpolations of the quantile and histogram parametrizations are so close to each other that the difference is almost imperceptible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = qp.plotting.plot_native(ens_n[15], xlim=(-5.,5.))\n",
    "qp.plotting.plot_native(ens_h[15], axes=axes)\n",
    "qp.plotting.plot_native(ens_i[15], axes=axes, label='interp')\n",
    "qp.plotting.plot_native(ens_s[15], axes=axes, label='spline')\n",
    "qp.plotting.plot_native(ens_q[15], axes=axes, label='quantile')\n",
    "qp.plotting.plot_native(ens_m[15], axes=axes, label='mixmod')\n",
    "#qp.qp_plot_native(ens_f[15], axes=axes, label='flex')\n",
    "leg = fig.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also interpolate the function onto an evenly spaced grid point and cache those values with the `gridded` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = np.linspace(-3., 3., 100)\n",
    "gridded = ens_n.pdf(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_gridded = ens_n.gridded(grid)[1]\n",
    "check = gridded - cached_gridded\n",
    "print(check.min(), check.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Parametrizations\n",
    "\n",
    "`qp` supports quantitative comparisons between different distributions, across parametrizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symm_lims = np.array([-1., 1.])\n",
    "all_lims = [symm_lims, 2.*symm_lims, 3.*symm_lims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, let's compare the different parametrizations to the truth using the Kullback-Leibler Divergence (KLD).  The KLD is a measure of how close two probability distributions are to one another -- a smaller value indicates closer agreement.  It is measured in units of bits of information, the information lost in going from the second distribution to the first distribution.  The KLD calculator here takes in a shared grid upon which to evaluate the true distribution and the interpolated approximation of that distribution and returns the KLD of the approximation relative to the truth, which is not in general the same as the KLD of the truth relative to the approximation.  Below, we'll calculate the KLD of the approximation relative to the truth over different ranges, showing that it increases as it includes areas where the true distribution and interpolated distributions diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a single pair of pdfs. (the 15th in each ensemble)\n",
    "klds = ens_n.kld(ens_s, limits=all_lims[0])[15]\n",
    "print(klds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all the other ensemble types\n",
    "ensembles = [ens_n, ens_h, ens_i, ens_s, ens_q, ens_m]\n",
    "for ensemble in ensembles[1:]:\n",
    "    D = []\n",
    "    for lims in all_lims:\n",
    "        klds = ens_n.kld(ensemble, limits=lims)\n",
    "        D.append(\"%.2e +- %.2e\" % (klds.mean(), klds.std()))\n",
    "    print(ensemble.gen_class.name + ' approximation: KLD over 1, 2, 3, sigma ranges = ' + str(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The progression of KLD values should follow that of the root mean square error (RMSE), another measure of how close two functions are to one another.  The RMSE also increases as it includes areas where the true distribution and interpolated distribution diverge.  Unlike the KLD, the RMSE is symmetric, meaning the distance measured is not that of one distribution from the other but of the symmetric distance between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ensemble in ensembles[1:]:\n",
    "    D = []\n",
    "    for lims in all_lims:\n",
    "        rmses = ens_n.rmse(ensemble, limits=lims)\n",
    "        D.append(\"%.2e +- %.2e\" % (rmses.mean(), rmses.std()))\n",
    "    print(ensemble.gen_class.name + ' approximation: RMSE over 1, 2, 3, sigma ranges = ' + str(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the KLD and RMSE metrics suggest that the quantile approximation is better in the high density region, but samples work better when the tails are included. We might expect the answer to the question of which approximation to use to depend on the application, and whether the tails need to be captured or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and retreiving ensembles\n",
    "\n",
    "You can store and retrieve ensembles from disk, using the `qp.Ensemble.write_to` and `qp.Ensemble.read_from` methods.\n",
    "\n",
    "These work in two steps, first the convert the Ensemble data to `astropy.table` objects, then they write the tables.  This means you can store the data in any format support by `astropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs = ens_n.build_tables()\n",
    "print(tabs.keys())\n",
    "print()\n",
    "print(\"Meta Data\")\n",
    "print(tabs['meta'])\n",
    "print()\n",
    "print(\"Object Data\")\n",
    "print(tabs['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a loopback test showing that we get the same results before and after a write/read cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_list = ['_n', '_h', '_i', '_s', '_q', '_m']\n",
    "filetypes = ['fits', 'hdf5']\n",
    "for ens, suffix in zip(ensembles, suffix_list):\n",
    "    for ft in filetypes:\n",
    "\n",
    "        outfile = \"test%s.%s\" % (suffix, ft)\n",
    "\n",
    "        pdf_1 = ens.pdf(bins)        \n",
    "        ens.write_to(outfile)\n",
    "        ens_r = qp.read(outfile)\n",
    "        pdf_2 = ens_r.pdf(bins)\n",
    "\n",
    "        check = pdf_1 - pdf_2\n",
    "        print(suffix, ft, check.min(), check.max())\n",
    "\n",
    "        os.unlink(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the moments of each approximation and compare those to the moments of the true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_moments = range(3)\n",
    "all_moments = []\n",
    "for ens in ensembles:\n",
    "    moments = []\n",
    "    for n in which_moments:\n",
    "        moms = qp.metrics.calculate_moment(ens, n, limits=(-3, 3))\n",
    "        moments.append(\"%.2e +- %.2e\" % (moms.mean(), moms.std()))\n",
    "    all_moments.append(moments)\n",
    "    \n",
    "print('moments: '+str(which_moments))\n",
    "for ens, mom in zip(ensembles, all_moments):\n",
    "    print(ens.gen_class.name+': '+str(mom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
