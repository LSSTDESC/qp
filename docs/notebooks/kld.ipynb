{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kullback-Leibler Divergence\n",
    "\n",
    "In this notebook, we try and gain some intuition about the magnitude of the KL divergence by computing its value between two Gaussian PDFs as a function of the \"tension\" between them.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You'll need the `qp` package and its dependencies (notably `scipy` and matplotlib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The Kullback-Leibler divergence between probability distributions $P$ and $Q$ is:\n",
    "\n",
    "$D(P||Q) = \\int_{-\\infty}^{\\infty} \\log \\left( \\frac{P(x)}{Q(x)} \\right) P(x) dx$\n",
    "\n",
    "The wikipedia page for the KL divergence gives the following useful interpretation of the KLD:\n",
    "\n",
    "> KL divergence is a measure of the difference between two probability distributions $P$ and $Q$. It is not symmetric in $P$ and $Q$. In applications, $P$ typically represents ... a precisely calculated theoretical distribution, while $Q$ typically represents ... [an] approximation of $P$.\n",
    ">\n",
    "> Specifically, the Kullback–Leibler divergence from $Q$ to $P$, denoted $D_{KL}(P‖Q)$, is a measure of the information gained when one revises one's beliefs from ... $Q$ to ... $P$. In other words, it is the amount of information lost when $Q$ is used to approximate $P$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Gaussian Illustration\n",
    "\n",
    "\"Information\" is not a terribly familiar quantity to most of us, so let's compute the KLD between two Gaussians:\n",
    "\n",
    "* The \"True\" 1D Gaussian PDF, $P(x)$, of unit width and central value 0\n",
    "\n",
    "* An \"approximating\" 1D Gaussian PDF, $Q$, of width $\\sigma$ and centroid $x$\n",
    "\n",
    "How does the KLD between these PDFs vary with offset $x$ and width $\\sigma$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import qp\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = qp.PDF(truth=sps.norm(loc=0.0, scale=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, sigma = 2.0, 1.0\n",
    "Q = qp.PDF(truth=sps.norm(loc=x, scale=sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infinity = 100.0\n",
    "D = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "print D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. Two equal-width Gaussians overlapping at their 1-sigma points have a KLD of 2 nats. \n",
    "\n",
    "> The unit of information here is a \"nat\" rather than a \"bit\" because `qp` uses a natural logarithm in its KLD calculation. 1 nat = $1/\\log{2} \\approx 1.44$  bits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the two Gaussians are perfectly aligned, but the approximation is broader than the truth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, sigma = 0.0, 4.37\n",
    "Q = qp.PDF(truth=sps.norm(loc=x, scale=sigma))\n",
    "D = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "print D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. Two concentric 1D Gaussian PDFs differing in width by a factor of 4.37 have a KLD of 1 nat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximation Precision\n",
    "\n",
    "Suppose our approximating PDF is broader than the true PDF, but the centroids are aligned. If we were to use our approximation, we'd be over-estimating the uncertainty in the inference. The approximating PDF represents a lower _precision_ measurement. Let's look at how the KLD quantifies this change in precision, as a function of the change in PDF width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "widths = np.logspace(-3.0,3.0,13)\n",
    "D = np.empty_like(widths)\n",
    "\n",
    "x = 0.0\n",
    "infinity = 1000.0\n",
    "\n",
    "for k,sigma in enumerate(widths):\n",
    "    Q = qp.PDF(truth=sps.norm(loc=x, scale=sigma))\n",
    "    D[k] = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "    \n",
    "print zip(widths, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = widths\n",
    "y = np.log(widths*(2.0/np.pi))\n",
    "plt.plot(x, y, color='gray', linestyle='-', lw=8.0, alpha=0.5, label='log($2\\sigma/\\pi}$)')\n",
    "\n",
    "plt.plot(widths, D, color='black', linestyle='-', lw=2.0, alpha=1.0, label='Offset=0.0')\n",
    "plt.xscale('log')\n",
    "plt.ylim(0.0,32.0)\n",
    "plt.xlabel('Width of approximating Gaussian $\\sigma$')\n",
    "plt.ylabel('KL divergence (nats)')\n",
    "l = plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks as though using an increasingly broad approximation distribution leads to logarithmically increasing information loss. \n",
    "\n",
    "When the approximating distribution gets _narrower_ than the truth, information is lost at a faster rate, which is interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tension between PDFs\n",
    "\n",
    "Two measurements that disagree with each other will lead to parameter PDFs that have different cenrtroids. Let's tabulate the KLD for a range of distribution offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "separations = np.linspace(0.0,15.0,16)\n",
    "D = np.empty_like(separations)\n",
    "\n",
    "sigma = 1.0\n",
    "infinity = 100.0\n",
    "\n",
    "for k,x0 in enumerate(separations):\n",
    "    Q = qp.PDF(truth=sps.norm(loc=x0, scale=sigma))\n",
    "    D[k] = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "    \n",
    "print zip(separations, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(separations, D, color='k', linestyle='-', lw=2.0, alpha=1.0, label='Width=1.0')\n",
    "plt.xlabel('Separation between Gaussians')\n",
    "plt.ylabel('KL divergence (nats)')\n",
    "l = plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For separations greater than about 7 sigma, numerical precision starts to matter: the overlap integral out here is smaller than machine precision. `qp` uses a `safelog` function that replaces values smaller than the system threshold value with that threshold; the log of that threshold is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print np.log(sys.float_info.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Probably the precision analysis of the previous section suffered from the same type of numerical error, at very low approximation distribution widths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presumably what matters is the \"tension\" between the two distributions: the separation in units of the combined width, $\\sqrt{\\sigma_0^2 + \\sigma^2}$. This quantity comes up often in discussions of dataset combination, where the difference in centroids of two posterior PDFs needs to be expressed in terms of their widths: the quadratic sum makes sense in this context, since it would appear in the product of the two likelihoods were they to be combined.\n",
    "\n",
    "For a few different widths, let's plot KLD vs tension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infinity = 100.0\n",
    "widths = np.array([1.0,1.5,2.0,2.5,3.0,3.5,4.0]) \n",
    "separations = np.linspace(0.0,7.0,15)\n",
    "\n",
    "D = np.zeros([7,len(separations)])\n",
    "tensions = np.empty_like(D)\n",
    "\n",
    "for j,sigma in enumerate(widths):\n",
    "    \n",
    "    for k,x0 in enumerate(separations):\n",
    "        Q = qp.PDF(truth=sps.norm(loc=x0, scale=sigma))\n",
    "        D[j,k] = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "        tensions[j,k] = x0 / np.sqrt(sigma*sigma + 1.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tensions[0,:]\n",
    "y = x**2\n",
    "plt.plot(x, y, color='gray', linestyle='-', lw=8.0, alpha=0.5, label='$t^2$')\n",
    "\n",
    "plt.plot(tensions[0,:], D[0,:], color='black', linestyle='-', lw=2.0, alpha=1.0, label='Width=1.0')\n",
    "plt.plot(tensions[1,:], D[1,:], color='violet', linestyle='-', lw=2.0, alpha=1.0, label='Width=1,5')\n",
    "plt.plot(tensions[2,:], D[2,:], color='blue', linestyle='-', lw=2.0, alpha=1.0, label='Width=2.0')\n",
    "plt.plot(tensions[3,:], D[3,:], color='green', linestyle='-', lw=2.0, alpha=1.0, label='Width=2.5')\n",
    "plt.plot(tensions[4,:], D[4,:], color='yellow', linestyle='-', lw=2.0, alpha=1.0, label='Width=3.0')\n",
    "plt.plot(tensions[5,:], D[5,:], color='orange', linestyle='-', lw=2.0, alpha=1.0, label='Width=3.5')\n",
    "plt.plot(tensions[6,:], D[6,:], color='red', linestyle='-', lw=2.0, alpha=1.0, label='Width=4.0')\n",
    "plt.xlabel('Tension between Gaussians, $t$ (sigma)')\n",
    "plt.ylabel('KL divergence (nats)')\n",
    "l = plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The simple numerical experiments in this notebook suggest the following approximate extrapolations and hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "The KL divergence, in nats, between an approximating, lower precision, correctly aligned Gaussian of variance $\\sigma^{2}$ and a true Gaussian of variance $\\sigma_{0}^{2}$ is related to the ratio between the two distribution widths:\n",
    "\n",
    "\\begin{align*}\n",
    "D &= \\int_{-\\infty}^{\\infty}P(x)\\log\\left[\\frac{P(x)}{Q(x)}\\right]dx\\\\\n",
    "&= \\int_{-\\infty}^{\\infty}P(x)\\log[P(x)]dx-\\int_{-\\infty}^{\\infty}P(x)\\log[Q(x)]dx\\\\\n",
    "&= \\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma_{0}}\\exp[-\\frac{(x-x_{0})^{2}}{2\\sigma_{0}^{2}}]\\log[\\frac{1}{\\sqrt{2\\pi}\\sigma_{0}}\\exp[-\\frac{(x-x_{0})^{2}}{2\\sigma_{0}^{2}}]]dx-\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma_{0}}\\exp[-\\frac{(x-x_{0})^{2}}{2\\sigma_{0}^{2}}]\\log[\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp[-\\frac{(x-x_{0})^{2}}{2\\sigma^{2}}]]dx\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi}\\sigma_{0}}\\left(\\int_{-\\infty}^{\\infty}\\exp[-\\frac{(x-x_{0})^{2}}{2\\sigma_{0}^{2}}]\\left(-\\log[\\sqrt{2\\pi}\\sigma_{0}]-\\frac{(x-x_{0})^{2}}{2\\sigma_{0}^{2}}]\\right)dx-\\int_{-\\infty}^{\\infty}\\exp[-\\frac{(x-x_{0})^{2}}{2\\sigma_{0}^{2}}]\\left(-\\log[\\sqrt{2\\pi}\\sigma]-\\frac{(x-x_{0})^{2}}{2\\sigma^{2}}\\right)]dx\\right)\n",
    "\\end{align*}\n",
    "\n",
    "We substitute $u=\\frac{(x-x_{0})^{2}}{2\\sigma_{0}}$:\n",
    "\\begin{align*}\n",
    "D &= \\frac{1}{\\sqrt{2\\pi}\\sigma_{0}}\\left(\\int\\exp[-u]\\left(-\\log[\\sqrt{2\\pi}\\sigma_{0}]-u]\\right)\\frac{du}{\\frac{\\sqrt{2u}}{\\sigma_{0}}}-\\int\\exp[-u]\\left(-\\log[\\sqrt{2\\pi}\\sigma]-\\frac{\\sigma_{0}^{2}}{\\sigma^{2}}u\\right)]\\frac{du}{\\frac{\\sqrt{2u}}{\\sigma_{0}}}\\right)\\\\\n",
    "&= \\frac{1}{2\\sqrt{\\pi}}\\left(\\int\\left(-\\log[\\sqrt{2\\pi}\\sigma_{0}]u^{-\\frac{1}{2}}\\exp[-u]-u^{\\frac{1}{2}}\\exp[-u]\\right)du-\\int\\left(-\\log[\\sqrt{2\\pi}\\sigma]u^{-\\frac{1}{2}}\\exp[-u]-\\frac{\\sigma_{0}^{2}}{\\sigma^{2}}u^{\\frac{1}{2}}\\exp[-u]\\right)du\\right)\\\\\n",
    "&= -\\frac{1}{2\\sqrt{\\pi}}\\left(\\left(\\log[\\sqrt{2\\pi}\\sigma_{0}]\\left[\\sqrt{\\pi}erf[u^{\\frac{1}{2}}]\\right]+\\left[\\frac{\\sqrt{\\pi}}{2}erf[u^{\\frac{1}{2}}]-u^{-\\frac{1}{2}}\\exp[-u]\\right]\\right)-\\left(\\log[\\sqrt{2\\pi}\\sigma]\\left[\\sqrt{\\pi}erf[u^{\\frac{1}{2}}]\\right]+\\frac{\\sigma_{0}^{2}}{\\sigma^{2}}\\left[\\frac{\\sqrt{\\pi}}{2}erf[u^{\\frac{1}{2}}]-u^{-\\frac{1}{2}}\\exp[-u]\\right]\\right)\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We transform back and evaluate this at the limits:\n",
    "\n",
    "\\begin{align*}\n",
    "D &= -\\frac{1}{2\\sqrt{\\pi}}\\left(\\left(\\log[\\sqrt{2\\pi}\\sigma_{0}]\\left[2\\sqrt{\\pi}]\\right]+\\left[\\sqrt{\\pi}]\\right]\\right)-\\left(\\log[\\sqrt{2\\pi}\\sigma]\\left[2\\sqrt{\\pi}]\\right]+\\frac{\\sigma_{0}^{2}}{\\sigma^{2}}\\left[\\sqrt{\\pi}]\\right]\\right)\\right)\\\\\n",
    "&= -\\frac{1}{2}\\left(\\log[\\frac{\\sigma_{0}}{\\sigma}]+1-\\frac{\\sigma_{0}^{2}}{\\sigma^{2}}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma_0$ is the width of the true distribution.\n",
    "\n",
    "We can perhaps take the KL divergence to provide a generalized quantification of increase of precision, as in: the increase in precision $\\alpha$ going from approximation to truth (which in the 1D Gaussian case is just $r^{-1}\\equiv\\frac{\\sigma_{0}}{\\sigma}$) is given by $\\alpha\\approx\\exp[-2D]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tension\n",
    "\n",
    "The KL divergence, in nats, between an approximating Gaussian and a true Gaussian is _approximately_ equal to the square of the tension between the two distributions.  By a similar derivation, we obtain:\n",
    "\n",
    "## $D = \\log[r]-\\frac{1}{2}(1-r^{-2})+\\frac{1}{2}(1+r^{-2})t^{2} \\approx t^2$ \n",
    "\n",
    "where tension $t$ is defined as\n",
    "\n",
    "## $t = \\frac{\\Delta x}{\\sqrt{\\left(\\sigma_0^2 + \\sigma^2\\right)}}$\n",
    "\n",
    "and has, in some sense, \"units\" of \"sigma\". The KLD is the information lost when using the approximation: the information loss rises in proprtion to the tension squared. An analytic derivation of this result would be welcome! The above formula is most accurate when the two distributions hwave the same width: it would be good to have a more general formula.\n",
    "\n",
    "Still: we can see that the KL divergence might provide a route to a generalized quantification of tension.  The square root of the KLD between a PDF and its approximation, in nats, gives an approximate sense of the tension between the two distributions, in \"units\" of \"sigma\":\n",
    "\n",
    "## $t \\approx \\sqrt{D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
